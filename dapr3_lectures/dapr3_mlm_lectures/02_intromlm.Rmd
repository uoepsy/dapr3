---
title: "<b>Multilevel Models</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    self-contained: true
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)

options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")

xaringanExtra::use_share_again()
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)

knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
theme_set(
    theme_minimal() + 
    theme(text = element_text(size=20))
)
source("jk_source/jk_presfuncs.R")

library(xaringanthemer)
style_mono_accent(
  base_color = "#88B04B", # DAPR3 
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  code_font_size = "0.7rem",
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)
```

class: inverse, center, middle

<h2>Part 1: LM to MLM</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>

???
- last week = little taster. 
- often, starting with LM is a good place, so that's what we'll do

---
# Terminology

```{r echo=FALSE, eval=FALSE}
tribble(
  ~word, ~freq,
  "multi-level model", 154000 + 31300,
  "hierarchical linear model", 24000,
  "mixed-effect model", 56500 + 191000,
  "mixed model", 1500000
  "random coefficient model", 11200+6920,
  "random-effect model", 101000 + 501000,
  "random parameter model", 2140 + 1460,
  "random-intercept model", 17100 + 2930, 
  "variance components model", 6210 + 5560,
  "partial pooling", 5120,
  "mixed error-component model", 62,
  "random slope model", 4010 + 1620,
  "panel data model", 55400,
  "latent curve model", 1520,
  "growth curve model", 18400
) -> mlmname


#mlmname$freq[mlmname$freq > 100000] <- c(75000,85000, 110000,80000,95000)*1.5

#wordcloud2(mlmname, shape="diamond", size=.4)
library(wordcloud)
wordcloud(words = mlmname$word, freq = mlmname$freq, random.order=FALSE,
          min.freq=1,
          scale=c(4,.5),rot.per=0,
          fixed.asp=T,
          #ordered.colors=T,
          colors="#88B04B")
```

```{r echo=FALSE, fig.cap="(size weighted by hits on google scholar)", fig.align="center", fig.asp=.9}
knitr::include_graphics("jk_img_sandbox/mlmname.png")
```

???
- lots of different names
- lots of different terminology
- [DESCRIBE PLOT]

---
# Notation 
<!-- $$ -->
<!-- \begin{align} -->
<!-- & \text{for observation }i \\ -->
<!-- \quad \\ -->
<!-- & \color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 \; + \; \beta_1 \cdot{} x_{i} } + \varepsilon_i \\ -->
<!-- \end{align} -->
<!-- $$ -->
**Simple regression**  
.pull-left[
$\begin{align} & \text{for observation }i \\ \quad \\ \quad \\ & \color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 \; + \; \beta_1 \cdot{} x_{i} } + \varepsilon_i \\ \end{align}$
]

???
- DAPR2 = almost entire focus on this [EQUATION]
- [COLOURS] = observations equals model plus error
- indexing is just at one level: observation i
- EG: test score ~ revision hours
  - we have score of person i 
  - intercept and slope x hours i
  - residual for that person i

---
# Notation 

<!-- $$ -->
<!-- \begin{align} -->
<!-- & \text{for observation }j\text{ in group }i \\ -->
<!-- \quad \\ -->
<!-- & \text{Level 1:} \\ -->
<!-- & \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ -->
<!-- & \text{Level 2:} \\ -->
<!-- & \color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}} \\ -->
<!-- & \color{blue}{\beta_{1i}} = \gamma_{10} + \color{orange}{\zeta_{1i}} \\ -->
<!-- \quad \\ -->
<!-- & \text{Where:} \\ -->
<!-- & \gamma_{00}\text{ is the population intercept, and }\color{orange}{\zeta_{0i}}\text{ is the deviation of group }i\text{ from }\gamma_{00} \\ -->
<!-- & \gamma_{10}\text{ is the population slope, and }\color{orange}{\zeta_{1i}}\text{ is the deviation of group }i\text{ from }\gamma_{10} \\ -->
<!-- $$ -->
**Multi-level**  
.pull-left[
$\begin{align} & \text{for observation }j\text{ in group }i \\ \quad \\ & \text{Level 1:} \\ & \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ & \text{Level 2:} \\ & \color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}} \\ & \color{blue}{\beta_{1i}} = \gamma_{10} + \color{orange}{\zeta_{1i}} \\ \quad \\ \end{align}$
]

???
- MLM = models that fit a structure with >1 level of observation
- canonical example = child j from class i
- NOTE i and j in some textbooks are switched 
- the MLM has a level 1 equation
  - school/child, this is for each child j [EQUATION]   
  - betas are now indexed to the school i from which that child j comes

- [OVERSIMPLIFIED] - its like lots of regression models for each group, smushed into one model



--

.pull-right[
$\begin{align} & \text{Where:} \\ & \gamma_{00}\text{ is the population intercept}\\ & \text{and  }\color{orange}{\zeta_{0i}}\text{ is the deviation of group }i\text{ from }\gamma_{00} \\ \qquad \\ & \gamma_{10}\text{ is the population slope,}\\ & \text{and }\color{orange}{\zeta_{1i}}\text{ is the deviation of group }i\text{ from }\gamma_{10} \\ \end{align}$
]

???
- to define each beta, we need another equation
- equations for betas are at level 2
  - level 2 = higher than level 1
  - school/child: this is a school-level equation specifying school level deviations
- for each school i, some deviation for that group, zeta_i from an overall average

---
count: false
# Notation 

**Multi-level**  
.pull-left[
$\begin{align} & \text{for observation }j\text{ in group }i \\ \quad \\ & \text{Level 1:} \\ & \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij} \\ & \text{Level 2:} \\ & \color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}} \\ & \color{blue}{\beta_{1i}} = \gamma_{10} + \color{orange}{\zeta_{1i}} \\ \quad \\ \end{align}$
]
.pull-right[
$\begin{align} & \text{Where:} \\ & \gamma_{00}\text{ is the population intercept}\\ & \text{and  }\color{orange}{\zeta_{0i}}\text{ is the deviation of group }i\text{ from }\gamma_{00} \\ \qquad \\ & \gamma_{10}\text{ is the population slope,}\\ & \text{and }\color{orange}{\zeta_{1i}}\text{ is the deviation of group }i\text{ from }\gamma_{10} \\ \end{align}$
]


We are now assuming $\color{orange}{\zeta_0}$, $\color{orange}{\zeta_1}$, and $\varepsilon$ to be normally distributed with a mean of 0, and we denote their variances as $\sigma_{\color{orange}{\zeta_0}}^2$, $\sigma_{\color{orange}{\zeta_1}}^2$, $\sigma_\varepsilon^2$ respectively.   

The $\color{orange}{\zeta}$ components also get termed the "random effects" part of the model, Hence names like "random effects model", etc.

???
- clever MLM = model group deviations as a distribution
- NOTE: a bit like the errors epsilon are deviations around a line, zetas are random deviations around some estimate
- what model is actually _estimating_ is __not__ the zetas, but _variances_ (sd) of the zetas

- hence "random effects" - we are supposing the school differences are because we have randomly sampled a set of schools
- different schools = different set of deviations
- randomly drawn from a population of schools

- NOTE we can choose what to be random effect
  - e.g. random intercept


---
# Notation 

**Mixed-effects == Multi Level**

Sometimes, you will see the levels collapsed into one equation, as it might make for more intuitive reading:

$\color{red}{y_{ij}} = \underbrace{(\gamma_{00} + \color{orange}{\zeta_{0i}})}_{\color{blue}{\beta_{0i}}} \cdot 1 + \underbrace{(\gamma_{10} + \color{orange}{\zeta_{1i}})}_{\color{blue}{\beta_{1i}}} \cdot x_{ij}  +  \varepsilon_{ij} \\$

.footnote[
**other notation to be aware of**  

- Many people use the symbol $u$ in place of $\zeta$  

- Sometimes people use $\beta_{00}$ instead of $\gamma_{00}$  

- In various resources, you are likely to see $\alpha$ used to denote the intercept instead of $\beta_0$  

]

???
- MLM collapses into one line
- sub in level 2 eqs into level 1
- termed "Mixed Effects" because effects = a mix of fixed values and random deviations around them. 



---
# Notation 

__Matrix form (optional)__

And then we also have the condensed matrix form of the model, in which the Z matrix represents the grouping structure of the data, and $\zeta$ contains the estimated random deviations. 


$\begin{align} \color{red}{\begin{bmatrix} y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\ y_{32} \\ \end{bmatrix}} & = \color{blue}{\begin{bmatrix} 1 & x_{11} \\ 1 & x_{12} \\ 1 & x_{21} \\ 1 & x_{22} \\1 & x_{31} \\ 1 & x_{32} \\ \end{bmatrix} \begin{bmatrix} \gamma_{00} \\ \beta_1 \\  \end{bmatrix}} & + & \color{orange}{ \begin{bmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 1 \\ \end{bmatrix} \begin{bmatrix}\zeta_{01} \\ \zeta_{02} \\ \zeta_{03} \end{bmatrix}} & + & \begin{bmatrix} \varepsilon_{11} \\ \varepsilon_{12} \\ \varepsilon_{21} \\ \varepsilon_{22} \\ \varepsilon_{31} \\ \varepsilon_{32} \end{bmatrix} \\ \qquad \\ \\ \color{red}{\boldsymbol y}\;\;\;\;\; & = \qquad \mathbf{\color{blue}{X \qquad \;\;\boldsymbol \beta}} & + & \qquad \; \mathbf{\color{orange}{Z \qquad \;\;\;\;\; \boldsymbol \zeta}} & + & \;\;\;\varepsilon \\ \end{align}$

<!-- $$ -->
<!-- \begin{align}  -->
<!-- \color{red}{ -->
<!-- \begin{bmatrix} -->
<!-- y_{11} \\ y_{12} \\ y_{21} \\ y_{22} \\ y_{31} \\ y_{32} \\ -->
<!-- \end{bmatrix} -->
<!-- } & =  -->
<!-- \color{blue}{ -->
<!-- \begin{bmatrix} -->
<!-- 1 & x_{11} \\ -->
<!-- 1 & x_{12} \\ -->
<!-- 1 & x_{21} \\ -->
<!-- 1 & x_{22} \\ -->
<!-- 1 & x_{31} \\ -->
<!-- 1 & x_{32} \\ -->
<!-- \end{bmatrix}  -->
<!-- \begin{bmatrix}  -->
<!-- \gamma_{00} \\ \beta_1 \\   -->
<!-- \end{bmatrix} -->
<!-- }  -->
<!-- & -->
<!-- + & -->
<!-- \color{orange}{ -->
<!-- \begin{bmatrix}  -->
<!-- 1 & 0 & 0 \\  -->
<!-- 1 & 0 & 0 \\ -->
<!-- 0 & 1 & 0 \\ -->
<!-- 0 & 1 & 0 \\ -->
<!-- 0 & 0 & 1 \\ -->
<!-- 0 & 0 & 1 \\ -->
<!-- \end{bmatrix} -->
<!-- \begin{bmatrix}  -->
<!-- \zeta_{01} \\ \zeta_{02} \\ \zeta_{03}  -->
<!-- \end{bmatrix} -->
<!-- } -->
<!-- & + & -->
<!-- \begin{bmatrix}  -->
<!-- \varepsilon_{11} \\ \varepsilon_{12} \\ \varepsilon_{21} \\ \varepsilon_{22} \\ \varepsilon_{31} \\ \varepsilon_{32}  -->
<!-- \end{bmatrix} \\  -->
<!-- \qquad \\  -->
<!-- \\ -->
<!-- \color{red}{\boldsymbol y}\;\;\;\;\; & = \qquad \mathbf{\color{blue}{X \qquad \;\;\boldsymbol \beta}} & + & \qquad \; \mathbf{\color{orange}{Z \qquad \;\;\;\;\; \boldsymbol \zeta}} & + & \;\;\;\varepsilon \\  -->
<!-- \end{align} -->
<!-- $$ -->

???
- [OPTIONAL] 
- can also express this as matrix of data and coefficients
- same as we did in prev lecture for simple regression

- y = vector length n
- x = n by p matrix
- beta = vector p coefficients
- z = n by g matrix of grouping structure
- zeta = a matrix of random effects zeta, 


---
# Fixed vs Random

.pull-left[
$\begin{align}& \text{Level 1:} \\ & \color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{1ij} + \beta_2 \cdot x_{2ij}} + \varepsilon_{ij} \\ & \text{Level 2:} \\ & \color{blue}{\beta_{0i}} = \underbrace{\gamma_{00}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{0i}}_{\textrm{random}}} \\ & \color{blue}{\beta_{1i}} = \underbrace{\gamma_{10}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{1i}}_{\textrm{random}}} \\ \quad \\ \end{align}$
]
.pull-right[
$\color{red}{y_{ij}} = (\underbrace{\gamma_{00}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{0i}}_{\textrm{random}}}) \cdot 1 + (\underbrace{\gamma_{10}}_{\textrm{fixed}} + \color{orange}{\underbrace{\zeta_{1i}}_{\textrm{random}}}) \cdot x_{1ij} + \underbrace{\beta_2}_{\textrm{fixed}} \cdot x_{2ij} +  \varepsilon_{ij} \\$
]

$\color{orange}{\zeta_i}$ is "random" because considered a random sample from larger population such that $\color{orange}{\zeta_i} \sim N(0, \sigma^2_{\color{orange}{\zeta_i}})$. 

???
- both MLM or "mixed effects", we model our "effects" (i.e., the estimated influence that variables exert over our outcome) as randomly varying across our observed groups. 
- our effects can either be:
  - a fixed constant for all our groups
  - a fixed estimate around which our groups randomly vary.   

- [EQUATION] effect of beta2 is just a single constant
- effect of beta0 and beta1 are fixed values around which we have groups deviating

---
# Grouping = Fixed or Random?

I have groups, should I:  

a. include group as a predictor $\color{blue}{\beta \cdot Group}$  
b. consider the grouping to be 'clusters' and include group-level random effects $\color{orange}{\zeta_{i}}$

| Criterion: | Repetition: <br> _If the experiment were repeated:_ | Desired inference: <br> _The conclusions refer to:_ |
|----------------|--------------------------------------------------|----------------------------------------------------|
| Fixed effects  | <center>Same levels would be used</center>     |    <center>The levels used </center>                                   |
| Random effects | <center>Different levels would be used</center>   | <center>A population from which the levels used<br> are just a (random) sample</center> |

Practical points:  
- Sometimes there isn't enough variability between groups to model random effects. 
  - $\sigma^2_{\color{orange}{\zeta}}$ gets estimated as (too close to) zero.
- Sometimes you might not have sufficient data (e.g. only have 6 'clusters'). 
  - estimate of $\sigma^2_{\color{orange}{\zeta}}$ needs sufficient $n$  

???
- random parts = those we consider to be random sample from larger population
- school/child example - schools themselves are a sample of schools
- instead of schools, we had whether the child was public vs private school
  - only 2 groups, and chance is that these are of specific inferential interest
- you can think of it as "if someone were to replicate this experiment, would they use the same levels of this variable?". 


- important: not everything that can be modelled as a random effect SHOULD be modelled as such.  
- for example, recall these models estimate the variance in group deviations around fixed estimates. 
  - with 3 groups, we have 3 deviations. but that's only 3 datapoints to base an estimate of variance on  


---
# Advantages of MLM

Multi-level models can be used to answer multi-level questions!  
<br><br>
Do phenomena at Level X predict __outcomes__ at Level Y?  

__example:__  
Does number of siblings predict variation in scores in childrens' first year of school?  

$$
\begin{align}
\textrm{score}_{ij} &= \beta_{0i} + \beta_1\textrm{year}_j + \varepsilon_{ij} \\
\beta_{0i} &= \gamma_{00} + \zeta_{0i} + \gamma_{01}\textrm{nr siblings}_i
\end{align}
$$
<br>
Single equation:  
$$
\begin{equation}
\textrm{score}_{ij} = (\gamma_{00} + \zeta_{0i}) + \gamma_{01}\textrm{nr siblings}_i + \beta_1\textrm{year}_j + \varepsilon_{ij}
\end{equation}
$$

???
- other than being one method of accounting for clustering in data, why fit MLM?
- MLM allows us to ask some interesting multilevel questions
- e.g. does some predictor at one level influences outcomes at another level.
- [EXAMPLE] we have measured childrens' performance on a test over time, so multiple observations per child. we fit a model where children's intercept varies, and we ask whether this is predicted by their nr of siblings 
- note that really this is identical to a beta. it's a fixed constant, it just applies to something measured at level 2



---
# Advantages of MLM

Multi-level models can be used to answer multi-level questions!  
<br><br>
Do phenomena at Level X influence __effects__ at Level Y?  

__example:__  
Does being mono/bi-lingual influence childrens' improvement in scores over time?  

$$
\begin{align}
\textrm{score}_{ij} &= \beta_{0} + \beta_{1i}\textrm{year}_j + \varepsilon_{ij} \\  
\beta_{1i} &= \gamma_{10} + \zeta_{1i} + \gamma_{11}\textrm{bilingual}_i
\end{align}
$$

<br>
Single equation:   
$$
\begin{equation}
\textrm{score}_{ij} = \beta_{0} + (\gamma_{10} + \zeta_{1i})\cdot\textrm{year}_j + \gamma_{11}\textrm{bilingual}_i\cdot\textrm{year}_j + \varepsilon_{ij}
\end{equation}
$$

???
- or if a variable at some level moderates the effect of a variable at a different level
- [EXAMPLE] 
we have measured childrens' performance on a test over time, so we have multiple observations for each child. we can ask whether the slope of time (and there's a slope for each child) differs depending on whether the child is bilingual
- and this is really just an interaction, but one effect is at child level, and the other is at observation level


---
# Advantages of MLM

Multi-level models can be used to answer multi-level questions!  
<br><br>
Do random variances covary?  

__example:__  
Do children who score higher at the start of school show greater improvements than those who start lower?  

$$
\begin{align}
\textrm{score}_{ij} &= \beta_{0i} + \beta_{1i}\textrm{year}_j + \varepsilon_{ij} \\
\beta_{0i} &= \gamma_{00} + \zeta_{0i}\\
\beta_{1i} &= \gamma_{10} + \zeta_{1i}\\
\end{align}
$$
$$
\begin{equation}
\begin{bmatrix} \zeta_{0i} \\ \zeta_{1i} \end{bmatrix}
\sim N
\left(
    \begin{bmatrix} 0 \\ 0 \end{bmatrix},
    \begin{bmatrix}
        \sigma_0^2 & \rho \sigma_0 \sigma_1 \\
        \rho \sigma_0 \sigma_1 & \sigma_1^2
    \end{bmatrix}
\right)
\end{equation}
$$



???
- can also get more complicated
- in estimating variances of random effects, the model can also estimate covariances
- [EQUATION]
- allows us to ask about how cluster level deviations in one aspect correlate with cluster level deviations at others
- [EXAMPLE] we have measured childrens' performance on a test over time, and each child has their own intercept and their own slope. we might be interested in whether children who start higher increase more (have steeper slopes)

---
# lme4

- **lme4** package (many others are available, but **lme4** is most popular).  

- `lmer()` function.  

- syntax is similar to `lm()`, in that we specify:   

    __*[outcome variable]*__ ~ __*[explanatory variables]*__, data = __*[name of dataframe]*__
    
- in `lmer()`, we add to this the random effect structure in parentheses:  

    __*[outcome variable]*__ ~ __*[explanatory variables]*__ + (__*[vary this]*__ | __*[by this grouping variable]*__),  
    data = __*[name of dataframe]*__, REML = __*[TRUE/FALSE]*__
    
```{r eval=FALSE}
lmer(score ~ 1 + year + (1 + year | school), data = ...
```


???
- before we go any furtehr, let's look at fitting these models in R
- it's straightforward
- lme4 package
- same notation
- _BUT_ - we specify the things we want to vary, and what we want to vary them by


---
class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: LM to MLM </b><b>School Example</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>

???
okay, lots of equations, let's take a spin a the visual side of this

---
# School Example: Data

.pull-left[

> Sample of 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ).   

]
.pull-right[
```{r}
crq_data <- read_csv("https://uoepsy.github.io/data/crqdata.csv")
head(crq_data)
```
]

???
Same data we saw last week. 
- 200 students, from 20 schools. 
- we're looking at how the level of routine a child has is associated with their regulation of emotions



---
count: false
# School Example: Data

.pull-left[

> Sample of 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ).   

```{r}
library(ICC)
ICCbare(schoolid, emot_dysreg, data = crq_data)
```

__Reminder:__ the Intraclass Correlation Coefficient is ratio of variance between clusters to the total variance (variance within + variance between).


```{r echo=TRUE, fig.show='hide'}
schoolplots <- 
  ggplot(crq_data, aes(x = crq, y = emot_dysreg, 
                  col = schoolid)) +
  geom_point()+
  facet_wrap(~schoolid) + 
  guides(col = "none") +
  labs(x = "Child Routines Questionnaire (CRQ)", 
       y = "Emotion Dysregulation Scale (EDS)")
```
]
.pull-right[

```{r fig.asp=.9}
schoolplots
```

]

???
- ICC is telling us that 24% variance in the emotion dysregulation scale can be attributed to clustering of children into schools
- and let's just remind ourselves of the facetted plots - one plot for each school. each dot is a child.

---
exclude: true
# ICC

.pull-left[
```{r fig.asp=.6}
library(ggridges)
ggplot(crq_data, aes(x = emot_dysreg, y = schoolid, 
                fill = schoolid)) +
  geom_density_ridges(jittered_points = TRUE, 
                      position = "raincloud", alpha = .4,
                      quantile_lines=TRUE,
                      quantile_fun=function(x,...) mean(x)) +
  guides(fill=FALSE)
```
]
.pull-right[
```{r}
library(ICC)
ICCbare(schoolid, emot_dysreg, data = crq_data)
```

__Reminder:__ the Intraclass Correlation Coefficient is ratio of variance between clusters to the total variance (variance within + variance between).

]

???
And we can see the clustering of our outcome variable, the emotion dysregulation scale, by different schools. These are just the distributions of children from each schools' scores, and we have an icc of .244, so we can say that roughly 24% of the variance in our outcome is explained by the clustering of schools. 

---
# School Example: fitting lm

.pull-left[
```{r highlight.output=c(11,12)}
lm_mod <- lm(emot_dysreg ~ 1 + crq, data = crq_data)
summary(lm_mod)
```

]

???
- start with the complete pooling again
- fit a simple linear model
- remember this "1" is just saying that we want the model to estimate an intercept

--

.pull-right[
```{r fig.asp=.8}
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1)
```

]

???
- and now, let's add the model fitted values of the model to the plot 
- while we have a plot for each school, the line is the same in all these. 

---
# School Example: Adding a random intercept

.pull-left[
vary the intercept by schools.
```{r highlight.output=c(13,19)}
library(lme4)
ri_mod <- lmer(emot_dysreg ~ 1 + crq + 
                 (1 | schoolid), data = crq_data)
summary(ri_mod)
```

]

???
- okay, let's switch to the random intercept model.  
- the "1" is that intercept, and we are saying "vary it by schools"
  - or "each school has can have a different intercept"

--

.pull-right[
```{r fig.asp=.8}
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1) + 
  geom_line(aes(y=fitted(ri_mod)), col = "red", lwd=1)
```
] 

???
- as you might expect. 
- and as you might expect, allowing intercept to vary between schools, mean height of line is different. 
- [BLUE LINE] = simple lm
- [RED LINE] = random intercept
- height is different, because different intercept, but slope is same

---
# School Example: Adding a random slope

.pull-left[
vary the intercept and the effect (slope) of CRQ by schools
```{r highlight.output=c(13,14,20,21)}
rs_mod <- lmer(emot_dysreg ~ crq + 
                 (1 + crq | schoolid), data = crq_data)
summary(rs_mod)
```

]

???
- in the next model here, let's _also_ allow the slope of crq to vary by schools
- so the association between the outcome EDS, and the predictor CRQ, can now be different for each school
- in our plots, that's the slope. so the slope will be different for each school
- The slope randomly varies between schools, around some fixed center. 


--

.pull-right[
```{r fig.asp=.8}
schoolplots + 
  geom_line(aes(y=fitted(lm_mod)), col = "blue", lwd=1) + 
  geom_line(aes(y=fitted(ri_mod)), col = "red", lwd=1) + 
  geom_line(aes(y=fitted(rs_mod)), col = "orange", lwd=1)
```
]

???
- [ORANGE] = random slope

---
# Partial Pooling vs No Pooling

.pull-left[
Why not fit a fixed effect adjustment to the slope of x for each group?  
`lm(y ~ x * group)`?

```{r}
fe_mod <- lm(emot_dysreg ~ crq * schoolid, data = crq_data)
```
]

.pull-right[
```{r fig.asp=.8}
schoolplots + 
  geom_line(aes(y=fitted(fe_mod)), col = "black", lwd=1)
```
]

???
- some questions in Piazza have started to wonder about the idea of "partial" pooling here.
- i want to contrast no-pooling (fixed effect of each cluster), in order to show why it is "partial"


---
# Partial Pooling vs No Pooling

.pull-left[
- We talked last week about how this results in a lot of output. With 20 schools, we get: intercept at reference school, adjustment for every other school, the effect of x at reference school, adjustment to effect of x for every other school. 
    ```{r}
    length(coef(fe_mod))
    ```
- information is not combined in anyway (data from school $i$ contributes to differences from reference school to school $i$, but nothing else. Information in schools 1 to 19 doesn't influence what the model thinks about school-20).  

]

???
- Coefficients from the no-pooling are the adjustments for intercept and slope for each school 


--

.pull-right[
```{r echo=FALSE}
m1<-lm(emot_dysreg~crq*schoolid, data = crq_data)
m2<-lmer(emot_dysreg~crq + (1 + crq | schoolid), data = crq_data)
crq_data %>% 
  mutate(
    lm_fit = fitted(m1),
    rs_fit = fitted(m2)
  ) %>%
  filter(schoolid %in% paste0("school",c(13,17,11,18))) %>%
  ggplot(., aes(x = crq)) + 
    geom_point(aes(y = emot_dysreg)) + 
    facet_wrap(~schoolid) +
    geom_line(aes(y = lm_fit, lty="fixed effects:\ny ~ x * g",col="fixed effects:\ny ~ x * g"), lwd=1) + 
    geom_line(aes(y = rs_fit, lty="random effects:\ny ~ x + (1 + x | g)", col="random effects:\ny ~ x + (1 + x | g)"), lwd=1) +
  scale_linetype_manual("model fitted values",values = c("fixed effects:\ny ~ x * g"=2,"random effects:\ny ~ x + (1 + x | g)"=1)) + 
  scale_color_manual("model fitted values",values = c("fixed effects:\ny ~ x * g"="black","random effects:\ny ~ x + (1 + x | g)"="orange"))+
  theme(legend.position="bottom")
```
]

???
- in the multilevel modelling approach, each school gets its own intercept and slope, but these 'borrow strength' from the others.   
- The borrowing of strength is more apparent for the (what would be) more extreme clusters, as well as those that have fewer datapoints. What happens to these cluster estimates is that they are shrunk towards the population average. 
- [PLOT]

---
# School Example: ICC from lmer

.pull-left[
```{r highlight.output=c(13,14)}
null_mod <- lmer(emot_dysreg ~ 1 + (1 | schoolid), data = crq_data) 
summary(null_mod)
```
]
.pull-right[
```{r}
0.0845 / (0.0845 + 0.2588)
```

Note: ICC is conditional on zero values of random-effects covariates.
In other words, it has computed the ICC based on a value of zero for the random slope variable(s), so any interpretation of the ICC is also based on a value of zero for the slope variable(s).

]

???
- recall that ICC is the proportion of variance between groups to the total variance.  
- we can get this from our lmer output!  
we're back to our childhood emotional dysregulation data here, and we can see that we have an estimated variance between schools, and the residual variance. between those, we can calculate the ratio of this to the total.  


Once we introduce random slopes/coefficients, things get more complicated. the ICC will be a function of the variable(s) for which random slopes are specified. 

---
# School Example: Explained Variance 

.pull-left[
__ $R^2$ __  

- Recall $R^2$ is proportion of variance explained

- In MLM, multiple variance components (not just $\varepsilon$). Do random effects "explain" variance?  
    - "marginal $R^2$" = variance explained due to fixed effects
    - "conditional $R^2$" = variance explained due to fixed + random

```{r warning=F, message=F}
library(performance)
rs_mod <- lmer(emot_dysreg ~ crq + 
                 (1 + crq | schoolid), 
               data = crq_data)
r2(rs_mod)
```


]

???
- no R squared given by default
- you will likely remember r squared from standard linear regression. in multilevel models, our r squared becomes a little more difficult - we have two metrics, the marginal (variance explained by fixed effects only), and the conditional (that explained by fixed and random)

--

.pull-right[
__Proportional Reduction in Variance (PRV)__  

- $PRV = \frac{\text{var}_{m0} - \text{var}_{m1}}{\text{var}_{m0}}$

- where $\text{var}_{m0}$ and $\text{var}_{m1}$ are variance components from models with and without a parameter.  


]

???
Another metric we have is the proportional reduction in variance. We can consider how the inclusion of a variable in a model reduces a variance component, for instance the residual variance. 


---
class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: LM to MLM </b><b>lme4 Output</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>

???
let's look now towards how we interpret the output from multilevel models in the lme4 package in R. 


---
# R: lmer output

.pull-left[
```{r echo=FALSE, highlight.output=c(20,21)}
my_data<-read_csv("lme4output.csv")
m=lmer(y ~ x + (1 + x | group), my_data)
summary(m, correlation=F)
```
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lmer2.png")
```
]

???
- more output than we get from lm
- first [FIXED EFFECTS]
- [PLOT] we've got an intercept and a slope, that defines a line! 


---
count: false
# R: lmer output

.pull-left[
```{r echo=FALSE, highlight.output=c(13,14)}
my_data<-read_csv("lme4output.csv")
m=lmer(y ~ x + (1 + x | group), my_data)
summary(m, correlation=F)
```
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lmer2a.png")
```
]

???
- [RANDOM]]
- what model estimates is the variances of the random effects
- cluster level variance of intercept, and of effect of x. 
- random effects are N(0,.. )
- [PLOT]
  - blue cluster has an intercept of 2 _above_ 
  - yellow cluster has an intercept of about 2 below
  - same logic holds for slope



---
count: false
# R: lmer output

.pull-left[
```{r echo=FALSE, highlight.output=c(13,14,20,21)}
my_data<-read_csv("lme4output.csv")
m=lmer(y ~ x + (1 + x | group), my_data)
summary(m, correlation=F)
```
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lmer3.png")
```
]

???
- The fixed and random parts together provide the cluster specific lines here.  
- we said the blue one had a higher intercept, and it has a more negative slope, and we can see that here.  


---
count: false
# R: lmer output

.pull-left[
```{r echo=FALSE, highlight.output=c(15)}
my_data<-read_csv("lme4output.csv")
m=lmer(y ~ x + (1 + x | group), my_data)
summary(m, correlation=F)
```
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lmer4.png")
```
]

???
- finally, we have the estimated standard deviation of our level 1 residuals. again, these are assumed to be normally distributed with a mean of 0.  
- and these are the deviations from our cluster specific lines to our individual observations y


---
# R: parameters

.pull-left[
```{r echo=FALSE}
my_data<-read_csv("lme4output.csv")
model=lmer(y ~ x + (1 + x | group), my_data)
summary(model, correlation=F)
```
]
.pull-right[
Fixed effects:  
```{r}
fixef(model)
```

Variance components:  
```{r}
VarCorr(model)
```


]


---
# R: ranef coef

.pull-left[
```{r echo=FALSE}
my_data<-read_csv("lme4output.csv")
model=lmer(y ~ x + (1 + x | group), my_data)
summary(model, correlation=F)
```
]
.pull-right[
```{r eval=F}
ranef(model)
```
```{r echo=F}
head(ranef(model)$group %>% round(.,4), 5L) %>% rbind(.,"...") -> op
row.names(op)[6] <- "..."
op
```
```{r eval=F}
coef(model)
```
```{r echo=F}
head(coef(model)$group %>% round(.,4), 5L) %>% rbind(.,"...") -> op
row.names(op)[6] <- "..."
op
```

coef = fixef + ranef

]

???
- fixef, ranef and coef.  


---
class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: LM to MLM </b><b>Estimation</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>



---
# Model Estimation

- For standard linear models, we can calculate the parameters using a *closed form solution*.


- Multilevel models are too complicated, we *estimate* all the parameters using an iterative procedure like Maximum Likelihood Estimation (MLE).

???
When it comes to how the parameters of our multilevel model are estimated, things can become more complicated. 
with simple linear regression models, there is one best set of values for our betas, which is the one that minimizes the sums of squared residuals, and these can be calculated algebraicially.  
but for multilevel models, which are considerably more complicated, we estimate the parameters using an iterative procedure like MLE

---
# Model Estimation: ML

Aim: find the values for the unknown parameters that maximize the probability of obtaining the observed data.  
How: This is done via finding values for the parameters that maximize the (log) likelihood function.

```{r echo=FALSE, out.height="450px"}
knitr::include_graphics("jk_img_sandbox/1stderiv.png")
```

???
you might remember this idea from logistic regression models at the end of DAPR2. 
the idea here is that the process step-by-step finds values for the parameters that have the greatest likelihood, given the data we observed - i.e, those values that maxmimise the probability of observing our data. 

Although we can think of max likelihood estimation in terms of a reaching the apex of a curve, this only really applies in the estimation of a single parameter

---
count:false
# Model Estimation: (log)Likelihood

- Data = multiple observations: $1, ..., n$ 

- From our axioms of probability, we can combine these *i.i.d* by multiplication to get the likelihood of our parameters given our entire sample

- Instead of taking the **product** of the individual likelihoods, we can take the **summation** of the log-likelihoods
    - This is considerably easier to do, and can be achieved because multiplication is addition on a log scale.

---
# Model Estimation: ML

In multilevel models, our parameter space is more complex (e.g. both fixed effects and variance components).

```{r echo=FALSE, out.height="450px"}
knitr::include_graphics("jk_img_sandbox/multisurftb.png")
```

???
in complex models with many parameters, our optimisation algorithms are trying to find the high point of a hugely complex surface. 

---
# Model Estimation: Convergence


- Sometimes a model is too complex to be supported by the data

- Balancing act between simplifying our model while preserving attribution of variance to various sources  

- Get used to seeing lots of errors and warnings:  

<br>

```
warning(s): Model failed to converge with max|grad| = 0.0021777 (tol = 0.002, component 1) (and others)
```
<br>
```
message(s): boundary (singular) fit: see help('isSingular')
```


---
# Model Estimation: ML vs REML

TODO add more detail  
- Standard ML results in biased estimates of variance components (esp. for small samples).

- Restricted Maximum Likelihood (REML) is the default in `lmer()`.

    - REML separates the estimation of fixed and random parts of the model, leading to less biased estimates of the variance components.  

???
in simple regression, it is rarely variance estimates that are of interest - rather, we are often focussed on the coefficients.  
but in multilevel models were are more often specifically interested in estimated variance of our random effects. 

an adaptation called REML can be used in which estimation of the random parts is done assuming the fixed parts to be known. 
sort of like estimating the fixed effects first and then the variance components.

this can lead to less biased estimates of our random effect variances
  



---
class: inverse, center, middle

<h2><b style="opacity:0.4;">Part 1: LM to MLM </b><b>Longitudinal Example</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2>

???
Okay, let's take a look at an example

---
# Longitudinal Example: Data & RQ

Researchers are interested in how cognition changes over time. 

.pull-left[
```{r}
cogtime <- read_csv("https://uoepsy.github.io/data/cogtimerpm.csv")
cogtime <- cogtime %>% 
  mutate(across(c(participant, isLhanded), factor))
head(cogtime, 12L)
```

]

???
let's suppose that we have a dataset on scores on a cognitive test over time. 
we have data from some participants - here's our participant identifier, and each one has 10 visits, and a score at each visit.

--

.pull-right[
```{r fig.asp=.7}
ggplot(cogtime, aes(x=visit_n, y = cog, col=participant))+
  geom_line(alpha = 0.5)+
  guides(col=FALSE)+
  scale_x_continuous(breaks=0:9)+
  ylim(0,100)
```
]

???
we can plot our data as a line for each participant, and see the overall pattern. 


---
# Longitudinal Example: Model Spec

__fixef effects__  

"cognition over time" = `cog ~ visit_n`  


__random effect structure__

- multiple data-points per participant: **1 | participant**

???
let's think about what our random effect structure might look like. we have several observations for each participant, and we might expect scores to vary between participants. so we'll fit a random intercept. 

--

- explanatory variable of interest (`visit_n`) varies *within* participants: **visit_n | participant**

???
our independent variable, visit_n, is also likely to vary between participants. some participants will decline slower, or faster, than others. 


--

- allow by-participant intercepts to correlate with by-participant slopes: **1 + visit_n | participant**  
(more on this in future weeks)

???
we'll also allow our random intercepts and slopes to correlate. we'll touch on this more in future weeks, but the default for lme4 is to allow random effects to covary. essentially, this means that we model a parameter to account for the extent that people's intercepts and slopes are correlated, for instance, people who start higher might decline less steeply,

--

__fitting the model__

```{r}
cogtime_model <- lmer(cog ~ visit_n + (1 + visit_n | participant), data = cogtime)
```

???
so here's our model 

---
# Longitudinal Example: Model Output

.pull-left[
__model output__

```{r}
summary(cogtime_model)
```
]
.pull-right[
__visual sketch__  

```{r echo=FALSE,fig.asp=.7}
library(ggdist)

plotline = MASS::mvrnorm(n=100, mu=fixef(cogtime_model),Sigma=sqrt(VarCorr(cogtime_model)[[1]])) %>%
  as_tibble()

ggplot(cogtime, aes(x=visit_n, y=cog))+
  geom_point(col=NA)+
  stat_dist_halfeye(aes(x=0,dist="norm",arg1=fixef(cogtime_model)[1],arg2=sqrt(diag(VarCorr(cogtime_model)[[1]]))[1]),alpha=.3, fill="orange") +
  geom_abline(data=plotline, aes(intercept=`(Intercept)`,slope=visit_n),alpha=.1)+ 
  geom_abline(intercept=fixef(cogtime_model)[1],slope=fixef(cogtime_model)[2],lwd=2)+
  scale_x_continuous(breaks=0:9)+
  ylim(0,100)
```
]

???
and if we examine our model output, we see that at baseline, the estimated average score is 67.3, and participants tend to vary around this, with a standard deviation of 1.21. 
for every visit, there is an estimated decline of 1.22 in our outcome, cognitive test scores.
again, participants vary around this estimate, some have less decline, some have more. 


---
# Longitudinal Example: Plotting Effects
#### **sjPlot::plot_model()**

.pull-left[
```{r eval=FALSE}
library(sjPlot)
plot_model(cogtime_model, type="eff") +
  scale_x_continuous(breaks=0:9)+
  ylim(0,100)
```
]

.pull-right[
```{r echo=FALSE, fig.asp=.7}
library(sjPlot)
plot_model(cogtime_model, term="visit_n", type="eff") +
  scale_x_continuous(breaks=0:9)+
  ylim(0,100)
```
]

???
as before, we can use sjPlot to plot our model predicted values


---
# Longitudinal Example: Plotting Effects
#### **effects::effect()**

.pull-left[
```{r eval=F}
library(effects)
as.data.frame(effect("visit_n",cogtime_model))
```
```
  visit_n   fit    se lower upper
1       1 67.34 1.208 64.96 69.72
2       3 64.90 1.452 62.04 67.77
3       6 61.25 2.083 57.14 65.35
4       8 58.81 2.582 53.72 63.90
5      10 56.37 3.110 50.24 62.50
```
```{r eval=FALSE}
as.data.frame(effect("visit_n",cogtime_model)) %>%
  ggplot(.,aes(x=visit_n, y=fit))+
  geom_line()+
  geom_ribbon(aes(ymin=lower,ymax=upper), alpha=.3) +
  scale_x_continuous(breaks=0:9)+
  ylim(0,100)
```
]


.pull-right[
```{r echo=FALSE, fig.asp=.7}
# p <- as.data.frame(effect("visit_n",cogtime_model)) %>%
#   ggplot(.,aes(x=visit_n, y=fit))+
#   geom_line(lwd=1,col="darkgreen")+
#   geom_ribbon(aes(ymin=lower,ymax=upper),col="darkgreen", fill="darkgreen", lty = "dashed", alpha=.4)
# save(p,file = "effplot.Rdata")
load("effplot.Rdata")
p + scale_x_continuous(breaks=0:9)+
  ylim(0,100)
```
]

???
if we want to extract the data and do this manually, we can use the effects package

---
# Longitudinal Example: Plotting Fits
#### **broom.mixed::augment()** for cluster-specific fits

.pull-left[
```{r}
library(broom.mixed)
augment(cogtime_model)
```
```{r eval=FALSE}
ggplot(augment(cogtime_model), 
       aes(x=visit_n, y=.fitted,
           group=participant))+
  geom_line()
```
]

???
and if we want to extract the cluster-specific fitted values we can use a function like augment() from the broom.mixed package. or fitted() 

--

.pull-right[
```{r echo=F, fig.asp=.8}
library(broom.mixed)
ggplot(augment(cogtime_model), 
       aes(x=visit_n, y=.fitted,
           group=participant))+
  geom_line() +
  scale_x_continuous(breaks=0:9)+
  ylim(0,100)
```
]

???
so here you can see we've plotted the fitted values, specifying via colour the different lines for each participant 


---
count: false
# Longitudinal Example: Plotting Fits
#### **broom.mixed::augment()** for cluster-specific fits

.pull-left[
```{r}
library(broom.mixed)
augment(cogtime_model)
```
```{r eval=FALSE}
ggplot(augment(cogtime_model), 
       aes(x=visit_n, y=.fitted,
           group=participant))+
  geom_line()
```
]

.pull-right[
```{r echo=F, fig.asp=.8}
library(broom.mixed)
p + scale_x_continuous(breaks=0:9)+
  ylim(0,100) + 
  geom_line(data=augment(cogtime_model), 
       aes(x=visit_n, y=.fitted,
           group=participant), alpha=.1)
```
]

???

generally, our main finding of interest is the fixed effect line, but it can be nice to also plot behind that the cluster specific fits, or a sample of them. to add context. 



---
# Longitudinal Example: Tables  

.pull-left[
```{r eval=FALSE}
library(sjPlot)
tab_model(cogtime_model, df.method = "kr")
```
]
.pull-right[
```{r eval=FALSE}
library(parameters)
model_parameters(cogtime_model, ci_method = "kr") %>% 
  print_html()
```
]

???
finally, we can also easily create tables of our model output. 
we'll talk about this df.method in just a second.. 


---
# Summary

- We can extend our linear model equation to model certain parameters as random cluster-level adjustments around a fixed center.

- $\color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 \; + \; \beta_1 \cdot{} x_{i} } + \varepsilon_i$  
becomes  
$\color{red}{y_{ij}} = \color{blue}{\beta_{0i} \cdot 1 + \beta_{1i} \cdot x_{ij}} + \varepsilon_{ij}$  
$\color{blue}{\beta_{0i}} = \gamma_{00} + \color{orange}{\zeta_{0i}}$

- We can express this as one equation if we prefer:
$\color{red}{y_{ij}} = \underbrace{(\gamma_{00} + \color{orange}{\zeta_{0i}})}_{\color{blue}{\beta_{0i}}} \cdot 1 +  \color{blue}{\beta_{1i} \cdot x_{ij}}  +  \varepsilon_{ij}$

- This allows us to model cluster-level variation around the intercept ("random intercept") and around slopes ("random slope"). 

- We can fit this using the **lme4** package in R

???
okay, let's take stock before we have a longer break. 
we've covered extending linear model equations to have equations specified at multiple levels, packaged in these parentheses if you prefer. 
what this allowed us to do, using the lme4 package, is model group or cluster level variation in our effects, and we talked briefly about how this has certain advantages such as the idea of more extreme clusters and more sparse clusters, being shrunk towards the average effect. 

we've also seen some of the questions that this allows us to answer. we used the children in schools example to discuss how we might ask whether school level predictors influence things that happen at the child level. 






---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: LM to MLM</h2>
<h2>Part 2: Inference in MLM</h2>


???
okay, let's talk about inference in multilevel models. 
this is a thorny issue, and there are a lot papers on the topic.
what we're aiming for is to have a broad overview of why it's a tricky issue, cover some of the options we have, and make a recommendation on which approach might be the most reliable to use.


---
# <p></p>

.pull-left[
you might have noticed...

```{r highlight.output=c(20,21)}
summary(cogtime_model)
```
]
.pull-right[
![](jk_img_sandbox/wotnop.png)
]

???
you will have probably noticed when we were discussing the output of an lmer() object, that we don't get any p-values. 
in lm, we're used to seeing an extra column here, with those little stars that people love to see. 


---
# Why no p-values?

**Extensive debate about how best to test parameters from MLMs.**  

???
there is a lot of debate on how we should be conducting inferential tests from multilevel models. 

--

In simple LM, we test the reduction in residual SS (sums of squares), which follows an $F$ distribution with a known $df$.
$$
\begin{align}
F \qquad = \qquad \frac{MS_{model}}{MS_{residual}} \qquad = \qquad \frac{SS_{model}/df_{model}}{SS_{residual}/df_{residual}} \\
\quad \\
df_{model} = k \\
df_{residual} = n-k-1 \\
\end{align}
$$
???
when we learned about standard linear model in DAPR2, tests were conducted on the reduction in sums of squared residuals, which follow an F distribution with k and n-k-1 degrees of freedom

--

The $t$-statistic for a coefficient in a simple regression model is the square root of $F$ ratio between models with and without that parameter. 

- Such $F$ will have 1 numerator degree of freedom (and $n-k-1$ denominator degrees of freedom).
- The analogous $t$-distribution has $n-k-1$ degrees of freedom

???
the t-statistics of the coefficients were tested against t distributions with n-k-1 degrees of freedom accordingly. 

---
# Why no p-values?

In MLM, the distribution of a test statistic when the null hypothesis is true is **unknown.**

???
in multilevel models, the distribution of our test statistics is often unknown. remember that we cannot solve our equations here and find the parameter estimates algebraically to minimise residual sums of squares, but instead we find the best estimates via max likelihood. 

--

Under very specific conditions (normally distributed outcome variable, perfectly balanced designs), we can separate out sums of squares to calculate $F$, and hence can use an $F$ distribution and correctly determine the denominator $df$.  

But for most situations:
  - unclear how to calculate denominator $df$
  - unclear whether the test statistics even follow an $F$ distribution
  
???
In all but the most controlled experimental designs, it is unclear how to calculate the denominator degrees of freedom in order to perform the relevant test. 
  

---
# Options for inference

1. df approximations  

2. Likelihood Ratio Tests  

3. Bootstrap  

???
There are three main approaches to drawing inferences from multilevel models. 


---
count:false
# df approximations

.pull-left[

Loading the package **lmerTest** will fit your models and print the summary with p-values approximated by the Satterthwaite method.
```{r eval=F}
library(lmerTest)
full_model <- lmer(cog ~  1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime, REML = TRUE)
summary(full_model)
```

```
Linear mixed model fit by REML. t-tests use Satterthwaite

 ...
 ...

Random effects:
Groups      Name        Variance Std.Dev. Corr
participant (Intercept) 10.06    3.17         
            visit_n      1.22    1.11     0.69
Residual                37.93    6.16         
Number of obs: 200, groups:  participant, 20

Fixed effects:
           Estimate Std. Error    df t value Pr(>|t|)    
(Intercept)    68.56       1.18 19.00    58.2  < 2e-16 ***
visit_n        -1.22       0.29 19.00    -4.2  0.00048 ***

 ...
```

]

.pull-right[
The **pbkrtest** package implements the slightly more reliable Kenward-Rogers method for model comparison.  
Good for small samples
```{r}
library(pbkrtest)
restricted_model <- lmer(cog ~ 1 + 
                           (1 + visit_n | participant), 
                         data = cogtime, REML = TRUE)
full_model <- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime, REML = TRUE)
KRmodcomp(full_model, restricted_model)
```
]

---
# Likelihood ratio tests

- Compares the log-likelihood of two competing models.  

- remember "likelihood" = a function that associates to a parameter the probability (or probability density) of observing the given sample data. 

- ratio of two likelihoods is **asymptotically** $\chi^2$-square distributed.

    - *this means for small samples (at any "levels") it may be unreliable*

```{r}
restricted_model <- lmer(cog ~ 1 + 
                           (1 + visit_n | participant), 
                         data = cogtime, REML = FALSE)
full_model <- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime, REML = FALSE)
anova(restricted_model, full_model)
```


---
# Bootstrap

- Parametric Bootstrap  
  assumes that explanatory variables are fixed and that model specification and the distributions such as $\zeta_i \sim N(0,\sigma_{\zeta})$ and $\varepsilon_i \sim N(0,\sigma_{\varepsilon})$ are correct.
  
- Case-based Bootstrap  
  minimal assumptions - we just need to ensure that we correctly specify the hierarchical dependency of data.  
  requires decision of at which levels to resample.  
  (discussed more next week)


???

What we are going to focus on here is bootstrapping multilevel models.  
There are various different ways we can bootstrap, which vary in the assumptions they make about our model and about the process that generates our data.  
Today we'll focus on parametric bootstrapping. This assumes that our model is correct, for instance, in that our residuals and our random effect terms are normally distributed. 

---
# Parametric Bootstrap

The basic premise is that we:  

1. fit model(s) to data

2. Do many times:

  - simulate data based on the parameters of the fitted model(s)
  - compute some statistic/estimates from the simulated data

3. Construct a distribution of possible values  

---
# Parametric Bootstrap

.pull-left[
## Confidence Intervals
```{r eval=FALSE}
full_model <- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime)
confint(full_model, method = "boot")
```
```
              2.5 %  97.5 %
.sig01       0.6561  5.4831
.sig02      -0.1509  1.0000
.sig03       0.6630  1.5906
.sigma       5.5275  6.8391
(Intercept) 66.3313 70.8488
visit_n     -1.7774 -0.6318
```
]
.pull-right[
## LRT
(a bootstrapped version of the likelihood ratio test):  
```{r eval=FALSE}
library(pbkrtest)
restricted_model <- lmer(cog ~ 1 + 
                           (1 + visit_n | participant), 
                         data = cogtime)
full_model <- lmer(cog ~ 1 + visit_n + 
                     (1 + visit_n | participant), 
                   data = cogtime)
PBmodcomp(full_model, restricted_model)
```
```
Bootstrap test; time: 79.81 sec; samples: 1000; extremes: 1;
Requested samples: 1000 Used samples: 979 Extremes: 1
large : cog ~ 1 + visit_n + (1 + visit_n | participant)
cog ~ 1 + (1 + visit_n | participant)
       stat df p.value    
LRT    13.1  1 0.00029 ***
PBtest 13.1    0.00204 ** 
---
Signif. codes:  0 *** 0.001 ** 0.01 * 0.05 . 0.1   1
```
]


---
# Summary

- Lots of debate around how best to conduct statistical inferences based on multi-level models. 

- Lots of options:

  - approximations for $df$: _easy to implement. generally reliable. debate as to whether null is actually F. distribution_  
      - fit `lmer()` with package **lmerTest**   
      - `KRmodcomp(mod2, mod1)` from package **pbkrtest**  
      
  - likelihood ratio tests: _very quick, but best avoided unless big $n$ at all levels_  
      - `anova(mod1, mod2)`   
      
  - parametric bootstrap: _time consuming, but probably most reliable option (although can be problematic with unstable models)_  
      - `PBmodcomp(mod2, mod1)` from package **pbkrtest**    
      - `confint(mod, method="boot")`   
      


???
In summary then, making statistical inferences from multilevel models is a difficult, and slightly contentious issue. 
There are various approaches that we can use, many of which have certain drawbacks, such as requiring perfectly balanced designs, or being less reliable with small sample sizes. 
ultimately, the decision is yours to make - you may find conducting standard likelihood ratio tests, using the anova function, are more convenient as they do not take much time to compute, but we would recommend a bootstrapping approach as being worth the extra little bit of time. 


---
class: inverse, center, middle, animated, rotateInDownLeft

# End



<!-- --- -->
<!-- class: inverse, center, middle -->
<!-- exclude: true -->

<!-- <h2 style="text-align: left;opacity:0.3;">Part 1: LM to MLM</h2> -->
<!-- <h2 style="text-align: left;opacity:0.3;">Part 2: Inference in MLM</h2> -->
<!-- <h2>Extra slides (optional): Inference Examples</h2> -->

<!-- ??? -->
<!-- let's look at some examples -->

<!-- --- -->
<!-- exclude: true -->
<!-- # Data -->

<!-- ```{r eval = FALSE} -->
<!-- nursedf <- read_csv("https://uoepsy.github.io/data/nurse_stress.csv") -->
<!-- nursedf <- nursedf %>%  -->
<!--   mutate(across(c(hospital, expcon, gender, wardtype, hospsize), factor)) -->
<!-- head(nursedf) -->
<!-- ``` -->


<!-- _The files nurses.csv contains three-level simulated data from a hypothetical study on stress in hospitals. The data are from nurses working in wards nested within hospitals. It is a cluster-randomized experiment. In each of 25 hospitals, four wards are selected and randomly assigned to an experimental and a control condition. In the experimental condition, a training program is offered to all nurses to cope with job-related stress. After the program is completed, a sample of about 10 nurses from each ward is given a test that measures job-related stress. Additional variables are: nurse age (years), nurse experience (years), nurse gender (0 = male, 1 = female), type of ward (0 = general care, 1 = special care), and hospital size (0 = small, 1 = medium, 2 = large)._   -->
<!-- (From https://multilevel-analysis.sites.uu.nl/datasets/ ) -->

<!-- ??? -->
<!-- our data come from nurses within 25 hospitals and we have information on their age, gender, years of experience, stress levels, and whether they took part in a training program to cope with jobrelated stress.  -->


<!-- --- -->
<!-- exclude: true -->
<!-- # test of a single parameter -->

<!-- > After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much? -->

<!-- model summary:   -->

<!-- ```{r eval = FALSE} -->
<!-- mod1 <- lmer(Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital), data = nursedf) -->
<!-- summary(mod1) -->
<!-- ``` -->

<!-- ??? -->
<!-- we can see our model summary here. -->
<!-- after fitting many multilevel models, you will start to get into the habit of being able to spot when an effect is likely to be significant. -->
<!-- t stats > about 2 will tend to pattern with p < .05. NOTE this isn't a rule to follow or based decisions on.  -->

<!-- --- -->
<!-- exclude: true -->
<!-- # test of a single parameter -->

<!-- > After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much? -->

<!-- __Likelihood Ratio Test:__ -->
<!-- ```{r eval = FALSE} -->
<!-- mod0 <- lmer(Zstress ~ 1 + experien + age + gender + (1 | hospital), data = nursedf) -->
<!-- mod1 <- lmer(Zstress ~ 1 + experien + age + gender + expcon + (1 | hospital), data = nursedf) -->
<!-- anova(mod0, mod1) -->
<!-- ``` -->


<!-- --- -->
<!-- exclude: true -->
<!-- # test of a single parameter -->

<!-- > After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much? -->

<!-- __Parametric Bootstrap__  -->
<!-- ```{r eval = FALSE} -->
<!-- mod0 <- lmer(Zstress ~ 1 + experien + age + gender + (1 | hospital), data = nursedf) -->
<!-- mod1 <- lmer(Zstress ~ 1 + experien + age + gender +  expcon + (1 | hospital), data = nursedf) -->
<!-- PBmodcomp(mod1, mod0) -->
<!-- ``` -->
<!-- ```{r eval = FALSE, echo=F} -->
<!-- pbs = PBmodcomp(mod1, mod0) -->
<!-- pbs  -->
<!-- ``` -->

<!-- ??? -->
<!-- here we fit a reduced model without the IV, and the full model. -->
<!-- the parametric bootstrap indicates an improvement in model fit for the full model, suggesting the attending the program influences nurses' stress levels -->

<!-- --- -->
<!-- exclude: true -->
<!-- # test of a single parameter -->

<!-- > After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, **by how much?** -->

<!-- __Parametric Bootstrap Confidence Intervals__ -->
<!-- ```{r echo=T, eval=F} -->
<!-- mod1 <- lmer(Zstress ~ 1 + experien + age + gender +  expcon + (1 | hospital), data = nursedf) -->
<!-- confint(mod1, method="boot") -->
<!-- ``` -->
<!-- ```{r eval = FALSE, echo=F, highlight.output=c(8)} -->
<!-- cis = confint(mod1, method="boot") -->
<!-- cis -->
<!-- ``` -->

<!-- ??? -->
<!-- we can also construct parametric bootstrap confidence intervals.   -->
<!-- this way we can get an estimate for the value of the fixed effect -->


<!-- --- -->
<!-- exclude: true -->
<!-- # test of a single parameter -->

<!-- > After accounting for nurses' age, gender and experience, does having been offered a training program to cope with job-related stress appear to reduce levels of stress, and if so, by how much? -->

<!-- Attendance of training programs on job-related stress was found to predict stress levels of nurses in ` length(unique(nursedf$hospital))` hospitals, beyond individual nurses' years of experience, age and gender (Parametric Bootstrap Likelihood Ratio Test statistic = ` pbs$test[2,"stat"]`, p` map_chr(pbs$test[2,"p.value"], ~ifelse(.<.001, "<.001", paste0("=",.)))`). Having attended the training program was associated with a decrease in ` fixef(mod1)["expcon1"]` (Bootstrap 95% CI [` paste(round(cis[7,],2), collapse=", ")`] ) standard deviations on the measure of job-related stress. -->

<!-- ??? -->
<!-- allowing us to discuss the effect size - how much it estimates the change in stress.  -->

<!-- --- -->
<!-- exclude: true -->
<!-- # testing that several parameters are simultaneously zero -->

<!-- > Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience?  -->

<!-- __Likelihood Ratio Test__ -->
<!-- ```{r eval = FALSE} -->
<!-- mod0 <- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf) -->
<!-- mod1 <- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf) -->
<!-- anova(mod0, mod1, test="Chisq") -->
<!-- ``` -->

<!-- --- -->
<!-- exclude: true -->
<!-- # testing that several parameters are simultaneously zero -->

<!-- > Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience?  -->

<!-- __Kenward-Rogers $df$-approximation__ -->
<!-- ```{r eval = FALSE} -->
<!-- mod0 <- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf) -->
<!-- mod1 <- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf) -->
<!-- KRmodcomp(mod1, mod0) -->
<!-- ``` -->

<!-- --- -->
<!-- exclude: true -->
<!-- # testing that several parameters are simultaneously zero -->

<!-- > Do ward type and hospital size influence levels of stress in nurses beyond the effects of age, gender, training and experience?  -->

<!-- __Parametric Bootstrap__ -->
<!-- ```{r eval = FALSE} -->
<!-- mod0 <- lmer(Zstress ~ experien + age + gender + expcon + (1 | hospital), data = nursedf) -->
<!-- mod1 <- lmer(Zstress ~ experien + age + gender + expcon + wardtype + hospsize + (1 | hospital), data = nursedf) -->
<!-- PBmodcomp(mod1, mod0) -->
<!-- ``` -->

<!-- ??? -->
<!-- If we are interested in the collective effects of various parameters, such as the influence of workplace-related factors, like wardtype and hospital size, then we can do the same but where the full model has a number of further predictors -->

<!-- --- -->
<!-- exclude: true -->
<!-- # testing random effects  -->

<!-- __are you sure you want to?__ -->

<!-- - Justify the random effect structure based on study design, theory, and practicalities more than tests of significance. -->

<!-- - If needed, the __RLRsim__ package can test a single random effect (e.g. `lm()` vs `lmer()`). -->


<!-- ```{r eval = FALSE} -->
<!-- library(RLRsim) -->
<!-- mod0 <- lm(stress ~ expcon + experien + age + gender + wardtype + hospsize, data = nursedf) -->
<!-- mod1 <- lmer(stress ~ expcon + experien + age + gender + wardtype + hospsize +  -->
<!--                (1 | hospital), data = nursedf) -->
<!-- exactLRT(m = mod1, m0 = mod0) -->
<!-- ``` -->

<!-- ??? -->
<!-- we can also, should we wish to, test the benefit of including our random effect. We would rarely want to do this, because our random effects should be based on our theory and/or study design. But if we really need to - then we can use an exact likelihood ratio test.  -->

<!-- --- -->
<!-- exclude: true -->
<!-- class: inverse, center, middle, animated, rotateInDownLeft -->

<!-- # End  -->

