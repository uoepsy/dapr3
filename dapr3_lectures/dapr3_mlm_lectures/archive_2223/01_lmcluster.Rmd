---
title: "<b>Linear Models and Clustered Data</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    self-contained: true
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
params: 
    show_extra: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)

options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")

xaringanExtra::use_share_again()
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)

knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
theme_set(
    theme_minimal() + 
    theme(text = element_text(size=20))
)
source("jk_source/jk_presfuncs.R")

library(xaringanthemer)
style_mono_accent(
  base_color = "#88B04B", # DAPR3 
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  code_font_size = "0.7rem",
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)
```

class: inverse, center, middle

<h2>Part 1: Linear Regression Refresh</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Clustered Data</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Where we're going</h2>
<h2 style="text-align: left;opacity:0.3;">Extra slides (optional): Other Approaches</h2>

???

Before we get to the main focus of this 5 week block, we're going to start with a refresher of Linear models  

Treat as a sort of high-level overview of DAPR2.

limited time, so we might have to go quickly. but do remember that things like piazza and the lab sessions are there for you to ask any questions, and that inludes on stuff like this 


---

# Models

.pull-left[
__deterministic__  

given the same input, deterministic functions return *exactly* the same output

- $y = mx + c$  

- area of sphere = $4\pi r^2$  

- height of fall = $1/2 g t^2$
    - $g = \textrm{gravitational constant, }9.8m/s^2$
    - $t = \textrm{time (in seconds) of fall}$

]

???
getting straight into it, what all of this is about is making models of the world, that we can use in some way.  

models tend to come in two forms, the deterministic, mathematical model.  
think laws of nature and physics, geometry etc.  

--

.pull-right[
__statistical__  

.br2.f4.white.bg-gray[
$$ 
\textrm{outcome} = (\textrm{model}) + \color{black}{\textrm{error}}
$$
]

- handspan = height + randomness  

- cognitive test score = age + premorbid IQ + ... + randomness

]

???
for us though, we're working with statistical models.  
these are models where there is an overall pattern, but there is randomness.  
i.e. individual variation from the pattern


---
# The Linear Model

.br3.pa2.f2[
$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error} \\
\color{red}{y_i} & = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_i} + \varepsilon_i \\
\text{where } \\
\varepsilon_i & \sim N(0, \sigma) \text{ independently} \\
\end{align}
$$
]

???
the linear model is one such type of model, where an outcome variable y
is modelled as a linear combination of some set of predictor variables x, plus some error term.  
this is what we worked with all last year, y = beta0 + beta1 x + e  

---
# The Linear Model

.flex.items-top[
.w-50.pa2[
Our proposed model of the world:

$\color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_i} + \varepsilon_i$  
  
{{content}}
]
.w-50.pa2[
```{r bb, echo=F, fig.asp=.6}
x <- tibble(x=c(-1,4))
f <- function(x) {5+2*x}
p1 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=f,size=1,colour="blue") +
  geom_segment(aes(x=0,xend=0,y=0,yend=f(0)),colour="blue", lty="dotted") +
  geom_segment(aes(x=0,xend=1,y=f(0),yend=f(0)),colour="blue",linetype="dotted") +
  geom_segment(aes(x=1,y=f(0),xend=1,yend=f(1)),colour="blue",linetype="dotted") +
  annotate("text",x=.5,y=2.5,label=expression(paste(beta[0], " (intercept)")),
           size=5,parse=TRUE,colour="blue") +
  annotate("text",x=1.4,y=6,label=expression(paste(beta[1], " (slope)")),
           size=5,parse=TRUE,colour="blue") +
    ggtitle(expression(paste(beta[0]," = 5, ",beta[1]," = 2")))+
  scale_y_continuous(breaks=0:13)+
  scale_x_continuous(limits = c(-0.3, 4), breaks=0:4)
p1 +
  ggtitle("")+
  scale_y_continuous("y",labels=NULL)+
  scale_x_continuous(limits=c(-0.3,4), breaks=c(0,1), labels=c("0","1"))
  
```
]]

???
in it's simplest form with one continuous predictor, it looks something like this - a straight line.  

--

Our model _fitted_ to some data (note the $\widehat{\textrm{hats}}$):  

$\hat{y}_i = \color{blue}{\hat \beta_0 \cdot{} 1 + \hat \beta_1 \cdot{} x_i}$  

{{content}}

???
the model is our blue line, with model predicted values (use y hat for those)

--

For the $i^{th}$ observation:
  - $\color{red}{y_i}$ is the value we observe for $x_i$   
  - $\hat{y}_i$ is the value the model _predicts_ for $x_i$   
  - $\color{red}{y_i} = \hat{y}_i + \hat\varepsilon_i$  
  
???
and there is variation around that line, these deviations from observed value to model predicted value are our residuals. 

---
# An Example


.flex.items-top[
.w-50.pa2[

$\color{red}{y_i} = \color{blue}{5 \cdot{} 1 + 2 \cdot{} x_i} + \hat\varepsilon_i$  
  
__e.g.__   
for the observation $x_i = 1.2, \; y_i = 9.9$:  

$$
\begin{align}
\color{red}{9.9} & = \color{blue}{5 \cdot{}} 1 + \color{blue}{2 \cdot{}} 1.2 + \hat\varepsilon_i \\
& = 7.4 + \hat\varepsilon_i \\
& = 7.4 + 2.5 \\
\end{align}
$$
]
.w-50.pa2[
```{r errplot,fig.asp=.6,echo=FALSE}
xX <-1.2
yY <- 9.9
p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  geom_point(aes(x=xX,y=yY),size=3,colour="red") +
  geom_segment(aes(x=xX,xend=xX,y=f(xX),yend=yY),linetype="dotted",colour="black") +
  annotate("text",.8,8.6,label=expression(paste(epsilon[i]," (residual)")),colour="black",size=5)
```
]]

???
and the individual observations are going to deviate a little from this due to other stray causes and randomness  

so if our estimate for beta0 is 5 and beta1 is 2, our line is defined by those numbers  
- it hits the y axis at 5
- every 1 it increases in x, it increases by 2 in y

---
# Categorical Predictors

.pull-left[  
```{r echo=FALSE, results="asis"}
set.seed(993)
tibble(
  x = sample(c("Category0","Category1"), size = 30, replace = T),
  y = 5 + 2*(x == "Category1") + rnorm(30,0,1) %>% round(2)
) %>% select(y,x) -> df
cat("<br>")
df %>% sample_n(6) %>% rbind(., c("...","...")) %>% kable()
```
]
.pull-right[
```{r echo=FALSE, fig.asp=.8}

ggplot(df,aes(x=as.numeric(x=="Category1"),y=y))+
  geom_point()+
  stat_summary(geom="point",shape=4,size=6)+
  stat_summary(geom="path", aes(group=1))+
  scale_x_continuous(name="isCategory1",breaks=c(0,1),
                     labels=c("0\n(FALSE)","1\n(TRUE)"))+
  geom_segment(x=0,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category0"]),
               lty="dashed",col="blue")+
  geom_segment(x=1,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category1"]),
               lty="dashed",col="blue")+
  annotate("text",x=0.9,y=6,label=expression(paste(beta[1], " (slope)")),
           size=5,parse=TRUE,colour="blue")+
  labs(title="y ~ x    (x is categorical)")
```
]

???
the linear model is really useful, and can be applied to a range of scenarios. 
categorical predictors get entered into our model as sets of variables of 0s and 1s. we can do some clever things to change what the 0s and 1s represent, but the default is:
- zero as some reference category
- changes of 1 representing changes from that reference to some other category


---
# Multiple Predictors

.pull-left[
```{r echo=FALSE, fig.asp=.8}
mwdata <- read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,main="y~x1+x2")
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

```
]

???
we can have more than one predictor too. rather than fitting a line, our model becomes a 3 dimensional surface fitted to a 3 dimensional cloud of datapoints.  
residuals continue to be deviations from this model to the observations.  

--

.pull-right[
```{r echo=FALSE, fig.asp=.8}
fit<-lm(wellbeing~outdoor_time+social_int+routine, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int, routine = "Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,70), main="y~x1+x2+x3\n(x3 is categorical)")
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int-2, routine = "No Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)
y <- wellbeing
par(new=TRUE)
persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,80), axes=F)


```
]

???
add in another categorical predictor, and we have two surfaces.  
add in another continuous predictor, and i can't visualise it anymore because we're in more than 3 dimensions, but the logic persists.  

---
exclude: true
# Multiple Predictors

.pull-left[
```{r echo=F, out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SSblank1.png")
```
]
.pull-right[]

---
# Multiple Predictors

.pull-left[
```{r echo=F, out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SSblank1cov.png")
```
]
.pull-right[
```{r echo=F}
x1 = rnorm(100)
x2 = rnorm(100)
y = .2*x1 + .3*x2 + rnorm(100)
```

```{r}
lm(y ~ x1)
```

]

???
when we have multiple predictors, what do the coefficients represent?  
one way i like to think of this is with venn diagrams.  
consider the simple model with 1 predictor.  
the circle for y is all of the variance in y, and likewise for x1  

the overlap is the shared variance, or covariance, and the linear model is really just re-expressing that.  

so think of this blue bit as the bit of y that is explained by x1  



---
# Multiple Predictors

.pull-left[
```{r echo=F, out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SStype3.png")
```
]
.pull-right[
```{r}
lm(y ~ x1 + x2)
```

```{r}
resx1.x2 = resid(lm(x1 ~ x2))
lm(y ~ resx1.x2)
```
]


???
enter x2, and the regression coefficients we get are more like the red and blue bits here.  

it's the bit of y explained by x1 _after_ controlling for x2  

we're basically saying that we want to look at the bit of x1 that explains y that is _unique_ to x1 

because x2 will explain a bit of x1

---
# Interactions

.pull-left[
```{r echo=FALSE, fig.asp=.8}
mwdata2<-read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv") %>% mutate(
  isRural = factor(ifelse(location=="rural","rural","notrural"))
)
par(mfrow=c(1,2))
fit<-lm(wellbeing~outdoor_time+isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2], col="red")

fit<-lm(wellbeing~outdoor_time*isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2+x1:x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2]+coef(fit)[4], col="red")
par(mfrow=c(1,1))
```
]

???
we can extend this to include interactions, where the effect of x1 on y _depends on_ x2. 

here we have a continuous by categorical interaction, which is best visualised as two lines that aren't parallel

--

.pull-right[
```{r echo=FALSE, fig.asp=.8}
scs_study <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
fit<-lm(dass ~ scs*zn, data = scs_study)
steps=20
scs <- with(scs_study, seq(min(scs),max(scs),length=steps))
zn <- with(scs_study, seq(min(zn),max(zn),length=steps))
newdat <- expand.grid(scs=scs, zn=zn)
dass <- matrix(predict(fit, newdat), steps, steps)
x1 <- scs
x2 <- zn
y <- dass
p <- persp(x2,x1,y, theta = -89,phi=10, col = NA, main = "y~x1+x2+x1:x2")
```
]

???
if it's continuous by continuous, this is more like a twisted surface.

but the logic is the same, right? imagine we just took the close edge and far edge of the surface - we'd have two non-parallel lines. with the continuous variable it's just that we're also considering the bit in between the lines, because x2 can take any value


---
# Notation

$\begin{align} \color{red}{y} \;\;\;\; & = \;\;\;\;\; \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_1 + ... + \beta_k \cdot x_k} & + & \;\;\;\varepsilon \\ \qquad \\ \color{red}{\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ \vdots \\ y_n \end{bmatrix}} & = \color{blue}{\begin{bmatrix} 1 & x_{11} & x_{21} & \dots & x_{k1} \\ 1 & x_{12} & x_{22} &  & x_{k2} \\ 1 & x_{13} & x_{23} &  & x_{k3} \\ 1 & x_{14} & x_{24} &  & x_{k4} \\ 1 & x_{15} & x_{25} &  & x_{k5} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{1n} & x_{2n} & \dots & x_{kn} \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix}} & + & \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \varepsilon_4 \\ \varepsilon_5 \\ \vdots \\ \varepsilon_n \end{bmatrix} \\ \qquad \\ \\\color{red}{\boldsymbol y} \;\;\;\;\; & = \qquad \qquad \;\;\; \mathbf{\color{blue}{X \qquad \qquad \qquad \;\;\;\: \boldsymbol \beta}} & + & \;\;\; \boldsymbol \varepsilon \\ \end{align}$

???
so the notation we saw a lot of in DAPR2 was this top bit here
y = beta0 + beta1 x +.... + e

again, outcome (red) = model (blue) + error

you might also see it like this, in matrices. 

which are collapsed to look like this. 

- y is a vector of length n
- x is an n by p matrix (p is the number of predictors)
- b is the vector of coefficients. 
- e is the vector of residuals

and regression is just finding those betas that minimise the residuals.  


---
# Link functions

$\begin{align} \color{red}{y} = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & (-\infty, \infty) \\ \qquad \\ \qquad \\ \color{red}{ln \left( \frac{p}{1-p} \right) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & [0,1] \\ \qquad \\ \qquad \\ \color{red}{ln (y) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & (0, \infty) \\ \end{align}$  

???
now this can be extended so that instead of modelling y directly, we model some function of y.  

this is useful for studying outcomes that aren't continuous, so things like binary yes/no outcomes. 

fundamentally, the model is the same, but there are extra bits we need to do when interpreting betas. 

---
# Linear Models in R

- Linear regression
```{r eval=FALSE, echo=TRUE}
linear_model <- lm(continuous_y ~ x1 + x2 + x3*x4, data = df)
```

- Logistic regression
```{r eval=FALSE, echo=TRUE}
logistic_model <- glm(binary_y ~ x1 + x2 + x3*x4, data = df, family=binomial(link="logit"))
```

- Poisson regression
```{r eval=FALSE, echo=TRUE}
poisson_model <- glm(count_y ~ x1 + x2 + x3*x4, data = df, family=poisson(link="log"))
```

???
and we saw how to fit these in R using lm() and glm() with the family argument to specify the distribution


---
# Inference for Coefficients

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum1.png")
```

???
when it came to conducting inferences (i.e. drawing conclusions) from the linear models, it was fairly straightforward because it was all contained in the summary




---
# Inference for Coefficients

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum2.png")
```

???
each coefficient has a standard error, so we can reason about what values for the coefficients we _would_ expect if the true value in the population is zero  


---
# Inference for Coefficients

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum3.png")
```

???
this allows us to calculate a statistic which essentially standardises "how far away from zero is our coefficient, in units of these standard errors"


---
# Inference for Coefficients

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum4.png")
```

???
which in turn we can express as a probability of observing the coefficient _if the true value in the population is zero_  

so a quick reminder, the fundamental idea here of statistics here is:

- we take a sample, and calculate an estimate
- samples will vary, so the estimate could be higher/lower than the true value in the population
- we're asking _if the true value in the population is zero_, what's the probability of observing a sample estimate as extreme as this one? that's the p-value


---
# Sums of Squares

Rather than specific coefficients, we can also think of our model as a whole.  
We can talk in terms of the sums of squared residuals  
$\sum^{n}_{i=1}(y_i - \hat y_i)^2$.  

.pull-left[
```{r echo=FALSE, out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SS3xmodel1.png")
```
]


.pull-right[

```{r echo=F, fig.asp=.8}
set.seed(993)
df <- 
  tibble(
    x1 = rnorm(100)+3,
    y = x1*2 + rnorm(100)
  )

# SST, SSM and SSR
plt_sst = 
  ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=mean(df$y)), lty="dashed",col="red")+
  geom_text(x=1,y=8,label="bar(y)",parse=T, size=6)+
  geom_curve(x=1.1,xend=2,y=8,yend=mean(df$y),curvature=-.2)+
  labs(title="SS Total")

plt_ssr = ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,y=y,yend=fitted(lm(y~x1,df))), lty="dashed",col="red")+
  labs(title="SS Residual")

plt_ssm = ggplot(df, aes(x=x1,y=y))+
  geom_point()+
  geom_hline(yintercept=mean(df$y), lty="dashed")+
  geom_smooth(method=lm,se=F)+
  geom_segment(aes(x=x1,xend=x1,yend=mean(df$y),y=fitted(lm(y~x1,df))), lty="dashed",col="blue")+
  labs(title="SS Model")

plt_sst / (plt_ssm + plt_ssr) & scale_x_continuous("x",breaks=NULL) & scale_y_continuous("y",breaks=NULL)

```

]

???
as well as the coefficients, in DAPR2 we saw that we can think about partitioning up the variance into different parts 

the total sums of squares, the model explained sums of squares, and the residual sums of squares

---
# $R^2$


.pull-left[
```{r echo=F, out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SSr2.png")
```
]
.pull-right[
$R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}$
```{r echo=FALSE}
x1 = rnorm(100)
x2 = rnorm(100)
y = .2*x1 + .3*x2 + rnorm(100)
```
```{r}
mdl <- lm(y ~ x1 + x2)
summary(mdl)$r.squared
```
]

???
so this gave us metrics like the Rsquared, so get a picture of the proportion of variance explained by the model as a whole

so that includes this middle bit of the venn diagram


---
# Inference: Joint tests

```{r echo=F}
set.seed(2345)
tibble(
  x1=rnorm(100),
  x2=rnorm(100),
  x3=rnorm(100),
  grp=rep(letters[1:4],e=25),
  y=10+x1-2*x2+2*x3+(grp=="a")*4 + rnorm(100)
) -> df
```

We can test reduction in residual sums of squares:

.pull-left[
```{r}
m1 <- lm(y ~ x1, data = df)
```
```{r echo=FALSE,  out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SS3xmodel1.png")
```
]
.pull-right[
```{r}
m2 <- lm(y ~ x1 + x2 + x3, data = df)
```
```{r echo=FALSE,  out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SS3xfullmodel.png")
```

]

???
but we also can test reductions in sums of squares, which is essentially a model comparison.

so we compare this model on the left, to the one on the right

---
# Inference: Joint tests

i.e. isolating the improvement in model fit due to inclusion of additional parameters

.pull-left[
```{r}
m1 <- lm(y ~ x1, data = df)
m2 <- lm(y ~ x1 + x2 + x3, data = df)
anova(m1,m2)
```

]
.pull-right[
```{r echo=FALSE,  out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SS3xincrement.png")
```
]

???
and we're isolating this bit here - the improvement in the model due to the inclusion of x2 and x3 combined

---
# Inference: Joint tests

"additional parameters" could be a set of coefficients for levels of a categorical variable.  
This provides a way of assessing "are there differences in group means?".  

.pull-left[
```{r}
m1 = lm(y ~ x1, data = df)
m2 = lm(y ~ x1 + grp, data = df)
coef(m2)
anova(m1, m2)
```

]
.pull-right[
```{r echo=FALSE,  out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SSblankq.png")
```
]

???
one thing this is really useful for is that the additional bit we add to the model might be a load of different groupings.  

so rather than the coefficients which test the specific differences between groups, this joint test can tell us "does the grouping make a difference?"


---
exclude: true
# Inference: Joint tests

This is kind of where traditional analysis of variance sits.  
Think of it as testing the addition of each variable entered in to the model,

.pull-left[
```{r}
m2 = lm(y ~ x1 + grp, data = df)
anova(m2)
```

]
.pull-right[
```{r echo=FALSE,  out.height="350px",out.width="400px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SStype1.png")
```
]

???
this is where "traditional ANOVA" sits. 
we have p categorical predictors with k levels and we assess the reduction in RSS due to inclusion grouping by those k levels


---
# Assumptions

Our model:  

$\color{red}{y} = \color{blue}{\mathbf{X \boldsymbol \beta}} + \varepsilon \qquad \text{where } \boldsymbol \varepsilon \sim N(0, \sigma) \text{ independently}$


Our ability to generalise from the model we fit on sample data to the wider population requires making some _assumptions._

???
Now.. all this inference relies on our model meeting certain assumptions 

--

- assumptions about the nature of the **model** .tr[
(linear)
]

--

- assumptions about the nature of the **errors** .tr[
(normal)
]


.footnote[
You can also phrase the linear model as: $\color{red}{\boldsymbol  y} \sim Normal(\color{blue}{\mathbf{X \boldsymbol \beta}}, \sigma)$
]



---
# Assumptions: The Broad Idea

All our work here is in
aim of making **models of the world**.  

- Models are models. They are simplifications. They are therefore wrong.  

.pull-left[]
.pull-right[
![](jk_img_sandbox/joeymap.jpg)
]


---
count:false
# Assumptions: The Broad Idea

All our work here is in
aim of making **models of the world**.  

- Models are models. They are simplifications. They are therefore wrong.  

- Our residuals ( $y - \hat{y}$ ) reflect everything that we **don't** account for in our model.  


--

- In an ideal world, our model accounts for _all_ the systematic relationships. The leftovers (our residuals) are just random noise.  

--

  - If our model is mis-specified, or we don't measure some systematic relationship, then our residuals will reflect this.

--

- We check by examining how much "like randomness" the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d ("independent and identically distributed")
    - _these ideas tend to get referred to as our "assumptions"_

--

- We will **never** know whether our residuals contain only randomness - we can never observe everything! 


---
# Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  


]
.pull-right[

```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
df<-tibble(x=runif(1000,1,10),xr = round(x), y=1*x+rnorm(1000))
df$y2 <- resid(lm(y~x,df))
#df$y[df$x==6]<-6+rnorm(100,0,3)
df %>% group_by(xr) %>% summarise(m=mean(y2), s=sd(y2)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y")
p2 <- ggplot(df, aes(x=xr,y=y2))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  #geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("residuals")

p1 / p2
```

]

---
# Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- __mean of the residuals = zero across the predicted values on the linear predictor.__    

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  


]
.pull-right[


```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
tibble(x = runif(1000,1,10),
       xr = round(x),
       s = abs(5-x)*2,
       e = map_dbl(s,~rnorm(1,0,1)),
       y = x + s + e) -> df
df$y2 <- resid(lm(y~x,df))
#df$y[df$x==6]<-6+rnorm(100,0,3)
df %>% group_by(xr) %>% summarise(m=mean(y2), s=sd(y2)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y")
p2 <- ggplot(df, aes(x=xr,y=y2))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("residuals")

p1 / p2
```

]

---
# Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- __spread of residuals is normally distributed and constant across the predicted values on the linear predictor.__  


]
.pull-right[

```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
tibble(x = runif(1000,1,10),
       xr = round(x),
       s = abs(x)/2,
       e = map_dbl(s,~rnorm(1,0,.)),
       y = x + e) -> df
df$y2 <- resid(lm(y~x,df))
#df$y[df$x==6]<-6+rnorm(100,0,3)
df %>% group_by(xr) %>% summarise(m=mean(y2), s=sd(y2)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y")
p2 <- ggplot(df, aes(x=xr,y=y2))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("residuals")

p1 / p2
```
]

---
# Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  



]
.pull-right[
__`plot(model)`__

```{r echo=FALSE}
df<-tibble(x=runif(1000,1,10),xr = round(x), y=1*x+rnorm(1000))
```

```{r fig.asp=.8}
my_model <- lm(y ~ x, data = df)
plot(my_model, which = 1)
```

]


---
# Assumptions: Recipe Book
<br><br>
<center>
<div class="acronym">
L
</div> inearity<br>
<div class="acronym">
I
</div> ndependence<br>
<div class="acronym">
N
</div> ormality<br>
<div class="acronym">
E
</div> qual variance<br>
</center>
.footnote["Line without N is a Lie!" (Umberto)]

---
# What if our model doesn't meet assumptions?

- is our model mis-specified?  
  - is the relationship non-linear? higher order terms? (e.g. $y \sim x + x^2$)
  - is there an omitted variable or interaction term? 
  
  
--

- transform the outcome variable?
  - makes things look more "normal"
  - but can make things more tricky to interpret:  
    `lm(y ~ x)` and `lm(log(y) ~ x)` are quite different models

--

- bootstrap
  - do many times: resample (w/ replacement) your data, and refit your model.
  - obtain a distribution of parameter estimate of interest. 
  - compute a confidence interval for estimate
  - celebrate

--

__looking ahead:__ these don't help if we have violated our assumption of independence...

---
# Summary

- we can fit a linear regression model which takes the form $\color{red}{y} = \color{blue}{\mathbf{X} \boldsymbol{\beta}} + \boldsymbol{\varepsilon}$  

- in R, we fit this with `lm(y ~ x1 + .... xk, data = mydata)`.  

- we can extend this to different link functions to model outcome variables which follow different distributions.  

- when drawing inferences from a fitted model to the broader population, we rely on certain assumptions.  

  - one of these is that the errors are independent.


---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Linear Regression Refresh</h2>
<h2>Part 2: Clustered Data</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Where we're going</h2>
<h2 style="text-align: left;opacity:0.3;">Extra slides (optional): Other Approaches</h2>

???
okay. we're now going to move on to the main focus of this first block of dapr3.  
we're going to start by getting to grips with what it means for us to have clustered data  


---
# Clustered Data

.pull-left[
- children within schools  

- patients within clinics  

- observations within individuals  
]
.pull-left[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/h2.png")
```
]

???
what is clustered data?  

simply put - it's when our observations have some grouping.  
note we're not talking about something like an "old vs young" or "drug vs placebo", or "condition1 vs condition2" groups. 
those are typically the things we're wanting to study.  

the _clusters_ are the groupings in our data that are kind of another level of random sampling. 

children schools
patients clinics
observations individuals


---
# Clustered Clustered Data?

.pull-left[
- children within classrooms within schools within districts etc...  

- patients within doctors within hospitals... 

- time-periods within trials within individuals
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/h3.png")
```
]

.footnote[
Other relevant terms you will tend to see: "grouping structure", "levels", "hierarchies". 
]

???
and we can have clusters of clusters too. 

[SLIDE EXAMPLES]

so you can see that this can get quite complicated. Typically, this gets called the grouping structure, or multilevel data, or the hierarchical structure.  



---
# Importance of Clustering

Clustering will likely result in measurements on observational units within a given cluster being more similar to each other than to those in other clusters.  

- For example, our measure of academic performance for children in a given class will tend to be more similar to one another (because of class specific things such as the teacher) than to children in other classes.

```{r echo=FALSE, out.width="60%",fig.align="center"}
knitr::include_graphics("jk_img_sandbox/lev1.png")
```

???
why do we need to think about clustering?  

observations within a cluster are going to be more similar to one another than they are to those in other clusters

for instance, children in a given class might score more similarly to one another than they do to children in other classes, because of things like how good the teacher is

but this like 'non-independence'! the children are not independent from one another.  
call back to our regression assumptions!  

and this happens a lot right?
we've said children and schools, employees in departments etc. 

probably the most relevant for your dissertations, is repeatedly measuring the same participants. So we get 30 people, but we give each one 20 trials, we've suddenly got 500 observations, but it's all clustered.


---
# ICC (intra-class correlation coefficient)

Clustering is expressed in terms of the correlation among the measurements within the same cluster - known as the __intra-class correlation coefficient (ICC).__


There are various formulations of ICC, but the basic principle = ratio of *variance between groups* to *total variance*.  

<br>
$\rho = \frac{\sigma^2_{b}}{\sigma^2_{b} + \sigma^2_e} \\ \qquad \\\textrm{Where:} \\ \sigma^2_{b} = \textrm{variance between clusters} \\ \sigma^2_e = \textrm{variance within clusters (residual variance)} \\$

???
at the outset, we might want to quantify how much clustering there is in our outcome variable.
and for that we have the ICC

basic premise is that this is the ratio of between cluster variance to the total variance  

we'll take a look at this in just a second

--

Can also be interpreted as the correlation between two observations from the same group. 

???
it's also interpreted as the expected correlation between two observations from the same group  

---
# ICC (intra-class correlation coefficient)

The larger the ICC, the lower the variability is within the clusters (relative to the variability between clusters). The greater the correlation between two observations from the same group. 

.pull-left[
```{r echo=FALSE, fig.asp=.8, fig.align="center"}
iccgen <- function(j,n,e,icc,coef=0){
  v = (icc*e)/(1-icc)
  es = e/(v+e)
  v = if(is.infinite(v)){v=e}else{v/(v+e)}
  npj = n/j
  tibble(
    j = letters[1:j],
    zeta_j = rnorm(j,0,sqrt(v))
  ) %>%
    mutate(
      e_ij = map(j, ~rnorm(npj, 0, sqrt(es)))
    ) %>% unnest() %>%
    mutate(
      x = rnorm(n, 10, 5),
      y = 5 + coef*x + zeta_j + e_ij
    )
}
set.seed(3406)
sims = map_dfr(set_names(c(0,.5,.75,.95,.99,1)), 
        ~iccgen(j=10,n=100,e=1,icc=.,coef=0), .id="icc") %>%
  group_by(icc, j) %>%
  mutate(
    m = mean(y)
  ) %>% ungroup

ggplot(sims, aes(x=j, y=y))+
  geom_jitter(height=0, size=2,aes(col=j))+
  scale_y_continuous(NULL, labels=NULL)+
  stat_summary(geom="errorbar",aes(x=j,y=m,col=j),lwd=1)+
  facet_grid(icc~.)+
  guides(col=F)+
  labs(x="cluster") -> p1

# ggplot(sims, aes(x=0, y=y))+
#   see::geom_violinhalf(aes(x=.5))+
#   geom_jitter(height=0,width=.5, size=2,aes(col=j), alpha=.5)+
#   scale_y_continuous(NULL, labels=NULL)+
#   scale_x_continuous(NULL, labels=NULL)+
#   #stat_summary(geom="errorbar",aes(x=j,y=m,col=j),lwd=1)+
#   facet_grid(icc~.)+
#   guides(col=F)-> p2
# p1 + p2 + plot_layout(widths=c(8,1))
p1
```

]

???
so between cluster variance vs total variance.  

[DESCRIBE PLOT]



--

.pull-right[

```{r echo=FALSE, fig.asp=.8, fig.align="center"}
set.seed(875)
sims = map_dfr(set_names(c(0,.5,.75,.95,.99,1)), 
        ~iccgen(j=10,n=100,e=1,icc=.,coef=.1), .id="icc")

ggplot(sims, aes(x=x, y=y))+
  geom_point(aes(col=j))+
  geom_line(aes(col=j))+
  facet_grid(icc~.)+
  guides(col=F)+
  scale_y_continuous(NULL, labels=NULL)+
  scale_x_continuous("X", labels=NULL)
```
]

???
and you could imagine if we were assessing the relationship between some outcome and a predictor, and all of the variance in the outcome is due to the clustering then we'd have this bottom one here

---
# Clustered Data & Linear Models

.pull-left[
#### Why is it a problem?

Clustering is something **systematic** that our model should (arguably) take into account.  

- remember, $\varepsilon \sim N(0, \sigma) \textbf{ independently}$ 

]

--

.pull-right[
#### How is it a problem?  

Standard errors will often be smaller than they should be, meaning that:  

  - confidence intervals will often be too narrow 
  
  - $t$-statistics will often be too large  
  
  - $p$-values will often be misleadingly small

]

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Linear Regression Refresh</h2>
<h2><b style="opacity:0.4;">Part 2: Clustered Data</b><b>A Practical Comment on Data</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Where we're going</h2>
<h2 style="text-align: left;opacity:0.3;">Extra slides (optional): Other Approaches</h2>


---
# Wide Data/Long Data

.pull-left[
__Wide Data__  

```{r echo=FALSE}
tibble(
  ID = sprintf("%03d",1:4), 
  age = rdunif(4, 18, 75),
  trial_1 = c(10, 7.5, 12, 10.5),
  trial_2 = c(12.5, 7, 14.5, 17),
  trial_3 = c(18, 5, 11, 14)
) -> wided
wided %>% rbind(.,"...")
```



]
.pull-right[
__Long Data__


```{r echo=FALSE}
tibble(
  ID = sprintf("%03d",1:4), 
  age = rdunif(4, 18, 75),
  trial_1 = c(10, 7.5, 12, 10.5),
  trial_2 = c(12.5, 7, 14.5, 17),
  trial_3 = c(18, 5, 11, 14)
) -> wided
pivot_longer(wided, 3:5, names_to="trial", values_to="score") -> longd
longd %>% rbind(.,"...")
```


]


---
# Wide Data/Long Data


```{r echo=FALSE}
knitr::include_graphics("https://www.fromthebottomoftheheap.net/assets/img/posts/tidyr-longer-wider.gif")
```
.footnote[
Source: Examples of wide and long representations of the same data. Source: Garrick Aden-Buieâ€™s (\@grrrck) [Tidy Animated Verbs](https://github.com/gadenbuie/tidyexplain)
]

---
# Long Data = Good for Plotting

.pull-left[
__`group` aesthetic__  

```{r fig.asp=.5}
ggplot(longd, aes(x=trial,y=score, group=ID, col=ID))+
  geom_point(size=4)+
  geom_path()
```

]
.pull-right[
__`facet_wrap()`__  
```{r fig.asp=.5}
ggplot(longd, aes(x=trial,y=score))+
  geom_point(size=4)+
  geom_path(aes(group=1))+
  facet_wrap(~ID)
```
]

---
# Long Data = Good for by-Cluster Computations

```{r}
longd %>% 
  group_by(ID) %>%
  summarise(
    ntrials = n_distinct(trial),
    meanscore = mean(score),
    sdscore = sd(score)
  )
```



---
# Summary

- Clustering can take many forms, and exist at many levels  

- Clustering is something systematic that we would want our model to take into account

  - Ignoring it can lead to incorrect statistical inferences

- Clustering is typically assessed using intra-class correlation coefficient (ICC) - the ratio of variance between clusters to the total variance $\rho = \frac{\sigma^2_b}{\sigma^2_b + \sigma^2_e}$

- Tidying your data and converting it to *long* format (one observational unit per row, and a variable identifying the cluster ID) is a good start. 

---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 2


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Linear Regression Refresh</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Clustered Data</h2>
<h2>Part 3: Where we're going</h2>
<h2 style="text-align: left;opacity:0.3;">Extra slides (optional): Other Approaches</h2>

---
# Our Data

.pull-left[
> Sample of 200 pupils from 20 schools completed a survey containing the Emotion Dysregulation Scale (EDS) and the Child Routines Questionnaire (CRQ). 

```{r}
crq_data <- read_csv("https://uoepsy.github.io/data/crqdata.csv")
head(crq_data)
```

```{r}
library(ICC)
ICCbare(x = schoolid, y = emot_dysreg, data = crq_data)
```
]
.pull-right[
```{r echo=FALSE, fig.align="center", fig.asp=.9}
ggplot(crq_data, aes(x=crq ,y=emot_dysreg))+
  geom_point(size=4)+
  geom_smooth(method="lm", se=F)+
  labs(x="",y="") -> p1
ggplot(crq_data, aes(x=crq ,y=emot_dysreg, col=schoolid))+
  geom_point(size=4)+
  geom_smooth(method="lm", se=FALSE)+
  facet_wrap(~schoolid, scales="free_y")+
  guides(col=FALSE)+
  labs(x="childhood routine CRQ",y="emotion dysregulation EDS") +
  theme(text = element_text(size=16))-> p2

p1 / p2 + plot_layout(heights=c(1,2))
```
]

???
data
ICC
plot

---
# Ignoring Clustering

.pull-left[
__(Complete pooling)__  

+ `lm(y ~ 1 + x, data = df)`  

+ Information from all clusters is pooled together to estimate over x  

```{r}
model <- lm(emot_dysreg ~ crq, data = crq_data)
```
```{r echo=FALSE, out.width="300px"}
.pp(summary(model),l=list(c(10:12)))
```
]
.pull-right[
```{r echo=FALSE}
df <- crq_data %>% rename(y=emot_dysreg, x = crq) %>% mutate(cluster_var = gsub("school","cluster",schoolid))
model <- lm(y ~ x, data = df)
df %>% mutate(
  f = fitted(model)
) %>%
ggplot(.,aes(x=x,y=y))+geom_point(size=4)+
  geom_smooth(method="lm")+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.2) + 
  labs(x="childhood routine CRQ",y="emotion dysregulation EDS")
```
]

???
one thing we could do, but i suggest we don't, is simply ignore the clustering.

lm

information from all clusters is pooled together to estimate the relationship

---
# Ignoring Clustering

.pull-left[
__(Complete pooling)__  

+ `lm(y ~ 1 + x, data = df)`  

+ Information from all clusters is pooled together to estimate over x  

```{r}
model <- lm(emot_dysreg ~ crq, data = crq_data)
```
```{r echo=FALSE, out.width="300px"}
.pp(summary(model),l=list(c(10:12)))
```

But different clusters show different patterns.  
Residuals are __not__ independent.  
]
.pull-right[

```{r echo=FALSE}
library(ggfx)
library(ggforce)
model <- lm(y ~ x, data = df)
df %>% mutate(
  f = fitted(model)
) -> pdat 

ggplot(pdat,aes(x=x,y=y))+
  with_blur(geom_point(aes(col=cluster_var),size=4,alpha=.2), sigma = unit(0.7, 'mm')) + 
  geom_point(data = filter(pdat,cluster_var %in% c("cluster1")),aes(col=cluster_var),size=4) + 
  
  geom_mark_ellipse(aes(label = schoolid, filter = cluster_var == "cluster1"),
                    con.colour  = "#526A83", con.cap = 0, 
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) + 
  guides(col=FALSE, alpha=FALSE)+
  geom_smooth(method="lm",se=F)+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.2) + 
  labs(x="childhood routine CRQ",y="emotion dysregulation EDS")
```
]

???
note it doesn't represent the relationship for each school very well 


---
count:false
# Ignoring Clustering

.pull-left[
__(Complete pooling)__  

+ `lm(y ~ 1 + x, data = df)`  

+ Information from all clusters is __pooled__ together to estimate over x  

```{r}
model <- lm(emot_dysreg ~ crq, data = crq_data)
```
```{r echo=FALSE, out.width="300px"}
.pp(summary(model),l=list(c(10:12)))
```

But different clusters show different patterns.  
Residuals are __not__ independent.  
]
.pull-right[

```{r echo=FALSE}
library(ggfx)
library(ggforce)
model <- lm(y ~ x, data = df)
df %>% mutate(
  f = fitted(model)
) -> pdat 

ggplot(pdat,aes(x=x,y=y))+
  with_blur(geom_point(aes(col=cluster_var),size=4,alpha=.2), sigma = unit(0.7, 'mm')) + 
  geom_point(data = filter(pdat,cluster_var %in% c("cluster8")),aes(col=cluster_var),size=4) + 
  
  geom_mark_ellipse(aes(label = schoolid, filter = cluster_var == "cluster8"),
                    con.colour  = "#526A83", con.cap = 0, 
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) + 
  
  
  guides(col=FALSE, alpha=FALSE)+
  geom_smooth(method="lm",se=F)+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.2) + 
  labs(x="childhood routine CRQ",y="emotion dysregulation EDS")
```
]

???
residuals aren't independent, for instance all the residuals from school8 are positive - they're all above the regression line

---
# Fixed Effects Models

.pull-left[

__(No pooling)__  

- `lm(y ~ x * cluster, data = df)`  

- Information from a cluster contributes to estimate *for that cluster*, but information is not pooled (estimate for cluster 1 completely independent from estimate for cluster 2). 

```{r}
model <- lm(emot_dysreg ~ 1 + crq * schoolid, 
            data = crq_data)
```
{{content}}
]
.pull-right[
```{r echo=FALSE}
model1 <- lm(y~x*cluster_var,df)
df %>% mutate(
  f = fitted(model1)
) %>%
ggplot(.,aes(x=x,y=y,col=cluster_var))+
  geom_point()+
  geom_line(aes(y=f,group=cluster_var))+
  geom_smooth(df %>% mutate(f = fitted(model1)) %>% filter(cluster_var == "cluster1"), method="lm", se=F, fullrange = T , mapping=aes(x=x,y=y,col=cluster_var), lty="dashed", lwd=.5) +
  guides(col=FALSE)+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.4) + 
  labs(x="childhood routine CRQ",y="emotion dysregulation EDS") +
  xlim(0,6)
```
]

???
using tools from DAPR2, we might consider adding in schoolid as a predictor into our model 

crq, school, and interaction. 

--

+ Lots of estimates (separate for each cluster). 
+ Variance estimates constructed based on information *only* within each cluster. 

```{r echo=FALSE}
.pp(summary(model),l=list(c(10:34)))
```

???
practical point is that we get out a whole load of coefficients here. 

we have the intercept nad slope for the reference school [PLOT],

and then the adjustments to the intercept for each school

and way down below, the adjustments to the slope for each school

but more importantly, the variance estimates constructed here are based on the information _only_ within each cluster. 

NO pooling


---
# Random Effects Models (MLM)

.pull-left[
__(Partial Pooling)__

- `lmer(y ~ 1 + x + (1 + x| cluster), data = df)`
- cluster-level variance in intercepts and slopes is modeled as randomly distributed around fixed parameters.
- effects are free to vary by cluster, but information from clusters contributes (according to cluster $n$ and outlyingness of cluster) to an overall fixed parameter. 

```{r}
library(lme4)
model <- lmer(emot_dysreg ~ 1 + crq + 
                (1 + crq | schoolid),
              data = crq_data)
summary(model)$coefficients
```


]
.pull-right[

```{r echo=FALSE}
model = lmer(y~x+(1+x|cluster_var),df)
df %>% mutate(
  fit = fitted(model)
) %>%
ggplot(., aes(x=x, y=y))+
  geom_line(aes(y=fit, group=cluster_var, col=cluster_var))+
  geom_abline(intercept=fixef(model)[1], slope=fixef(model)[2],lwd=2)+
  geom_point(aes(col=cluster_var),alpha=.3)+
  geom_segment(aes(y=y, yend = fit, x = x, xend = x, 
                   col=cluster_var), alpha=.3)+
  labs(x="childhood routine CRQ",y="emotion dysregulation EDS")+
  guides(col=FALSE)+
  xlim(0,6)
```
]

???
in between these two approach is multilevel models, also termed "random effects models".  

and relative to the complete pooling (ignore the clustering) and no pooling (explicitly model cluster differences), this gets termed partial pooling.  

in multilevel models, the differences in clusters, in their intercepts and slopes, is modelled as a distribution around some fixed center. 

in this way the information from the clusters is partially pooled to contribute to an overal intercept and slope. 

the extent to which it contributes depends on how big the cluster is, and how far away it is from the rest of them. 

this makes sens right? i'm going to probably pay more attention to a school with 40 students than one with 2?  




---
# Random Effects Models (MLM)

.pull-left[
__(Partial Pooling)__

- `lmer(y ~ 1 + x + (1 + x| cluster), data = df)`
- cluster-level variance in intercepts and slopes is modeled as randomly distributed around fixed parameters.
- effects are free to vary by cluster, but information from clusters contributes (according to cluster $n$ and outlyingness of cluster) to an overall fixed parameter. 

```{r}
library(lme4)
model <- lmer(emot_dysreg ~ 1 + crq + 
                (1 + crq | schoolid),
              data = crq_data)
summary(model)$coefficients
```


]
.pull-right[

```{r echo=FALSE}
library(ggforce)
library(ggfx)
model = lmer(y~x+(1+x|cluster_var),df)
dfm <- 
  df %>% group_by(cluster_var) %>%
  summarise(x = 0) %>% 
  mutate(mf = predict(model, newdata=.),
         ff = fixef(model)[1] + x*fixef(model)[2],
         cv = ifelse(mf>ff, -.2, .2))

dfi <- bind_rows(df,
                 df %>% group_by(cluster_var) %>% summarise(x = 0)) %>%
  mutate(fit2 = predict(model, newdata=.))


df %>% mutate(
  fit = fitted(model)
) -> pdat

dfs <- tibble(
  cluster_var = c("cluster1","cluster6"),
  yf = c(
    coef(model)$cluster_var["cluster1","(Intercept)"]+(3*fixef(model)[2]),
    coef(model)$cluster_var["cluster6","(Intercept)"]+(3*fixef(model)[2])
    ),
  yg = c(
    coef(model)$cluster_var["cluster1","(Intercept)"]+(3*coef(model)$cluster_var["cluster1","x"]), 
    coef(model)$cluster_var["cluster6","(Intercept)"]+(2.7*coef(model)$cluster_var["cluster6","x"])
  )
)

filter(pdat, cluster_var %in% c("cluster1","cluster6")) %>%
  group_by(cluster_var) %>%
  mutate(
    fitmc = mean(fit),
    xgc = x-mean(x),
    fix = fitmc + fixef(model)[2]*xgc,
  ) -> counterf


ggplot(pdat, aes(x=x, y=y))+
  geom_abline(intercept=fixef(model)[1], slope=fixef(model)[2],lwd=2)+
  with_blur(geom_line(aes(y=fit, group=cluster_var, col=cluster_var), alpha=.5), sigma = 2)+
  with_blur(geom_point(aes(col=cluster_var), alpha=.5), sigma = 2)+
  with_blur(geom_segment(aes(y=y, yend = fit, x = x, xend = x, col=cluster_var), alpha=.2), sigma = 2)+
  with_blur(geom_curve(data=dfm[dfm$mf>dfm$ff,], 
             aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),
             curvature = -.5, alpha=.3), sigma = 2)+
  with_blur(geom_curve(data=dfm[dfm$mf<dfm$ff,],
             aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),
             curvature = .5, alpha=.3), sigma = 2)+
  with_blur(geom_line(data=dfi, aes(y=fit2, group=cluster_var, col=cluster_var),
            lty = "dashed", alpha=.3), sigma = 2)+
  
  geom_line(data = filter(pdat, cluster_var %in% c("cluster6")), aes(y=fit, group=cluster_var, col=cluster_var))+
  geom_point(data = filter(pdat, cluster_var %in% c("cluster6")), aes(col=cluster_var))+
  geom_segment(data = filter(pdat, cluster_var %in% c("cluster6")), aes(y=y, yend = fit, x = x, xend = x, col=cluster_var), alpha=.7)+
  geom_line(data=filter(dfi, cluster_var %in% c("cluster6")), aes(y=fit2, group=cluster_var, col=cluster_var),lty = "dashed")+
  geom_curve(data=filter(dfm[dfm$mf>dfm$ff,], cluster_var %in% c("cluster6")),aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),curvature = -.5)+
  geom_curve(data=filter(dfm[dfm$mf<dfm$ff,], cluster_var %in% c("cluster6")),aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),curvature = .5)+
  
  #geom_line(data=counterf, aes(y=fix,group=cluster_var), alpha=.5)+
  # geom_abline(intercept=coef(model)$cluster_var["cluster5","(Intercept)"], slope=fixef(model)[2],
  #             lwd=1,alpha=.2)+
  # geom_curve(data = dfs[1,], aes(x=3,xend=3,y=yf, yend=yg), lwd=.5, curvature=.5, alpha=.3)+
  # geom_abline(intercept=coef(model)$cluster_var["cluster12","(Intercept)"], slope=fixef(model)[2],
  #             lwd=1,alpha=.2)+
  # geom_curve(data = dfs[2,], aes(x=3,xend=2.7,y=yf, yend=yg), lwd=.5, curvature=-.5, alpha=.3)+
  
  
  guides(col = FALSE)+
  labs(x="childhood routine CRQ",y="emotion dysregulation EDS") + 
  # geom_mark_ellipse(aes(label = schoolid, filter = cluster_var == "cluster6"),
  #                   con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
  #                   show.legend = FALSE) +
  #scale_x_continuous(limits=c(0,9), breaks=0:9)+
  #scale_y_continuous(limits=c(0,70))+
  NULL

```
]

???
so in the plot, we have our overall intercept and line, and these are probably going to be the things we're interested in for our research, because we're not likely interested in the schools specifically..

but for each school, it has a deviation from the overall intercept to it's own intercept.

and a deviation from the overall slope to it's own slope.  


---
# Summary

With clustered data, there are many possible approaches. Some of the main ones are:  

- Ignore it (**complete pooling**)  

    - and make inappropriate inferences.  

- Completely partition out any variance due to clustering into fixed effects for each cluster (__no pooling__).  

    - and limit our estimates to being cluster specific and low power.  
    
- Model cluster-level variance as randomly distributed around fixed parameters, __partially pooling__ information across clusters.  

    - best of both worlds?  
    

???
so that's just a taster of what's to come

---
class: inverse, center, middle, animated, rotateInDownLeft

# End

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Linear Regression Refresh</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Clustered Data</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Where we're going</h2>
<h2>Extra slides (optional): Other Approaches</h2>

```{r echo=F}
set.seed(2345)
tibble(
  x1=rnorm(100),
  x2=rnorm(100),
  x3=rnorm(100),
  grp=rep(letters[1:4],e=25),
  y=10+x1-2*x2+2*x3+(grp=="a")*4 + rnorm(100)
) -> df

df2 <- read.table("jk_source/growth.txt", header=T) %>%
  transmute(y = gain, 
            grp = fct_recode(factor(supplement), "horse"="control",
                            "cat"="supergain",
                            "dog"="supersupp",
                            "parrot"="agrimore"), 
            condition = fct_recode(factor(diet),"scotland"="wheat",
                            "england"="oats",
                            "wales"="barley"),
            x3 = rnorm(n())
          )
```

---
# Other: Repeated Measures ANOVA

## ANOVA revisited

.pull-left[
- We've been using `anova()` as a test of whether _several_ parameters are simultaneously zero

    - The "omnibus"/"overall F"/"joint" test (can also be viewed as a model comparison)  
    
]

--

.pull-right[
- Traditional "ANOVA" is essentially a linear model with categorical predictor(s) where it looks for overall differences in group means (or differences in differences in group means etc).    

    - Still quite popular in psychology because we often design experiments with discrete conditions. 
    
    - Require post-hoc tests to examine _where_ any differences are.
]

---
exclude: true
# ANOVA revisited

In R, functions typically create anova tables from linear model objects:
```{r eval=F}
lin_mod <- lm(y ~ grp, df)
anova(lin_mod)
car::Anova(lin_mod)
```
or are wrappers which use the `lm()` function internally. So they're doing the same thing.
```{r eval=F}
summary(aov(y ~ grp, df))
```

```{r echo=FALSE,fig.height=3.5,fig.width=8,fig.align="center"}
ggplot(df,aes(x=grp, y=y,color=grp))+
  geom_jitter(width=.1)+
  stat_summary(fun.y = "mean", 
               fun.ymin = "mean", fun.ymax= "mean", 
               size= 0.3, geom = "crossbar")+
  guides(col="none")
```


---
exclude: true
# ANOVA sums of squares

.pull-left[

With multiple predictors, sums of squares can be calculated differently

1. Sequential Sums of Squares = Order matters
2. <p style="opacity:0.4">Partially Sequential Sums of Squares</p>
3. <p style="opacity:0.4">Partial Sums of Squares</p>

```{r echo=FALSE,  out.height="150px",out.width="200px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SStype1b.png")
```

]
.pull-right[
__sequential SS__
```{r}
anova(lm(y~x1*x2,df))
anova(lm(y~x2*x1,df))
```
]

---
exclude: true
# ANOVA sums of squares

.pull-left[

with multiple predictors, sums of squares can be calculated differently

1. <p style="opacity:0.4">Sequential Sums of Squares</p>
2. <p style="opacity:0.4">Partially Sequential Sums of Squares</p>
3. Partial Sums of Squares = Each one calculated as if its the last one in sequential SS (order doesn't matter).

```{r echo=FALSE,  out.height="150px", out.width="200px", fig.align="center"}
knitr::include_graphics("jk_img_sandbox/SStype3.png")
```


]
.pull-right[
__partial SS__
```{r}
car::Anova(lm(y~x1*x2,df), type="III")
car::Anova(lm(y~x2*x1,df), type="III")
```

]
    
---
# Other: Repeated Measures ANOVA

## ANOVA: Partitioning Variance

```{r fig.asp=.7, echo=FALSE}
knitr::include_graphics("jk_img_sandbox/anova.png")
```


.footnote[  
The terminology here can be a nightmare.  
$SS_{between}$ sometimes gets referred to as $SS_{model}$, $SS_{condition}$, $SS_{regression}$, or $SS_{treatment}$.  
Meanwhile $SS_{residual}$ also gets termed $SS_{error}$.  
To make it all worse, there are inconsistencies in the acronyms used, $SSR$ vs. $RSS$! 

]

---
# Other: Repeated Measures ANOVA

## ANOVA: Partitioning Variance

```{r fig.asp=.7, echo=FALSE}
knitr::include_graphics("jk_img_sandbox/rmanova.png")
```

---
# Other: Repeated Measures ANOVA in R


.pull-left[
```{r echo=FALSE, fig.asp=.9}
set.seed(853)
tibble(
  condition = letters[1:4],
  cmean = c(0,2,3,3),
  ceff = c(.5, 1, 3, .5),
  smeans = map(cmean, ~rnorm(10,.,1)),
  seffs = map(ceff, ~rnorm(10, .,1)),
  sid = map(condition, ~paste0(.,1:10)),
) %>% unnest(c(smeans,sid,seffs)) %>%
  mutate(
    data = map2(smeans,seffs, ~tibble(t = 1:3, y = .y*t+rnorm(3,.x,.1)))
  ) %>% unnest(data) -> df

df$subject = df$sid
df$t = factor(df$t)

ggplot(df, aes(x=t,y=y,col=sid))+
  geom_point()+
  geom_path(aes(group=sid))+
  guides(col=FALSE,fill=FALSE) -> pwithin
ggplot(df,aes(x=condition,y=y))+
  geom_boxplot(aes(group=condition, col=condition),alpha=.3)+
  guides(col=FALSE,fill=FALSE) -> pbetween
pwithin
```
]
.pull-right[
```{r}
library(afex)
aov_ez(id = "subject", dv = "y", data = df, within = "t")
library(ez)
ezANOVA(data = df, dv = y, wid = subject, 
        within = t)
```

]


---
# Other: Mixed ANOVA in R


.pull-left[
```{r echo=FALSE, fig.asp=.9}
pbetween / 
  pwithin + facet_wrap(~condition)
```
]
.pull-right[
```{r}
library(afex)
aov_ez(id = "subject", dv = "y", data = df, 
       between = "condition", within = "t")
library(ez)
ezANOVA(data = df, dv = y, wid = subject, 
        within = t, between = condition)
```
]

---
# Other: Advantages of MLM over ANOVA

- MLM can easily be extended to modelling different types of dependent variables. ANOVA cannot. 

- MLM can be extended to model more complex grouping structures (e.g. children in schools in districts, or independent clusters of both participants as well as experimental items)

- MLM allows us to model an effect as varying 

- MLM provides more options for dealing with unbalanced designs and missing data.  

- Rpt Measures and Mixed ANOVA are special cases of the MLM! You can still get out your ANOVA-style tables from the MLM!  

---
# Other: Cluster Robust Standard Errors

.pull-left[
<!-- https://rlbarter.github.io/Practical-Statistics/2017/05/10/generalized-estimating-equations-gee/ -->

Don't include clustering as part of the model directly, but incorporate the dependency into our residuals term. 

$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error}^* \\
\end{align}
$$
.footnote[`*` Where errors are clustered]

]

.pull-right[
Simply adjusts our standard errors:
```{r}
library(plm)
clm <- plm(emot_dysreg ~ 1 + crq, data=crq_data, 
           model="pooling", index="schoolid")
```
```{r echo=FALSE}
.pp(summary(clm),l=list(c(13:16)))
```
```{r}
sqrt(diag(vcovHC(clm, 
                 method='arellano', 
                 cluster='group')))
```
]

---
# Other: Generalised Estimating Equations 

.pull-left[
Don't include clustering as part of the model directly, but incorporate the dependency into our residuals term. 

$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error}^* \\
\end{align}
$$

  
GEE is a "population average" model:  

- __population average:__ how the _average_ response changes for a 1-unit increase in $x$, while accounting for within-group correlation.   
- __subject specific:__ how the response changes for a group when they increase 1-unit in $x$.  


.footnote[`*` Where errors are clustered, and follow some form of correlational<br>structure within clusters (e.g. based on how many timepoints apart<br>two observations are).]


]

.pull-right[
Specifying a correlational structure for residuals within clusters can influence _what_ we are estimating
```{r}
library(geepack)
# needs to be arranged by cluster, 
# and for cluster to be numeric
crq_data <- 
  crq_data %>%
  mutate(
    cluster_id = as.numeric(as.factor(schoolid))
  ) %>% arrange(cluster_id)
# 
geemod  = geeglm(emot_dysreg ~ 1 + crq, 
                 data = crq_data, 
                 corstr = 'exchangeable',
                 id = cluster_id)
```
```{r echo=FALSE}
.pp(summary(geemod),l=list(c(6:9)))
```

]

???
https://quantscience.rbind.io/2020/12/28/unit-specific-vs-population-average-models/


---
class: inverse, center, middle, animated, rotateInDownLeft

# End

