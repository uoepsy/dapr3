---
title: "<b>Multilevel Model Inference</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: ""
output:
  xaringan::moon_reader:
    self-contained: true
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
params: 
    show_extra: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)

xaringanExtra::use_share_again()
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)
knitr::opts_chunk$set(
  dev = "svg",
  warning = FALSE,
  message = FALSE,
  cache = FALSE,
  fig.asp = .8
)

options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")

theme_set(
    theme_minimal() + 
    theme(text = element_text(size=20))
)

source("jk_source/jk_presfuncs.R")

library(xaringanthemer)

style_mono_accent(
  base_color = "#88B04B", # DAPR3 
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  code_font_size = "0.7rem",
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)
library(lme4)
d3 <- read_csv("https://uoepsy.github.io/data/dapr3_mindfuldecline.csv")
cogtime_model <- lmer(ACE ~ 1 + visit + 
                 (1 + visit | ppt), data = d3)
```

class: inverse, center, middle

<h1 style="text-align: left;">This Lecture:</h1>
<h3 style="text-align: left;">1. Inferential methods with `lmer`</h3>

???
- let's talk about inference
- difficult topic
- we're not going to go into too much depth here, but just focus on a couple of different options, and some considerations we need to make  


---
# <p></p>

you might have noticed...

```{r eval=FALSE}
library(lme4)
d3 <- read_csv("https://uoepsy.github.io/data/dapr3_mindfuldecline.csv")

cogtime_model <- lmer(ACE ~ 1 + visit + (1 + visit | ppt), data = d3)

summary(cogtime_model)
```


.pull-left[

```{r echo=FALSE, highlight.output=c(20,21)}
summary(cogtime_model)
```
]
.pull-right[
![](jk_img_sandbox/wotnop.png)
{{content}}

]

???
- first things first
- anybody notice lack of p values?
- in LM, there was an extra column
  - little stars

--

<br><br><br>
<center><b>Extensive debate about how best to test parameters from MLMs.</b></center>  

???
- lots and lots of debate  



---
# p-values in `lm()`

In simple LM, we test the reduction in residual SS (sums of squares), which follows an $F$ distribution with a known $df$.  

$F$ tests for each predictor will have $k$ numerator degrees of freedom (how many parameters) and $n-k-1$ denominator degrees of freedom.  

The $t$-statistics for coefficients will follow $t$-distributions with the same $n-k-1$ degrees of freedom.

<br> 

$$
\begin{align}
F \qquad & = \qquad \frac{MS_{model}}{MS_{residual}} \qquad &= \qquad \frac{SS_{model}/df_{model}}{SS_{residual}/df_{residual}} \\
\quad \\
& & df_{model} = k \\
& & df_{residual} = n-k-1 \\
\end{align}
$$
???
- in simple regression
- sums squared residuals
- followed F 
- degrees of freedom 
  - how many things are we testing?  
  - how many things are left over free to move around?  
- t tests for coefficients follow same logic. 

---
count: false
exclude: true
# p-values in `lm()`

In simple LM, we test the reduction in residual SS (sums of squares), which follows an $F$ distribution with a known $df$.  

$F$ tests for each predictor will have $k$ numerator degrees of freedom (how many parameters) and $n-k-1$ denominator degrees of freedom.  

The $t$-statistics for coefficients will follow $t$-distributions with the same $n-k-1$ degrees of freedom.

<br> 

$$
\begin{align}
F \qquad & = \qquad  \frac{\text{variance explained by model}/\text{nr model parameters}}{\text{variance left over}/\text{nr residuals that are free to vary}} & \\
\quad \\
& = \frac{\text{avg variance explained per model parameter}}{\text{avg variance explained per residual}} &
\end{align}
$$



???


---
# p-values in `lmer()`?  

- In certain *very specific* conditions, we can work out what the degrees of freedom are. 

- Parameter estimates from these models are not based on minimising sums of squared residuals  

  - They are the ML/REML estimates

  - This is good - it means they can handle unbalanced designs and complex random effect structures

- But: 
  - unclear how to calculate denominator $df$
  - unclear whether the test statistics even follow an $F$ distribution
  
???

- in very specific situations. if we have a perfectly balanced study with the same number of observations in each condition, and same number of obs per cluster etc. 
- we can then calculate the degrees of freedom


- but, even then, the distribution of a test statistic when the null hypothesis is true is **unknown.**
- remember idea of NHST

- in LM, we can solve our equations to find out B. 
- in MLM, we don't. we let computer go brrr. 


---
# Options for inference

<ol>
<li>approximating ddf</li>
  <ul>
  <li>satterthwaite</li>
  <li>kenward-rogers</li>
  <li>m-l-1</li>
  <li>...</li>
  </ul>
<li>likelihood based methods</li>
  <ul>
  <li>profile likelihood confidence interval</li>
  <li>likelihood ratio tests</li>
  </ul>
<li>bootstrap</li>
  <ul>
  <li>parametric</li>
    <ul>
    <li>confidence interval</li>
    <li>likelihood ratio test</li>
    </ul>
  <li>case-based</li>
    <ul>
    <li>confidence interval</li>
    </ul>
  </ul>
</ol>
  
<p>(there are others...)</p>  

???
- all this means: LOADS OF OPTIONS
- all of these are ways of getting either a p-value or a confidence interval
- all will be _slightly_ different from one another  

---
# Options for inference

<ol>
<li>approximating ddf</li>
  <ul>
  <li style="opacity:.4">satterthwaite</li>
  <li>kenward-rogers</li>
  <li style="opacity:.4">m-l-1</li>
  <li style="opacity:.4">...</li>
  </ul>
<li>likelihood based methods</li>
  <ul>
  <li>profile likelihood confidence interval</li>
  <li>likelihood ratio tests</li>
  </ul>
<li style="opacity:.4">bootstrap</li>
  <ul>
  <li style="opacity:.4">parametric</li>
    <ul>
    <li style="opacity:.4">confidence interval: <code>confint(model, method = "boot")</code></li>
    <li style="opacity:.4">likelihood ratio test: <code>pbkrtest::PBmodcomp(full_model, reduced_model)</code></li>
    </ul>
  <li style="opacity:.4">case-based</li>
    <ul>
    <li style="opacity:.4">confidence interval</li>
    </ul>
  </ul>
</ol>
  
<p style="opacity:.4">(there are others...)</p>  

???
- DON'T WORRY
- we're only going to look at a couple


---
# Kenward-Rogers 

- Adjusts standard errors to avoid small sample bias
- Approximates denominator degrees of freedom
- Models must be fitted with REML (which is good for small samples!)  

__fixed effects__  

```{r}
library(parameters)
model_parameters(cogtime_model, ci_method = "kr") # %>% print_html()
```

???
- kenward rogers is a good option for when we have small samples 
- it uses a complicated method of approximating the degrees of freedom
  - so it's still framed as F and t
- adjusts our standard errors to correct for bias from smaller samples
- useful = REML! 

- 'small samples' = ? 
- mainly talking level 2 to be honest

- SLIDE

---
# Kenward-Rogers 


- Adjusts standard errors to avoid small sample bias
- Approximates denominator degrees of freedom
- Models must be fitted with REML (which is good for small samples!)  

__Model Comparison__  

```{r}
restricted_model <- lmer(ACE ~ 1 + (1 + visit | ppt), data = d3, REML = TRUE)
full_model <- lmer(ACE ~ 1 + visit + (1 + visit | ppt), data = d3, REML = TRUE)  

library(pbkrtest)
KRmodcomp(full_model, restricted_model)
```

???
- can also do an F test
- remember JOINT TEST can be useful

---
# Likelihood ratio tests

.pull-left[

- Compares the log-likelihood of two competing models.
- ratio of two likelihoods is **asymptotically** $\chi^2$-square distributed.
    - *this means for small samples (at any level) it may be unreliable*
    - how small is too small?  
        ¯\\\_(ツ)\_/¯ 42? 

]
.pull-right[
__LRTs and ML/REML__  

<ul>
<li>if models differ in fixed effects only, then fit with ML</li> 
<li style="opacity:.6">if models differ in random effects only, then fit with REML (or ML if big N)</li>
<li style="opacity:.6">if models differ in both, then fit with ML</li>
</ul>
`anova()` will automatically re-fit models with ML.  

]
  
<br>
```{r}
restricted_model <- lmer(ACE ~ 1 + (1 + visit | ppt), data = d3, REML = FALSE)
full_model <- lmer(ACE ~ 1 + visit + (1 + visit | ppt), data = d3, REML = FALSE)  

anova(restricted_model, full_model)
```

???
- remember "likelihood" = a function that associates to a parameter the probability (or probability density) of observing the given sample data.
- LRT = two models
  - under which model are we more likely to observe our data?  
- question of how much "more likely under m1" is enough.  
- as N -> inf, then comparing likelihoods follows a chi square  

- but with small N, no! so not a great option for small samples.  
- however, v easy. SLIDE
- DRAW

---
# Likelihood-based CIs

- Evaluates the curvature of the likelihood surface at the estimate.  

  - sharp curvature = more certainty in estimate  
  
  - gradual curvature = less certainty in estimate  
  
- models need to be fitted with ML, not REML  


```{r}
confint(cogtime_model, method="profile")
```


???
- DRAW
if curvature of likelihood function is very sharp at the ML estimates, then changes to the parameter estimates in any direction greatly reduce the likelihood, indicating that there is more certainty that the ML estimates represent the best solution. 

---
# Summary

- Lots of debate around how best to conduct statistical inferences based on multi-level models. 

--

- Lots of options.  

--

|                  | df approximations                                                  | likelihood-based                                                    | 
| ---------------- | ------------------------------------------------------------------ | ------------------------------------------------------------------- | 
| tests or CIs for model parameters | `library(parameters)`<br>`model_parameters(model, ci_method="kr")` | `confint(model, type="profile")`                                    | 
| model comparison<br><small>(different fixed effects, same random effects)</small> | `library(pbkrtest)`<br>`KRmodcomp(model1,model0)`                  | `anova(model0,model)`                                               |
|                  | fit models with `REML=TRUE`.<br>good option for small samples      | fit models with `REML=FALSE`.<br>needs large N at both levels (40+) | 


- check your sample size! but if in doubt, go for KR.  


???
- to summarise
- it's DIFFICULT

- lots of options, with different strengths

- often in practice, use anova for things quickly
- then writing up switch to more robust method if necessary


---
class: inverse, center, middle, animated, rotateInDownLeft

# End

---
class: inverse, center, middle

<h1 style="text-align: left;">This Lecture:</h1>
<h3 style="text-align: left;opacity:.4">1. Inferential methods with `lmer`</h3>
<h3 style="text-align: left; ">Bonus content</h3>

---
# Bootstrap

<ul>
<li>Parametric Bootstrap<br>assumes that explanatory variables are fixed and that model specification and the distributions such as $\zeta_i \sim N(0,\sigma_{\zeta})$ and $\varepsilon_i \sim N(0,\sigma_{\varepsilon})$ are correct.</li><br>
<li style="opacity: .6">Case-based Bootstrap<br>minimal assumptions - we just need to ensure that we correctly specify the hierarchical dependency of data.<br>requires decision of at which levels to resample.<br>(discussed more next week)</li>
</ul>


???

What we are going to focus on here is bootstrapping multilevel models.  
There are various different ways we can bootstrap, which vary in the assumptions they make about our model and about the process that generates our data.  
Today we'll focus on parametric bootstrapping. This assumes that our model is correct, for instance, in that our residuals and our random effect terms are normally distributed. 

---
# Parametric Bootstrap

The basic premise is that we:  

1. fit model(s) to data

2. Do many times:

  - simulate data based on the parameters of the fitted model(s)
  - compute some statistic/estimates from the simulated data

3. Construct a distribution of possible values  

---
# Parametric Bootstrap

.pull-left[
## Confidence Intervals
```{r eval=FALSE}
full_model <- lmer(ACE ~ 1 + visit + 
              (1 + visit | ppt), data = d3)  
confint(full_model, method = "boot")
```
```
              2.5 %  97.5 %
.sig01       0.1130  0.6133
.sig02      -1.0000  0.1028
.sig03       0.2084  0.4225
.sigma       0.4253  0.5542
(Intercept) 85.3443 85.8334
visit       -0.5029 -0.2137
```
]
.pull-right[
## LRT
(a bootstrapped version of the likelihood ratio test):  
```{r eval=FALSE}
restricted_model <- lmer(ACE ~ 1 + 
                    (1 + visit | ppt), data = d3)
full_model <- lmer(ACE ~ 1 + visit + 
              (1 + visit | ppt), data = d3)  

library(pbkrtest)
PBmodcomp(full_model, restricted_model)
```
```
Bootstrap test; time: 71.91 sec; samples: 1000; extremes: 0;
large : ACE ~ 1 + visit + (1 + visit | ppt)
ACE ~ 1 + (1 + visit | ppt)
       stat df  p.value    
LRT    16.3  1 0.000055 ***
PBtest 16.3       0.001 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
]

---
# Adding more options


|                  | df approximations                                                                                                                                                                                                              | likelihood-based                                                                                 | parametric bootstrap                                                                                 |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- |
| model parameters | `library(parameters)`<br>`model_parameters(model, ci_method="kr")`                                                                                                                                                             | `confint(model, type="profile")`                                                                 | `confint(model, type="boot")`                                                                        |
| model comparison | `library(pbkrtest)`<br>`KRmodcomp(model1,model0)`                                                                                                                                                                              | `anova(model0,model)`                                                                            | `library(pbkrtest)`<br>`PBmodcomp(model1,model0)`                                                    |
|                  | for KR, fit models with `REML=TRUE` (a good option for small samples).<br>Other options available (e.g. Satterthwaite, m-l-1, ...)<br>easy to implement, but debate as to whether statistics actually follow an F distribution | if models differ in fixed effects, we cannot use $-2\Delta LL$ when models are fitted with REML. | Time consuming, but probably most reliable option (although can be problematic with unstable models) |

