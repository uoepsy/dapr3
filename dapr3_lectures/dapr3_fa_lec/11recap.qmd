---
title: "Recap!"
author: "Josiah King & John Martindale"
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
library(tidyverse)
library(patchwork)
source('_theme/theme_quarto.R')
```

# Course Overview

```{r}
#| results: "asis"
#| echo: false
block1_name = "multilevel modelling<br>working with group structured data"
block1_lecs = c("regression refresher",
                "introducing multilevel models",
                "more complex groupings",
                "centering, assumptions, and diagnostics",
                "recap")
block2_name = "factor analysis<br>working with multi-item measures"
block2_lecs = c(
  "what is a psychometric test?",
  "using composite scores to simplify data (PCA)",
  "uncovering underlying constructs (EFA)",
  "more EFA",
  "recap & exam prep"
  )

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")

course_table(block1_name,block2_name,block1_lecs,block2_lecs,week=10)
```

```{r}

```










# This week {transition="slide"}

- Lec1: Recap of core concepts
- Lec2: Exam prep session
- Lab: Mock Exam Qs

# Broad ideas 

## multivariate



::::{.columns}
:::{.column width="50%"}
__mixed models/multi-level models__  

- multiple values per cluster
- each value is an observation


```{r}
#| echo: false
tibble(
  person = rep(1:3,e=3),
  y = "...",
  x = "...",
  ` ...`= "..."
) |> gt::gt()
```

:::
:::{.column width="50%"}
__psychometrics__  

- multiple values ($y1, ..., y_k$) representing the same construct
- the set of values is "an observation" of Y

```{r}
#| echo: false
tibble(
  person = c(1:3,"..."),
  y1 = c("..."),
  y2 = c("..."),
  y3 = c("..."),
  ` ...` = c("..."),
) |> gt::gt()
```

:::
::::

## two questions


:::{.incremental}

- Q: To do anything with [construct Y], how do we get one number to represent an observation of Y?  
    - is one number enough - are $y1,y2,...,yk$ really unidimensional?  
    
<br><br>    
    
- Q: How does [set of scores $y1,y2,...,yk$] get at [construct Y]?  
    - is it a reliable measure?
    - are the variables equally representative?
    - is there just one dimension to Y or are there multiple?


:::



## scoring multi-item measures


::::{.columns}
:::{.column width="50%"}
**scale scores**  
add 'em all up, you've got Y  

- clinically 'meaningful'?  
- but only 'meaningful' if underlying model holds (which it almost definitely doesn't!)  
    
:::

:::{.column width="50%"}
**dimension reduction**  
identify dimensions that capture how people co-vary across across the items

- PCA: reduce to set of orthogonal dimensions sequentially capturing most variability

- FA: develop theoretical model of underlying dimensions (possibly correlated) that explain variability in items  

:::
::::


## understanding multi-item measures

::::{.columns}
:::{.column width="50%"}

:::

:::{.column width="50%"}
**dimension reduction**  

:::{style="opacity:.4"}
identify dimensions that capture how people co-vary across across the items

- PCA: reduce to set of orthogonal dimensions sequentially capturing most variability
    
::: 

- FA: develop theoretical model of underlying dimensions (possibly correlated) that explain variability in items  

:::
::::

## understanding multi-item measures

$$
\begin{align}
\text{Outcome} &=& \text{Model} &\quad + \quad& \text{Error} \\
\quad \\
\text{observed correlation} &=& \text{factor loadings and} &\quad + \quad& \text{unique variance for} \\
\text{matrix of items}& &\text{factor correlations} &\quad + \quad& \text{each item} \\
\end{align}
$$

# dimension reduction  

## the idea  

Correlations between times can reflect the extent to which items 'measure the same thing'

::::{.columns}
:::{.column width="50%"}
If items all measure completely different things  

```{r}
#| echo: false
df1 <- tibble(
  V1 = rnorm(100),
  V2 = rnorm(100),
  V3 = rnorm(100),
  V4 = rnorm(100)
) |> apply(2,\(x) as.numeric(cut(x,10))) |> as.data.frame()
names(df1)<-paste0("y",1:4)
df1$text = paste0("y1=",df1$y1,"<br>y2=",df1$y2,"<br>y3=",df1$y3,
                  "<br>y4=",df1$y4)
library(plotly)
plot_ly(df1,
        x = ~y1,
        y = ~y2,
        z = ~y3,
        color = ~y4,
        hovertext = ~text,
        hoverinfo = "text") |> add_markers()
```

:::

:::{.column width="50%"}
If items all measure the same thing perfectly

```{r}
#| echo: false
df <- tibble(
  V1 = rnorm(100),
  V2=V1,
  V3=V1,
  V4=V1
) |> apply(2,\(x) as.numeric(cut(x,10))) |> as.data.frame()
names(df)<-paste0("y",1:4)
df$text = paste0("y1=",df$y1,"<br>y2=",df$y2,"<br>y3=",df$y3,
                 "<br>y4=",df$y4)

plot_ly(df,
        x = ~y1,
        y = ~y2,
        z = ~y3,
        color = ~y4,
        hovertext = ~text,
        hoverinfo = "text") |> add_markers()
```

:::
::::


## the idea

Correlations between times can reflect the extent to which items 'measure the same thing'

::::{.columns}
:::{.column width="50%"}
If items all measure completely different things  

```{r}
#| echo: false
plot_ly(df1,
        x = ~y1,
        y = ~y2,
        z = ~y3,
        color = ~y4,
        hovertext = ~text,
        hoverinfo = "text") |> add_markers()
```

:::

:::{.column width="50%"}
If items all measure the same thing (imperfectly)

```{r}
#| echo: false
set.seed(24)
nitem <- 3
A <- matrix(runif(nitem^2,.41,1)*2-1, ncol=nitem) 
scor <- t(A) %*% A
scor[3,1:2] <- scor[1:2,3] <- scor[1:2,3]/5
df <- MASS::mvrnorm(n=100,mu=rep(0,3),Sigma = scor) |> as.data.frame()
df$V4 = rnorm(100,psych::principal(df,1,rotate="none")$scores[,1],1)
df <- df |> apply(2,\(x) as.numeric(cut(x,10))) |> as.data.frame()
names(df)<-paste0("y",1:4)
df$text = paste0("y1=",df$y1,"<br>y2=",df$y2,"<br>y3=",df$y3,
                 "<br>y4=",df$y4)
plot_ly(df,
        x = ~y1,
        y = ~y2,
        z = ~y3,
        color = ~y4,
        hovertext = ~text,
        hoverinfo = "text") |> add_markers()
```

:::
::::

## the idea


::::{.columns}
:::{.column width="50%"}

- people vary in lots of ways over k variables  
- capture the ways in which people vary.  

```{r}
#| echo: false
set.seed(205)
RR = matrix(c(1,.7,.7,.3,.3,.3,
              .7,1,.7,.3,.3,.3,
              .7,.7,1,.3,.3,.3,
              .3,.3,.3,1,.7,.7,
              .3,.3,.3,.7,1,.7,
              .3,.3,.3,.7,.7,1
), nrow=6)

dff = cbind(
  MASS::mvrnorm(n=100,mu=rep(0,6),Sigma = RR)
) |> as.data.frame()
names(dff) <- paste0("y",1:6)

somedata <- dff
```


:::

:::{.column width="50%"}

```{r}
#| echo: false
pairs(dff)
```

:::
::::

## the idea


::::{.columns}
:::{.column width="50%"}

- people vary in lots of ways over k variables  
- capture the ways in which people vary.  



:::

:::{.column width="50%"}

```{r}
#| echo: false
cor(dff) |> round(2) |> knitr::kable()
```

:::
::::



## dimension reduction - what do we get?

broadly:

- relationships between observed variables and the dimensions  

- amount of variance explained by each dimension

## loadings

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
principal(somedata, nfactors=6, rotate="none")
```
```{r}
#| echo: false
library(psych)
.pp(principal(somedata, nfactors=6, rotate="none"),top=10)
```

:::

:::{.column width="50%"}

$\text{loading}$ = `cor(item, dimension)`

:::
::::

## loadings

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
principal(somedata, nfactors=6, rotate="none")
```
```{r}
#| echo: false
library(psych)
.pp(principal(somedata, nfactors=6, rotate="none"),top=10)
```

:::

:::{.column width="50%"}

$\text{loading}^2$ = $R^2$ from `lm(item ~ dimension)`

![](img_sandbox/loadingvenn/s1.png)

:::
::::

## SSloadings & Variance Accounted for

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
principal(somedata, nfactors=6, rotate="none")
```
```{r}
#| echo: false
library(psych)
.pp(principal(somedata, nfactors=6, rotate="none"),top=13)
```

:::

:::{.column width="50%"}

$\text{SSloading}$ = "sum of squared loadings"  

$R^2$ from `lm(item1 ~ dimension)` + $R^2$ from `lm(item2 ~ dimension)` + $R^2$ from `lm(item3 ~ dimension)` + ....
   
![](img_sandbox/loadingvenn/s2.png)
      
:::
::::

## SSloadings & Variance Accounted for

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
principal(somedata, nfactors=6, rotate="none")
```
```{r}
#| echo: false
library(psych)
.pp(principal(somedata, nfactors=6, rotate="none"),top=15)
```

:::

:::{.column width="50%"}

Total variance = nr of items (e.g., explaining everything would have $R^2=1$ for each item)  

<br>

$\frac{\text{SSloading}}{\text{nr items}}$ = variance accounted for by each dimension
      
:::
::::

## PCA 

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
principal(somedata, nfactors=6, rotate="none")
```
```{r}
#| echo: false
library(psych)
.pp(principal(somedata, nfactors=6, rotate="none"),top=15)
```

:::

:::{.column width="50%"}

- Essentially a calculation 
- Re-expresses $k$ items as $k$ orthogonal dimensions (components) the sequentially capture most variance
- We decide to keep a subset of components based on:  
    - how many things we ultimately want
    - how much variance is captured
- Theory about *what* the dimensions *are* doesn't really matter
      
:::
::::

## conceptual shift to EFA  

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
fa(somedata, nfactors=2, rotate="oblimin", fm="ml")
```
```{r}
#| echo: false
library(psych)
.pp(fa(somedata, nfactors=2, rotate="oblimin", fm="ml"),top=17)
```

:::

:::{.column width="50%"}

- Is a **model** (set of parameters are estimated)
- ~~"variance captured by components"~~ 
- "variance explained by factors"  
- We choose a model that best explains our observed relationships
    - numerically (i.e. distinct factors that each capture something shared across items)
    - theoretically (i.e. factors make sense)
- in psych, PCA is often used as a type of EFA (components are interpreted meaningfully, considered as 'explanatory', and sometimes rotated! In most other fields, PCA is pure reduction


:::
::::

## EFA compared to PCA  

- Pretty much the same idea: captures relations between items and dimensions, and variance explained by dimensions  

- **BUT** - the aim is to *explain*, not just reduce  
    - best explanation might not be two orthogonal dimensions
    - rotations allow factors to be correlated  
    

## EFA output and rotations

<center>
outcome = model + error  
observed covariance = factors + unique item variance
</center>

<br>
- think of a rotation as a transformation applied to the factors  
- it doesn't change the numerical 'fit' of the model, but it changes the interpretation

## EFA output and rotations

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
fa(somedata, nfactors=2, rotate="oblimin", fm="ml")$Structure
```
```{r}
#| echo: false
library(psych)
.pp(fa(somedata, nfactors=2, rotate="oblimin", fm="ml")$Structure |> print(cutoff=0),top=14)
```

:::

:::{.column width="50%"}

Structure matrix show `cor(item, factor)`

- but dimensions are now correlated

![](img_sandbox/loadingvenn/s3.png)

:::
::::

## EFA output and rotations

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
fa(somedata, nfactors=2, rotate="oblimin", fm="ml")
```
```{r}
#| echo: false
library(psych)
.pp(fa(somedata, nfactors=2, rotate="oblimin", fm="ml") |> print(cutoff=0),top=23)
```

:::

:::{.column width="50%"}

Pattern matrix shows: 

- Loadings:  
    - like `lm(item ~ F1 + F2) |> coef()`  

![](img_sandbox/loadingvenn/s4.png)

:::
::::



## EFA output and rotations

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
fa(somedata, nfactors=2, rotate="oblimin", fm="ml")
```
```{r}
#| echo: false
library(psych)
.pp(fa(somedata, nfactors=2, rotate="oblimin", fm="ml") |> print(cutoff=0),top=15)
```

:::

:::{.column width="50%"}

Pattern matrix shows: 

- Communalities & Uniqueness:  
    - $R^2$ and $1-R^2$ from `lm(item ~ F1 + F2)`

![](img_sandbox/loadingvenn/s5.png)

:::
::::

## EFA output and rotations

::::{.columns}
:::{.column width="50%"}
```{r}
#| eval: false
library(psych)
fa(somedata, nfactors=2, rotate="oblimin", fm="ml")
```
```{r}
#| echo: false
library(psych)
.pp(fa(somedata, nfactors=2, rotate="oblimin", fm="ml") |> print(cutoff=0),top=15)
```

:::

:::{.column width="50%"}

SSloadings

- in the structure and the pattern matrix, SSloadings are simply summing the squared values of the columns.  



Variance Accounted For

- trickier because of factor correlations:  
```{r}
fa(somedata, nfactors=2, rotate="oblimin", fm="ml")$Vaccounted
```



:::
::::


## getting scores  

- PCA & EFA allow us to get weighted scores  

```{r}
#| eval: false
principal(somedata, nfactors=6, rotate="none")$scores


efa2 <- fa(somedata, nfactors=2, rotate="oblimin", fm="ml")
factor.scores(somedata, efa2, method = "...")$scores
```

## EFA is an explanatory model 

- Q: To do anything with [construct Y], how do we get one number to represent an observation of Y?  
    - is one number enough - are $y1,y2,...,yk$ really unidimensional?  
    
<br><br>    
    
- Q: How does [set of scores $y1,y2,...,yk$] get at [construct Y]?  
    - is it a reliable measure?
    - are the variables equally representative?
    - is there just one dimension to Y or are there multiple?
    
    

## Q: is it reliable?

**Am I consistently actually *measuring* a thing?**   

::::{.columns}
:::{.column width="50%"}

- this is all necessary because of measurement error  
    - with perfect measurement we would only need one variable

- more measurement error >>> lower reliability  
    - sometimes i'm scored too high, sometimes too low, etc..  noise!
    
- Reliability is a precursor to validity; a test cannot be valid if it is not reliable.  
:::

:::{.column width="50%" .fragment}

- lots of different ways to investigate reliability
    - test-retest
    - parallel forms
    - inter-rater
    - internal consistency (i.e. within a scale)
        - $\alpha$ (assumes equal loadings)
        - $\omega$ (based on factor model)
        
:::
::::


  


## Validity

**Am I measuring the thing I think I'm measuring?**  

- Lots of different types:  
    - face validity
    - content/construct validity
    - convergent validity
    - discriminant validity
    - predictive validity
- some can be assessed through expected relations with other constructs  
- some are assessed through studying the measurement scale and how it is interpreted

# End!  









