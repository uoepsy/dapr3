---
title: "Exploratory Factor Analysis (EFA)"
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---


```{r}
#| label: setup
#| include: false
library(tidyverse)
library(patchwork)
source('_theme/theme_quarto.R')
```


```{r}
#| include: false
library(psych)
set.seed(533)
makeitems <- function(){
  S = runif(5,.4,2)
  f = runif(5,.4,.99)
  R = f %*% t(f)
  diag(R) = 1
  items = round(MASS::mvrnorm(400, mu = rnorm(5,3,.6), Sigma=diag(S)%*%R%*%diag(S)))
  apply(items, 2, function(x) pmin(7,pmax(1,x)))
}
eg_data = do.call(cbind,lapply(1:2, function(x) makeitems()))
eg_data[,5] <- round(rowMeans(eg_data[,c(5,10)]))
eg_data <- eg_data[,-10]
eg_data[,1] <- max(eg_data[,1]) - eg_data[,1] + 1
eg_data[,6] <- max(eg_data[,6]) - eg_data[,6] + 1
eg_data <- as.data.frame(eg_data)
names(eg_data) <- paste0("item_",1:9)

library(lavaan)
set.seed(346)
eg_data <- simulateData(
  "F1 =~ .8*item1 + .7*item2 + .9*item3 + .67*item4 + .5*item5
   F2 =~ .5*item5 + .7*item6 + .9*item7 + .67*item8 + .5*item9
   F1 ~~ .3*F2
  ", sample.nobs= 400
)
eg_data <- as.data.frame(apply(eg_data,2,\(x) as.numeric(cut(x,7))))
names(eg_data) <- paste0("item_",1:9)

# mm = fa(eg_data, nfactors=2, rotate = "oblimin", fm="ml")
# mm

egitems = tibble(
  variable = paste0("item",1:9),
  wording = c("I worry that people will think I'm awkward or strange in social situations.","I often fear that others will criticize me after a social event.","I'm afraid that I will embarrass myself in front of others.","I feel self-conscious in social situations, worrying about how others perceive me.","I often avoid social situations because I’m afraid I will say something wrong or be judged.","I avoid social gatherings because I fear feeling uncomfortable.","I try to stay away from events where I don’t know many people.","I often cancel plans because I feel anxious about being around others.","I prefer to spend time alone rather than in social situations.")
)
```

# Announcements

- Exam date has been released: 
    - When: Saturday 13th December, 9:30-11:30.  
    - Where: Edinburgh International Conference Centre - Lomond Suite


# Course Overview

```{r}
#| results: "asis"
block1_name = "multilevel modelling<br>working with group structured data"
block1_lecs = c("regression refresher",
                "the multilevel model",
                "more complex groupings",
                "centering, assumptions, and diagnostics",
                "recap")
block2_name = "factor analysis<br>working with multi-item measures"
block2_lecs = c(
  "measurement and dimensionality",
  "exploring underlying constructs (EFA)",
  "testing theoretical models (CFA)",
  "reliability and validity",
  "recap & exam prep"
  )

source("https://raw.githubusercontent.com/uoepsy/junk/main/R/course_table.R")
course_table(block1_name,block2_name,block1_lecs,block2_lecs,week=7)
```

## Week 7 TR;DL

1. measurement is hard, especially in psych
2. multi-item measurement tools mean we have lots of variables capturing the same thing
3. in our original variables, lots of dimensions (i.e., lots of columns)
4. our goal in the second half of DAPR3 is to explore ways of expressing a lot of the same information using fewer variables. this is called "dimensionality reduction"
5. one way to reduce dimensionality is to take a scale score  (i.e., mean or sum of all individual scores) 
    - this assumes each item is equally representative of our construct
6. another way is PCA:  
    - finds new dimensions ("components") that:
      - are a weighted sum of original variables: 
      - sequentially capture variability in the data
      - are orthogonal (uncorrelated)
	- we can reduce dimensionality by taking the first $k$ components
7. a constant reminder: getting some numbers is not the same thing as measurement  


# This week {transition="slide"}

- EFA vs PCA
- EFA in R
- Item Suitability, Factor Extraction Methods
- Rotations & Simple Structures
- Evaluating Factor Solutions


# EFA vs PCA

*Real friends don't let friends do PCA.* (W. Revelle, 25 October 2020)

## Questions to ask before you start

::: columns
::: {.column width="50%"}
### PCA

-   Why are your variables correlated?
    -   Agnostic/don't care
-   What are your goals?
    -   Reduce reduce reduce!  
        
:::
::: {.column width="50%" .fragment}
### EFA

-   Why are your variables correlated?
    -   Believe there exist underlying "causes" of these correlations
-   What are your goals?
    -   Reduce, but also **learn** about/model their underlying
        (latent) causes
        

:::
:::

## "Latent variables"?


- Still just "dimensions" but.. 

- **Theorized common cause** of responses to a set of variables

    -   *Explain* correlations between measured variables
    -   Held to be real (can never test this)

## graphical thinking

::::{.columns}
:::{.column width="50%"}

![](img_sandbox/quickdiags/Slide1.png)
:::
:::{.column width="50%" .fragment}

![](img_sandbox/quickdiags/Slide2.png)
:::
::::

## PCA versus EFA: How are they different?

::: columns
::: {.column width="50%"}
**PCA**

- The observed measures are independent variables
- The component is a bit like a dependent variable (it's really just a composite!)
- Components **sequentially** capture as much variance in the measures as possible
- Components are determinate

:::

::: {.column width="50%"}
**EFA**

- The observed measures are dependent variables
- The factor is the independent variable
- Models the relationships between variables  
- Factors are *in*determinate


:::
:::

## As a diagram (PCA)

![](img_sandbox/diag_pca.png)

## As a diagram (PCA)

![](img_sandbox/diag_pca2.png)

## As a diagram (EFA)


![](img_sandbox/diag_efa1.png)

## Debates!   

Put these into two groups - those you feel more comfortable with conceptualising as latent variables and those you don't:  
(no right/wrong answers)  

:::woo
https://app.wooclap.com/events/ZFTQYR/
::: 

<br>


- Anxiety 
- Depression
- Exposure to distressing events
- Trust
- Socioeconomic Status
- Motivation
- Identity


## EFA: a model of observed relationships

- We have some observed variables that are correlated

- EFA tries to explain these patterns of correlations

- Aim is that the correlations between items _after removing the effect of the Factor_ are zero

```{r}
#| echo: false
tibble(
  variable = paste0("Q",1:3),
  wording = c(
    "I think cake is the best food",
    "I feel great when I eat cake",
    "I often eat cake"
  )
) |> gt::gt()
```

$$
\begin{align}
\rho(Q_{1},Q_{2} | \textrm{Love-Of-Cake})=0 \\
\rho(Q_{1},Q_{3} | \textrm{Love-Of-Cake})=0 \\
\rho(Q_{2},Q_{3} | \textrm{Love-Of-Cake})=0 \\
\end{align}
$$



## Sources of variance  

- In order to model these correlations, EFA looks to distinguish between common and unique variance.  

$$
\begin{equation}
var(\text{total}) = var(\text{common}) + \underbrace{var(\text{specific}) + var(\text{error})}_{\text{unique variance}}
\end{equation}
$$

<br>
 
| | | |
|-|-|-|
|common variance | (co)variance shared across items | true and shared | 
|specific variance | variance specific to an item that is not shared with any other items | true and unique | 
| error variance | variance due to measurement error | not 'true', unique |





## Optional: general factor model equation

$$\mathbf{\Sigma}=\mathbf{\Lambda}\mathbf{\Phi}\mathbf{\Lambda'}+\mathbf{\Psi}$$

-   $\mathbf{\Sigma}$: A $p \times p$ observed correlation matrix (from data)

-   $\mathbf{\Lambda}$: A $p \times m$ matrix of factor loadings (relates the $m$ factors to the $p$ items)

-   $\mathbf{\Phi}$: An $m \times m$ matrix of correlations between factors ("goes away" with orthogonal factors)

-   $\mathbf{\Psi}$: A diagonal matrix with $p$ elements indicating unique (error) variance for each item
    

## Optional: general factor model equation

```{r}
#| include: false
set.seed(34567)
RR = matrix(c(1,.6,.6,.6,
              .6,1,.6,.6,
              .6,.6,1,.6,
              .6,.6,.6,1), nrow=4)
tdf = MASS::mvrnorm(2e2,mu=c(0,0,0,0),Sigma=RR)
cor(tdf) |> round(2)

ll = fa(tdf, nfactors=1,rotate="none")$loadings[,1]
uu = fa(tdf, nfactors=1,rotate="none")$uniqueness

ll %*% t(ll) + diag(uu)

```

$$
\begin{align}
\text{Outcome} &= \quad\quad\quad\text{Model} &+ \text{Error} \quad\quad\quad\quad\quad \\
\quad \\
\mathbf{\Sigma} &= \quad\quad\quad\mathbf{\Lambda}\mathbf{\Lambda'} &+ \mathbf{\Psi} \quad\quad\quad\quad\quad\quad  \\
\quad \\
\begin{bmatrix}
1 & 0.53 & 0.57 & 0.57 \\
0.53 & 1 & 0.54 & 0.56 \\
0.57 & 0.54 & 1 & 0.56 \\
0.57 & 0.56 & 0.56 & 1 \\
\end{bmatrix} &= 
\begin{bmatrix}
0.747 \\
0.725 \\ 
0.746 \\ 
0.764 \\
\end{bmatrix}
\begin{bmatrix}
0.747 & 0.725 & 0.746 & 0.764 \\
\end{bmatrix} &+
\begin{bmatrix}
0.44 & 0 & 0 & 0 \\
0 & 0.47 & 0 & 0 \\
0 & 0 & 0.44 & 0 \\
0 & 0 & 0 & 0.42  \\
\end{bmatrix} \\
\quad \\
\begin{bmatrix}
1 & 0.53 & 0.57 & 0.57 \\
0.53 & 1 & 0.54 & 0.56 \\
0.57 & 0.54 & 1 & 0.56 \\
0.57 & 0.56 & 0.56 & 1 \\
\end{bmatrix} &= 
\begin{bmatrix}
0.56 & 0.54 & 0.56 & 0.57 \\
0.54 & 0.53 & 0.54 & 0.55 \\
0.56 & 0.54 & 0.56 & 0.57 \\
0.57 & 0.55 & 0.57 & 0.58 \\
\end{bmatrix} &+
\begin{bmatrix}
0.44 & 0 & 0 & 0 \\
0 & 0.47 & 0 & 0 \\
0 & 0 & 0.44 & 0 \\
0 & 0 & 0 & 0.42  \\
\end{bmatrix} \\
\end{align}
$$



## We make assumptions when we use models

- As EFA is a model, just like linear models and other statistical tools, using it requires us to make some assumptions:

    1.  The error terms are uncorrelated
    2.  The residuals/errors are not correlated with the factor
    3.  Relationships between items and factors should be linear, although there are models that can account for nonlinear relationships
        
## We make assumptions when we use models (2)

![](img_sandbox/diag_efa1.png)





# What does an EFA look like?

```{r}
#| eval: false
nF=2 #number of factors
nV=10 #number of variables

Psi<-matrix(nrow=nF, ncol=nF,     # the nF by nF factor correlation matrix
            data=c(1.00,0.00,
                   0.00,1.00),byrow=T)


Lambda<-matrix(nrow=nV, ncol=nF,  # the nV by nF factor loading matrix
                      #F1    F2
               data=c(0.70, 0.10, # item1
                      0.80, 0.08, # item2
                      0.70, 0.06, # item3
                      0.65, 0.10, # item4
                      0.84, 0.04, # item5
                      0.01, 0.65, # item6
                      0.10, 0.88, # item7
                      0.03, 0.90, #item8
                      0.10, 0.67,  #item9
                      0.02, 0.70), #item10
                byrow=T)


Theta<-matrix(nrow=nV, ncol=nV, # the nV by nV residual matrix
            #item1 item2 item3 item4 item5 item6 item7 item8 item9 item10
      data=c(1-0.70^2-0.10^2, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, #item1
             0.00, 1-0.80^2-0.08^2, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, #item2
             0.00, 0.00, 1-0.70^2-0.06^2, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, #item3
             0.00, 0.00, 0.00, 1-0.65^2-0.10^2, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, #item4
             0.00, 0.00, 0.00, 0.00, 1-0.84^2-0.04^2, 0.00, 0.00, 0.00, 0.00, 0.00, #item5
             0.00, 0.00, 0.00, 0.00, 0.00, 1-0.01^2-0.65^2, 0.00, 0.00, 0.00, 0.00, #item6
             0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1-0.10^2-0.88^2, 0.00, 0.00, 0.00, #item7
             0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1-0.03^2-0.90^2, 0.00, 0.00, #item8
             0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1-0.10^2-0.67^2, 0.00, #item9
             0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1-0.02^2-0.70^2), #item10
      byrow=T) 


#compute correlation matrix from Psi, Lambda and Theta

Sigma<-Lambda%*%Psi%*%t(Lambda)+Theta
#simulate data
library(MASS)
agg.items<-as.data.frame(mvrnorm(n=1000, mu=rep(0,10), Sigma=Sigma))
names(agg.items)<-c('item1','item2','item3','item4','item5','item6','item7','item8','item9','item10')

agg.items <- agg.items %>%
  mutate(
    item1 = as.numeric(cut(agg.items$item1, breaks = 6, labels = c(1:6))), 
    item2 = as.numeric(cut(agg.items$item2, breaks = 6, labels = c(1:6))), 
    item3 = as.numeric(cut(agg.items$item3, breaks = 6, labels = c(1:6))), 
    item4 = as.numeric(cut(agg.items$item4, breaks = 6, labels = c(1:6))), 
    item5 = as.numeric(cut(agg.items$item5, breaks = 6, labels = c(1:6))), 
    item6 = as.numeric(cut(agg.items$item6, breaks = 6, labels = c(1:6))), 
    item7 = as.numeric(cut(agg.items$item7, breaks = 6, labels = c(1:6))), 
    item8 = as.numeric(cut(agg.items$item8, breaks = 6, labels = c(1:6))), 
    item9 = as.numeric(cut(agg.items$item9, breaks = 6, labels = c(1:6))), 
    item10 = as.numeric(cut(agg.items$item10, breaks = 6, labels = c(1:6))), 
  )
```


## Some data

::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
cor(eg_data) |>
  heatmap()
```
:::
:::{.column width="50%"}
```{r}
#| echo: false
gt::gt(egitems)
```
:::
::::

## EFA in R

::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
library(psych)
myfa <- fa(eg_data, nfactors = 2, 
           fm = "ml", rotate = "oblimin")
myfa
```
:::
:::{.column width="50%"}
```{r}
#| echo: false
gt::gt(egitems)
```
:::
::::

## What does an EFA look like?


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: true
library(psych)
myfa <- fa(eg_data, nfactors = 2, 
           fm = "ml", rotate = "oblimin")
myfa
```
:::
:::{.column width="50%" style="font-size:.8em"}

-   **Factor loadings**, like PCA loadings, show the relationship of each measured variable to each factor.

    - Range between -1.00 and 1.00
    
    - Larger absolute values = stronger relationship between measured variable and factor

    - **Primary loadings**: refer to the factor on which a measured variable has it's highest loading
    - **Cross-loadings**: refer to all other factor loadings for a given measured variable

    - Square of a factor loading tells us how much item variance is explained by a factor

:::
::::



# Doing EFA - Overview {background-color="white"}

So how do we move from data and correlations to a factor analysis?

1. check suitability of items.
2. decide on factor extraction method and rotation.  
3. examine plausible number of factors.
4. based on 3, choose the range to examine from $n_{min}$ factors to $n_{max}$ factors.
5. do EFA, extracting from $n_{min}$ to $n_{max}$ factors. Compare each of these 'solutions' in terms of structure, variance explained, and --- by examining how the factors from each solution relate to the observed items --- assess how much theoretical sense they make.  
6. _if the aim is to develop a scale for future use_ - consider removing "problematic" items and start over again.


# Item Suitability, Factor Extraction {background-color="white"}

1. check suitability of items.
2. decide on factor extraction method<span style="opacity:.4"> and rotation.</span>  
3. <p style="opacity:.4">examine plausible number of factors.</p>  
4. <p style="opacity:.4">based on 3, choose the range to examine from $n_{min}$ factors to $n_{max}$ factors.</p>  
5. <p style="opacity:.4">do EFA, extracting from $n_{min}$ to $n_{max}$ factors. Compare each of these 'solutions' in terms of structure, variance explained, and --- by examining how the factors from each solution relate to the observed items --- assess how much theoretical sense they make.</p>  
6. <p style="opacity:.4">_if the aim is to develop a scale for future use_ - consider removing "problematic" items and start over again.</p>

## Data suitability

In short "is the data correlated?".  

- check correlation matrix (ideally roughly \> .20)
- we can take this a step further and calculate the squared multiple correlations (SMC) 
    - regress each item on all other items (e.g., $R^2$ for item1 ~ all other items)
    - tells us how much shared variation there is between an item and all other items
    
- there are also some statistical tests (e.g. Bartlett's test) and metrics (KMO adequacy)

## Extraction Methods

- For PCA, we discussed the use of the eigen-decomposition
    - this isn't estimation, this is just a calculation

- For EFA, we have a model (with error), so we need to **estimate** the model parameters (the factor loadings)


## Extraction Methods (2)  

- Maximum Likelihood Estimation (ml)
- Principal Axis Factoring (paf)
- Minimum Residuals (minres)

## Maximum likelihood estimation

Find values for the parameters that maximize the likelihood of obtaining the observed correlation matrix  


::::{.columns}
:::{.column width="50%"}
Pros: 

- quick and easy, very generalisable estimation method
- we can get various "fit" statistics (useful for model comparisons) 

:::

:::{.column width="50%"}
Cons: 

- Assumes a normal distribution
- Sometimes fails to converge
- Sometimes produces solutions with impossible values
    - Factor loadings $> 1$ (Heywood cases)
    - Factor correlations $> 1$

:::
::::

## Non-continuous data

- Sometimes (often) even when we assume a construct is continuous, we measure it with a **discrete** scale.

- E.g., Likert!  

- Simulation studies tend to suggest $\geq 5$ response categories can be treated as continuous
    - **provided that they have all been used!!**  

```{r}
#| echo: false
set.seed(34)
data.frame(
  item1 = as.numeric(cut(rnorm(200),7)),
  item2 = as.numeric(cut(rnorm(200),3))+2,
  item3 = as.numeric(cut(rnorm(200),7)),
  item4 = as.numeric(cut(rnorm(200),7))
) |>
  psych::multi.hist()
```


## Non-continuous data

```{r, echo=FALSE}
magick::image_read("img_sandbox/response.png")
```

## Polychoric Correlations

::::{.columns}
:::{.column width="50%"}  

- Estimates of the correlation between two theorized normally distributed continuous variables, based on their observed ordinal manifestations.  

```{r}
#| echo: false

set.seed(444)
df <- tibble(
  item1 = rnorm(1e3),
  item2 = .6*item1 + rnorm(1e3)
)
df <- as.data.frame(apply(df,2,scale))
df2 <- apply(df,2,function(x) cut(x,3,labels=c("disagree","neither","agree")))
names(df) <- paste0("l",names(df))
df <- cbind(df,df2)


thresh1 <- df |> group_by(item1) |>
  summarise(
    min = min(litem1),
    max = max(litem1),
    lab = max-((max-min)/2)
  )

thresh2 <- df |> group_by(item2) |>
  summarise(
    min = min(litem2),
    max = max(litem2),
    lab = max-((max-min)/2)
  )

as_tibble(df2) |>
  mutate(
    item2 = factor(item2,levels=c("agree","neither","disagree")),
    item1 = factor(item1,levels=c("disagree","neither","agree"))
  ) |>
  dplyr::select(item2,item1) |>
  table() |> knitr::kable()

df3 <- df2 |> as_tibble() |>
  mutate(
    item1 = ifelse(item1 == "disagree",1,
                   ifelse(item1=="neither",2,3)),
    item2 = ifelse(item2 == "disagree",1,
                   ifelse(item2=="neither",2,3))
  )

```

:::{.fragment}
- For factor analysis with polychoric correlations:  
```{r}
#| echo: true
#| eval: false
fa(data, nfactors = ??, fm = ??, rotate = ??, cor = "poly")
```

- If you just want to see the correlation estimates: `polychoric()` from the **psych** package 



:::



:::

:::{.column width="50%"}
```{r}
#| echo: false
ggplot(df,aes(x=litem1,y=litem2,col=interaction(item1,item2)))+
  geom_point(size=3,alpha=.5)+
  guides(col="none") +
  geom_vline(data=thresh1[-2,], aes(xintercept=min),size=1)+
  geom_hline(data=thresh2[-2,], aes(yintercept=min),size=1)+
  labs(x="underlying agreement with item 1",
       y="underlying agreement with item 2")+
  geom_text(data=thresh1,inherit.aes=F,aes(x=lab,y=-3,label=item1))+
  geom_text(data=thresh2,inherit.aes=F,aes(y=lab,x=-3.3,label=item2),angle=90)
```
:::
::::


## Choosing an extraction method

-   The straightforward option, as with many statistical models, is ML.

-   If ML solutions fail to converge, principal axis is a simple
    approach which typically yields reliable results.

-   If concerns over the distribution of variables, use PAF on the
    polychoric correlations.


# Factor rotation & Simple Structures {background-color="white"}

1. check suitability of items
2. decide on factor extraction method and rotation  
3. <p style="opacity:.4">examine plausible number of factors</p>  
4. <p style="opacity:.4">based on 3, choose the range to examine from $n_{min}$ factors to $n_{max}$ factors</p>  
5. <p style="opacity:.4">do EFA, extracting from $n_{min}$ to $n_{max}$ factors. Compare each of these 'solutions' in terms of structure, variance explained, and --- by examining how the factors from each solution relate to the observed items --- assess how much theoretical sense they make.</p>  
6. <p style="opacity:.4">_if the aim is to develop a scale for future use_ - consider removing "problematic" items and start over again.</p>

```{r}
#| echo: false
set.seed(63239)
eg2 = lavaan::simulateData(
  "l1 =~ -.8*item1+.7*item2+.85*item3+.9*item4+.8*item5
  l2 =~ -.8*item6+.7*item7+.85*item8+.9*item9+.8*item10
  l1~~.7*l2
  l1~~1*l1
  l2~~1*l2"
)
eg2[,5] <- rowMeans(eg2[,c(5,10)])
eg2 <- apply(eg2, 2, \(x) as.numeric(cut(x,7))) |> as.data.frame()
eg2 <- eg2[,-10]

mn = fa(eg2, nfactors=2, rotate = "none", fm="ml")
mr = fa(eg2, nfactors=2, rotate = "varimax", fm="ml")
mor = fa(eg2, nfactors=2, rotate = "oblimin", fm="ml")

x_axis <- c(1, 0)
y_axis <- c(0, 1)
new_x_axis <- mr$rot.mat %*% x_axis
new_y_axis <- mr$rot.mat %*% y_axis
newo_x_axis <- (mor$rot.mat %*% mor$Phi) %*% x_axis
newo_y_axis <- (mor$rot.mat %*% mor$Phi) %*% y_axis
original_axes <- data.frame(
  x = c(0, x_axis[1], 0, y_axis[1]),
  y = c(0, x_axis[2], 0, y_axis[2]),
  axis = c("Original X", "Original X", "Original Y", "Original Y")
)
rotated_axes <- data.frame(
  x = c(0, new_x_axis[1], 0, new_y_axis[1]),
  y = c(0, new_x_axis[2], 0, new_y_axis[2]),
  axis = c("Rotated X", "Rotated X", "Rotated Y", "Rotated Y")
)
orotated_axes <- data.frame(
  x = c(0, newo_x_axis[1], 0, newo_y_axis[1]),
  y = c(0, newo_x_axis[2], 0, newo_y_axis[2]),
  axis = c("Rotated X", "Rotated X", "Rotated Y", "Rotated Y")
)

p1 <- mn$loadings[,1:2] |>
  as.data.frame() |>
  rownames_to_column() |>
  ggplot(aes(x=ML1,y=ML2))+
  geom_point()+
  geom_vline(xintercept=0,size=1)+
  geom_hline(yintercept=0,size=1)+
  geom_text(aes(label=rowname),hjust=-.2)+
  # geom_segment(aes(xend=0,yend=ML2),lty="dashed",
  #              alpha=.6)+
  # geom_segment(aes(yend=0,xend=ML1),lty="dashed",
  #              alpha=.6)+
  labs(x="Loadings on Factor 1",
       y="Loadings on Factor 2")+
  xlim(-1,1)+ylim(-1,1)+
  geom_segment(data = original_axes, aes(x = 0, y = 0, xend = x, yend = y, color = axis), 
               arrow = arrow(length = unit(0.2, "cm")), size = 1) +
  geom_segment(data = original_axes, aes(x = 0, y = 0, xend = -x, yend = -y, color = axis), 
               arrow = arrow(length = unit(0.2, "cm")), size = 1)

p2 <- mn$loadings[,1:2] |>
  as.data.frame() |>
  rownames_to_column() |>
  ggplot(aes(x=ML1,y=ML2))+
  geom_point()+
  geom_vline(xintercept=0,size=1)+
  geom_hline(yintercept=0,size=1)+
  geom_text(aes(label=rowname),hjust=-.2)+
  # geom_segment(aes(xend=0,yend=ML2),lty="dashed",
  #              alpha=.3)+
  # geom_segment(aes(yend=0,xend=ML1),lty="dashed",
  #              alpha=.3)+
  xlim(-1,1)+ylim(-1,1)+
  geom_segment(data = original_axes, aes(x = 0, y = 0, xend = x, yend = y, color = axis), 
               arrow = arrow(length = unit(0.2, "cm")), size = 1) +
  geom_segment(data = original_axes, aes(x = 0, y = 0, xend = -x, yend = -y, color = axis), 
               arrow = arrow(length = unit(0.2, "cm")), size = 1) +
  geom_segment(data = rotated_axes, aes(x = 0, y = 0, xend = x, yend = y, color = axis), 
               linetype = "dashed", arrow = arrow(length = unit(0.2, "cm")), size = 1) +
  geom_segment(data = rotated_axes, aes(x = 0, y = 0, xend = -x, yend = -y, color = axis), 
               linetype = "dashed", arrow = arrow(length = unit(0.2, "cm")), size = 1)

p3 <- mn$loadings[,1:2] |>
  as.data.frame() |>
  rownames_to_column() |>
  ggplot(aes(x=ML1,y=ML2))+
  geom_point()+
  geom_vline(xintercept=0,size=1)+
  geom_hline(yintercept=0,size=1)+
  geom_text(aes(label=rowname),hjust=-.2)+
  # geom_segment(aes(xend=0,yend=ML2),lty="dashed",
  #              alpha=.3)+
  # geom_segment(aes(yend=0,xend=ML1),lty="dashed",
  #              alpha=.3)+
  labs(x="Loadings on Factor 1",
       y="Loadings on Factor 2")+
  xlim(-1,1)+ylim(-1,1)+
    geom_segment(data = original_axes, aes(x = 0, y = 0, xend = x, yend = y, color = axis), 
               arrow = arrow(length = unit(0.2, "cm")), size = 1) +
  geom_segment(data = original_axes, aes(x = 0, y = 0, xend = -x, yend = -y, color = axis), 
               arrow = arrow(length = unit(0.2, "cm")), size = 1) +
  geom_segment(data = orotated_axes, aes(x = 0, y = 0, xend = x, yend = y, color = axis), 
               linetype = "dashed", arrow = arrow(length = unit(0.2, "cm")), size = 1) +
  geom_segment(data = orotated_axes, aes(x = 0, y = 0, xend = -x, yend = -y, color = axis), 
               linetype = "dashed", arrow = arrow(length = unit(0.2, "cm")), size = 1) 

```

## What is rotation?  

Factor solutions can sometimes be complex to interpret.

-   the pattern of the factor loadings is not clear.
-   The difference between the primary and cross-loadings is small


::::{.columns}
:::{.column width="50%"}
```{r}
#| echo: false
#| fig-asp: 1
p1 + theme(legend.position="bottom")
```
:::
:::{.column width="50%"}
```{r}
fa(eg2, nfactors=2, rotate = "none", fm="ml")$loadings
```
:::
::::

## Types of rotation

```{r}
#| eval: false
# no rotation
fa(eg_data, nfactors = 2, rotate = "none", fm="ml")
# orthogonal rotations
fa(eg_data, nfactors = 2, rotate = "varimax", fm="ml")
fa(eg_data, nfactors = 2, rotate = "quartimax", fm="ml")
# oblique rotations
fa(eg_data, nfactors = 2, rotate = "oblimin", fm="ml")
fa(eg_data, nfactors = 2, rotate = "promax", fm="ml")
```

::::{.columns}
:::{.column width="50%"}
__Orthogonal__  

```{r}
#| echo: false
#| fig-asp: 1
p2 + theme(legend.position="bottom")
```

:::

:::{.column width="50%" .fragment}
__Oblique__  

```{r}
#| echo: false
#| fig-asp: 1
p3 + theme(legend.position="bottom")
```
:::
::::

## Why rotate?

-   Factor rotation is an approach to clarifying the relationships
    between items and factors.

    -   Rotation aims to maximize the relationship of a measured item
        with a factor.
    -   That is, make the primary loading big and cross-loadings small.

![](img_sandbox/diag_efa2.png)



## Rotational Indeterminacy


**Rotational indeterminacy** means that there are an infinite number of pairs of factor loadings and factor score matrices which will fit the data **equally well**, and are thus **indistinguishable** by any numeric criteria

$$\mathbf{\Sigma}=\mathbf{\Lambda}\color{orange}{\mathbf{\Phi}}\mathbf{\Lambda'}+\mathbf{\Psi}$$

There is no **unique solution** to the factor problem
    
We can not numerically tell rotated solutions apart, so **theoretical coherence** of the solution plays a big role!  
    
<!-- ## The impact of rotation -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- **Original correlations** -->

<!-- ```{r, echo=FALSE, warning=FALSE} -->
<!-- library(qgraph) -->
<!-- bfi2 <- na.omit(bfi) -->
<!-- qgraph(cor(bfi2[1:25])) -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- **EFA with no rotation and 5 factors** -->

<!-- ```{r, echo=FALSE, warning=FALSE} -->
<!-- no_rotate <- fa(bfi2, 5, rotate="none") -->
<!-- qgraph(no_rotate$loadings, minimum = 0.2) -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

<!-- ## The impact of rotation -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- **EFA with no rotation and 5 factors** -->

<!-- ```{r, echo=FALSE, warning=FALSE} -->
<!-- no_rotate <- fa(bfi2, 5, rotate="none") -->
<!-- qgraph(no_rotate$loadings, minimum = 0.2) -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- **EFA with orthogonal rotation and 5 factors** -->

<!-- ```{r, echo=FALSE, warning=FALSE} -->
<!-- orth_rotate <- fa(bfi2, 5, rotate="varimax") -->
<!-- qgraph(orth_rotate$loadings, minimum = 0.2) -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

<!-- ## The impact of rotation -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- **EFA with orthogonal rotation and 5 factors** -->

<!-- ```{r, echo=FALSE, warning=FALSE} -->
<!-- qgraph(orth_rotate$loadings, minimum = 0.2) -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- **EFA with oblique rotation and 5 factors** -->

<!-- ```{r, echo=FALSE, warning=FALSE} -->
<!-- obl_rotate <- fa(bfi2, 5, rotate="oblimin") -->
<!-- qgraph(obl_rotate$loadings, minimum = 0.2) -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

## How do I choose which rotation?

Easy recommendation: always to choose oblique.

Why?

- It is very unlikely factors have correlations of 0
- If they are close to zero, this is allowed within oblique rotation
- The whole approach is exploratory, and the constraint is unnecessary.

- However, there is a catch...


## Interpretation and oblique rotation

-   When we have an obliquely rotated solution, we need to draw a distinction between the **pattern** and **structure** matrix.  

- For orthogonal rotations, these are identical  

```{r}
#| echo: false
myfa <- fa(eg_data, nfactors = 2, fm = "ml", rotate = "oblimin")
```
::::{.columns}
:::{.column width="50%"}
__Pattern Matrix__  

matrix of regression weights (loadings) from factors to variables  
$item1 = \lambda_1 Factor1 + \lambda_2 Factor2 + u_{item1}$
```{r}
myfa$loadings
```

:::

:::{.column width="50%"}
__Structure Matrix__  

matrix of correlations between factors and variables.  
$cor(item1, Factor1)$
```{r}
myfa$Structure
```

:::
::::




:::notes
- When we use oblique rotation, the structure matrix is the pattern matrix multiplied by the factor correlations.

- In most situations, it doesn't impact what we do, but it's important to be aware of.  
:::

<!-- ## Simple structure -->

<!-- Adapted from Sass and Schmitt (2011): -->

<!-- 1.  Each variable (row) should have at least one zero loading -->
<!-- 2.  Each factor (column) should have same number of zero’s as there are factors -->
<!-- 3.  Every pair of factors (columns) should have several variables which load on one factor, but not the other -->
<!-- 4.  Whenever more than four factors are extracted, each pair of factors (columns) should have a large proportion of variables which do not load on either factor -->
<!-- 5.  Every pair of factors should have few variables which load on both factors -->


# How many factors? {background-color="white"} 

1. check suitability of items
2. decide on factor extraction method and rotation    
3. examine plausible number of factors  
4. based on 3, choose the range to examine from $n_{min}$ factors to $n_{max}$ factors  
5. <p style="opacity:.4">do EFA, extracting from $n_{min}$ to $n_{max}$ factors. Compare each of these 'solutions' in terms of structure, variance explained, and --- by examining how the factors from each solution relate to the observed items --- assess how much theoretical sense they make.</p>  
6. <p style="opacity:.4">_if the aim is to develop a scale for future use_ - consider removing "problematic" items and start over again.</p>

## How many factors? 

::: columns
::: {.column width="50%"}

-   Scree plots
-   Parallel Analysis
-   MAP
```{r}
#| echo: true
#| eval: false
scree(eg_data)

fa.parallel(eg_data)

VSS(eg_data, plot = FALSE)
```

<br>
 
But... if there's no strong steer, then we want a **range.**   

- Treat MAP as a minimum
- PA as a maximum
- Explore all solutions in this range and select the one that yields the best numerically and theoretically.

:::

::: {.column .fragment width="50%"}

```{r}
#| echo: false
par(mfrow=c(1,2))
scree(eg_data)
fa.parallel(eg_data)
```
```
The Velicer MAP achieves a minimum of 0.03  with  1  factors 
```

:::
:::



# Evaluating factor solutions {background-color="white"}

1. check suitability of items
2. decide on factor extraction method and rotation.  
3. examine plausible number of factors  
4. based on 3, choose the range to examine from $n_{min}$ factors to $n_{max}$ factors  
5. do EFA, extracting from $n_{min}$ to $n_{max}$ factors. Compare each of these 'solutions' in terms of structure, variance explained, and --- by examining how the factors from each solution relate to the observed items --- assess how much theoretical sense they make.  
6. <p style="opacity:.4">_if the aim is to develop a scale for future use_ - consider removing "problematic" items and start over again.</p>

## The EFA output - loading matrix 

![](img_sandbox/efa_output/Slide1.png)

## The EFA output - loading matrix (2)

![](img_sandbox/efa_output/Slide2.png)

## The EFA output - communalities  

![](img_sandbox/efa_output/Slide3.png)

## The EFA output - uniqueness

![](img_sandbox/efa_output/Slide4.png)

## The EFA output - complexity

![](img_sandbox/efa_output/Slide5.png)

## The EFA output - variance accounted for

![](img_sandbox/efa_output/Slide6.png)

## The EFA output - variance accounted for (2)

![](img_sandbox/efa_output/Slide7.png)

## The EFA output - variance accounted for (3)

![](img_sandbox/efa_output/Slide8.png)

## The EFA output - variance accounted for (4)

![](img_sandbox/efa_output/Slide9.png)

## The EFA output - factor correlations

![](img_sandbox/efa_output/Slide10.png)

<!-- ## The EFA output - other things (OPTIONAL) -->

<!-- ![](img_sandbox/efa_output/Slide11.png) -->

<!-- ## The EFA output - other things (OPTIONAL) -->

<!-- ![](img_sandbox/efa_output/Slide12.png) -->

<!-- ## The EFA output - other things (OPTIONAL) -->

<!-- ![](img_sandbox/efa_output/Slide13.png) -->


## Evaluating factor solutions - where to start

+ variance accounted for
    + in total (field dependent)
    + each factor (relative to one another)

+ salient loadings
    + meaning of factors is based on size and sign of 'salient' loadings
    + we decide what is 'salient'
    + in most research this is $\ge|.3|$ or $\ge|.4|$

+ Each factor has $\geq 3$ salient loadings (ideally $\geq 3$ *primary* loadings)
    + if not, may have extracted too many factors
    

## Evaluating factor solutions - looking for trouble

+ Items with no salient loadings?
    + maybe a problem item, which should be removed
    + maybe signal presence of another factor

+ Items with multiple salient loadings (cross-loadings)?
    + look at item complexity values.
    + makes defining the factors more difficult

+ Heywood cases
    + factor loadings $\geq |1|$
    + communalities $\geq |1|$
    + something is **wrong**; we do not trust these results  
    + Try different rotation, estimation method, eliminate items, rethink if FA is what you actually want to do


## Evaluating factor solutions - list of criteria

- how much variance is accounted for by a solution?
- do all factors load on 3+ items at a salient level?  
- do all items have at least one loading at a salient level?  
- are there any highly complex items?  
- are there any "Heywood cases" (communalities or standardised loadings that are >1)?  

- **is the factor structure (items that load on to each factor) coherent, and does it make theoretical sense?**
    

## Evaluating factor solutions - cautions!  

**Remember**:
If we choose to delete one or more items, we must start back at the beginning, and go back to determining how many factors to extract

<br>

**Very Important**: 
If one or more factors don't make sense, then either the items are bad, the theory is bad, the analysis is bad, or all three are bad!  

<br>

&#128169; The "garbage in garbage out" principle always applies

- PCA and factor analysis cannot turn bad data into good data


## Our example

::::{.columns}
:::{.column width="32%"}
```{r}
#| echo: true
fa1 <- fa(eg_data, nfactors=1, fm="ml")
fa1$loadings
```

:::
:::{.column width="2%"}
:::
:::{.column width="32%"}
```{r}
#| echo: true
fa2 <- fa(eg_data, nfactors=2, rotate = "oblimin", fm="ml")
fa2$loadings
fa2$Phi
```

:::
:::{.column width="2%"}
:::
:::{.column width="32%"}
```{r}
#| echo: true
fa3 <- fa(eg_data, nfactors=3, rotate = "oblimin", fm="ml")
fa3$loadings
fa3$Phi
```

:::
::::


## Our example (2)


::::{.columns}
:::{.column width="50%"}

```{r}
#| echo: true
eg_data <- eg_data |> select(-item_5)
```


:::
:::{.column width="50%"}

```{r}
#| echo: false
par(mfrow=c(1,2))
scree(eg_data)
fa.parallel(eg_data)
```
```
The Velicer MAP achieves a minimum of 0.04  with  1  factors 
```
:::
::::






## Our example (3)

::::{.columns}
:::{.column width="32%"}
```{r}
#| echo: true
fa1 <- fa(eg_data, nfactors=1, fm="ml")
fa1$loadings
```

:::
:::{.column width="2%"}
:::
:::{.column width="32%"}
```{r}
#| echo: true
fa2 <- fa(eg_data, nfactors=2, rotate = "oblimin", fm="ml")
fa2$loadings
fa2$Phi
```

:::
:::{.column width="2%"}
:::
:::{.column width="32%"}
```{r}
#| echo: true
fa3 <- fa(eg_data, nfactors=3, rotate = "oblimin", fm="ml")
fa3$loadings
fa3$Phi
```

:::
::::

## Our example (4)


::::{.columns}
:::{.column width="50%"}
```{r}
library(gt)
gt(egitems) |>
  tab_style(
    style = cell_text(color = "grey"),
    locations = cells_body(rows = 5)
  )
```

:::
:::{.column width="50%"}
```{r}
#| echo: true
fa2 <- fa(eg_data, nfactors=2, rotate = "oblimin", fm="ml")
fa2$loadings
fa2$Phi
```
:::
::::


