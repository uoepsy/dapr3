---
title: "<b>Linear Models and Clustered Data</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King, Umberto Noè, Tom Booth"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: "AY 2021-2022"
output:
  xaringan::moon_reader:
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
params: 
    show_extra: false
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_tile_view()
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)
xaringanExtra::use_share_again()
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)
knitr::opts_chunk$set(
  dev = "png",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
themedapr3 = function(){
  theme_minimal() + 
    theme(text = element_text(size=20))
}
source("jk_source/jk_presfuncs.R")
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  # base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  code_font_size = "0.7rem",
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)
```


---
class: inverse, center, middle

<h2>Part 1: Linear Regression Refresh</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Clustered Data</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Possible Approaches</h2>
<h2 style="text-align: left;opacity:0.3;">Extra: ANOVA & Repeated Measures (brief)</h2>

---

# Models

.pull-left[
__deterministic__  

given the same input, deterministic functions return *exactly* the same output

- $y = mx + c$  

- area of sphere = $4\pi r^2$  

- height of fall = $1/2 g t^2$
    - $g = \textrm{gravitational constant, }9.8m/s^2$
    - $t = \textrm{time (in seconds) of fall}$

]

--

.pull-right[
__statistical__  

.br2.f4.white.bg-gray[
$$ 
\textrm{outcome} = (\textrm{model}) + \color{black}{\textrm{error}}
$$
]

- handspan = height + randomness  

- cognitive test score = age + premorbid IQ + ... + randomness

]

---
# The Linear Model

.br3.pa2.f2[
$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error} \\
\color{red}{y_i} & = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_i} + \varepsilon_i \\
\text{where } \\
\varepsilon_i & \sim N(0, \sigma) \text{ independently} \\
\end{align}
$$
]

---
# Model structure

.flex.items-top[
.w-50.pa2[
Our proposed model of the world:

$\color{red}{y_i} = \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_i} + \varepsilon_i$  
  
{{content}}
]
.w-50.pa2[
```{r bb, echo=F, fig.asp=.6}
x <- tibble(x=c(-1,4))
f <- function(x) {5+2*x}
p1 <- x %>% ggplot(aes(x=x)) +
  stat_function(fun=f,size=1,colour="blue") +
  geom_segment(aes(x=0,xend=0,y=0,yend=f(0)),colour="blue", lty="dotted") +
  geom_segment(aes(x=0,xend=1,y=f(0),yend=f(0)),colour="blue",linetype="dotted") +
  geom_segment(aes(x=1,y=f(0),xend=1,yend=f(1)),colour="blue",linetype="dotted") +
  annotate("text",x=.5,y=2.5,label=expression(paste(beta[0], " (intercept)")),
           size=5,parse=TRUE,colour="blue") +
  annotate("text",x=1.4,y=6,label=expression(paste(beta[1], " (slope)")),
           size=5,parse=TRUE,colour="blue") +
    ggtitle(expression(paste(beta[0]," = 5, ",beta[1]," = 2")))+
  scale_y_continuous(breaks=0:13)+
  scale_x_continuous(limits = c(-0.3, 4), breaks=0:4)
p1 +
  ggtitle("")+
  scale_y_continuous("y",labels=NULL)+
  scale_x_continuous(limits=c(-0.3,4), breaks=c(0,1), labels=c("0","1"))+
  themedapr3()
  
```
]]

--

Our model _fitted_ to some data (note the $\widehat{\textrm{hats}}$):  

$\hat{y}_i = \color{blue}{\hat \beta_0 \cdot{} 1 + \hat \beta_1 \cdot{} x_i}$  

{{content}}

--

For the $i^{th}$ observation:
  - $\color{red}{y_i}$ is the value we observe for $x_i$   
  - $\hat{y}_i$ is the value the model _predicts_ for $x_i$   
  - $\color{red}{y_i} = \hat{y}_i + \varepsilon_i$  

---
# An Example



.flex.items-top[
.w-50.pa2[

$\color{red}{y_i} = \color{blue}{5 \cdot{} 1 + 2 \cdot{} x_i} + \varepsilon_i$  
  
{{content}}
]
.w-50.pa2[
```{r errplot,fig.asp=.6,echo=FALSE}
xX <-1.2
yY <- 9.9
p1 + ylab(expression(paste(hat(y)," = ",5 %.% 1 + 2 %.% x))) +
  geom_point(aes(x=xX,y=yY),size=3,colour="red") +
  geom_segment(aes(x=xX,xend=xX,y=f(xX),yend=yY),linetype="dotted",colour="black") +
  annotate("text",.8,8.6,label=expression(paste(epsilon[i]," (residual)")),colour="black",size=5)+
  #scale_y_continuous(labels=NULL)+
  themedapr3()
```
]]

--

__e.g.__   
for the observation $x_i = 1.2, \; y_i = 9.9$:  


$$
\begin{align}
\color{red}{9.9} & = \color{blue}{5 \cdot{}} 1 + \color{blue}{2 \cdot{}} 1.2 + \varepsilon_i \\
& = 7.4 + \varepsilon_i \\
& = 7.4 + 2.5 \\
\end{align}
$$

---
# Extending the linear model
## Categorical Predictors

.pull-left[  
```{r echo=FALSE, results="asis"}
set.seed(993)
tibble(
  x = sample(c("Category0","Category1"), size = 30, replace = T),
  y = 5 + 2*(x == "Category1") + rnorm(30,0,1) %>% round(2)
) %>% select(y,x) -> df
cat("<br>")
df %>% sample_n(6) %>% rbind(., c("...","...")) %>% kable()
```
]
.pull-right[
```{r echo=FALSE, fig.asp=.8}

ggplot(df,aes(x=as.numeric(x=="Category1"),y=y))+
  geom_point()+
  stat_summary(geom="point")+
  stat_summary(geom="path", aes(group=1))+
  scale_x_continuous(name="isCategory1",breaks=c(0,1),
                     labels=c("0\n(FALSE)","1\n(TRUE)"))+
  geom_segment(x=0,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category0"]),
               lty="dashed",col="blue")+
  geom_segment(x=1,xend=1,y=mean(df$y[df$x=="Category0"]),yend=mean(df$y[df$x=="Category1"]),
               lty="dashed",col="blue")+
  annotate("text",x=0.9,y=6,label=expression(paste(beta[1], " (slope)")),
           size=5,parse=TRUE,colour="blue")+
  labs(title="y ~ x    (x is categorical)")+
  themedapr3()
```
]



---

# Extending the linear model
## Multiple predictors

.pull-left[
```{r echo=FALSE, fig.asp=.8}
mwdata <- read_csv(file = "https://uoepsy.github.io/data/wellbeing.csv")
fit<-lm(wellbeing~outdoor_time+social_int, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int)
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,main="y~x1+x2")
obs <- with(mwdata, trans3d(outdoor_time,social_int, wellbeing, p))
pred <- with(mwdata, trans3d(outdoor_time, social_int, fitted(fit), p))
points(obs, col = "red", pch = 16)
#points(pred, col = "blue", pch = 16)
segments(obs$x, obs$y, pred$x, pred$y)

```
]

--

.pull-right[
```{r echo=FALSE, fig.asp=.8}
fit<-lm(wellbeing~outdoor_time+social_int+routine, data=mwdata)
steps=20
outdoor_time <- with(mwdata, seq(min(outdoor_time),max(outdoor_time),length=steps))
social_int <- with(mwdata, seq(min(social_int),max(social_int),length=steps))
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int, routine = "Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)

x1 <- outdoor_time
x2 <- social_int
y <- wellbeing

p <- persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,70), main="y~x1+x2+x3\n(x3 is categorical)")
newdat <- expand.grid(outdoor_time=outdoor_time, social_int=social_int-2, routine = "No Routine")
wellbeing <- matrix(predict(fit, newdat), steps, steps)
y <- wellbeing
par(new=TRUE)
persp(x1,x2,y, theta = 35,phi=10, col = NA,zlim=c(10,80), axes=F)


```
]

---

# Extending the linear model
## Interactions

.pull-left[
```{r echo=FALSE, fig.asp=.8}
scs_study <- read_csv("https://uoepsy.github.io/data/scs_study.csv")
fit<-lm(dass ~ scs*zn, data = scs_study)
steps=20
scs <- with(scs_study, seq(min(scs),max(scs),length=steps))
zn <- with(scs_study, seq(min(zn),max(zn),length=steps))
newdat <- expand.grid(scs=scs, zn=zn)
dass <- matrix(predict(fit, newdat), steps, steps)
x1 <- scs
x2 <- zn
y <- dass
p <- persp(x2,x1,y, theta = -89,phi=10, col = NA, main = "y~x1+x2+x1:x2")
```
]

--

.pull-right[
```{r echo=FALSE, fig.asp=.8}
mwdata2<-read_csv("https://uoepsy.github.io/data/wellbeing_rural.csv") %>% mutate(
  isRural = factor(ifelse(location=="rural","rural","notrural"))
)
par(mfrow=c(1,2))
fit<-lm(wellbeing~outdoor_time+isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2], col="red")

fit<-lm(wellbeing~outdoor_time*isRural, data=mwdata2)
with(mwdata2, plot(wellbeing ~ outdoor_time, col=isRural, xlab="x1",ylab="y",main="y~x1+x2+x1:x2\n(x2 is categorical)"))
abline(a = coef(fit)[1],b=coef(fit)[2])
abline(a = coef(fit)[1]+coef(fit)[3],b=coef(fit)[2]+coef(fit)[4], col="red")
par(mfrow=c(1,1))
```
]



---
# Notation

$\begin{align} \color{red}{y} \;\;\;\; & = \;\;\;\;\; \color{blue}{\beta_0 \cdot{} 1 + \beta_1 \cdot{} x_1 + ... + \beta_k \cdot x_k} & + & \;\;\;\varepsilon \\ \qquad \\ \color{red}{\begin{bmatrix}y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5 \\ \vdots \\ y_n \end{bmatrix}} & = \color{blue}{\begin{bmatrix} 1 & x_{11} & x_{21} & \dots & x_{k1} \\ 1 & x_{12} & x_{22} &  & x_{k2} \\ 1 & x_{13} & x_{23} &  & x_{k3} \\ 1 & x_{14} & x_{24} &  & x_{k4} \\ 1 & x_{15} & x_{25} &  & x_{k5} \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1 & x_{1n} & x_{2n} & \dots & x_{kn} \end{bmatrix} \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\ \vdots \\ \beta_k \end{bmatrix}} & + & \begin{bmatrix} \varepsilon_1 \\ \varepsilon_2 \\ \varepsilon_3 \\ \varepsilon_4 \\ \varepsilon_5 \\ \vdots \\ \varepsilon_n \end{bmatrix} \\ \qquad \\ \\\color{red}{\boldsymbol y} \;\;\;\;\; & = \qquad \qquad \;\;\; \mathbf{\color{blue}{X \qquad \qquad \qquad \;\;\;\: \boldsymbol \beta}} & + & \;\;\; \boldsymbol \varepsilon \\ \end{align}$

---

# Extending the linear model
## Link functions

$$
\begin{align}
\color{red}{y} = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & (-\infty, \infty) \\
\qquad \\
\qquad \\
\color{red}{ln \left( \frac{p}{1-p} \right) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & [0,1] \\
\qquad \\
\qquad \\
\color{red}{ln (y) } = \mathbf{\color{blue}{X \boldsymbol{\beta}} + \boldsymbol{\varepsilon}} & \qquad  & (0, \infty) \\
\end{align}
$$  

---
# Linear Models in R

- Linear regression
```{r eval=FALSE, echo=TRUE}
linear_model <- lm(continuous_y ~ x1 + x2 + x3*x4, data = df)
```

- Logistic regression
```{r eval=FALSE, echo=TRUE}
logistic_model <- glm(binary_y ~ x1 + x2 + x3*x4, data = df, family=binomial(link="logit"))
```

- Poisson regression
```{r eval=FALSE, echo=TRUE}
poisson_model <- glm(count_y ~ x1 + x2 + x3*x4, data = df, family=poisson(link="log"))
```

---
# Inference for the linear model

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum1.png")
```

---
# Inference for the linear model

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum2.png")
```

---
# Inference for the linear model

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum3.png")
```

---
# Inference for the linear model

```{r echo=FALSE, out.height="500px"}
knitr::include_graphics("jk_img_sandbox/sum4.png")
```

---
# Assumptions

Our model:  

$\color{red}{y} = \color{blue}{\mathbf{X \boldsymbol \beta}} + \varepsilon \\ \text{where } \boldsymbol \varepsilon \sim N(0, \sigma) \text{ independently}$


--

Our ability to generalise from the model we fit on sample data to the wider population requires making some _assumptions._

--

- assumptions about the nature of the **model** .tr[
(linear)
]

--

- assumptions about the nature of the **errors** .tr[
(normal)
]

--

.footnote[
You can also phrase the linear model as: $\color{red}{\boldsymbol  y} \sim Normal(\color{blue}{\mathbf{X \boldsymbol \beta}}, \sigma)$
]
---

# The Broad Idea

All our work here is in
aim of making **models of the world**.  

--

- Models are models. They are simplifications. They are therefore wrong.  

--

![](jk_img_sandbox/joeymap.jpg)

---
count:false
# The Broad Idea

All our work here is in aim of making **models of the world**.  

- Models are models. They are simplifications. They are therefore wrong.  

- Our residuals ( $y - \hat{y}$ ) reflect everything that we **don't** account for in our model.  

--

- In an ideal world, our model accounts for _all_ the systematic relationships. The leftovers (our residuals) are just random noise.  

--

    - If our model is mis-specified, or we don't measure some systematic relationship, then our residuals will reflect this.
    
--

- We check by examining how much "like randomness" the residuals appear to be (zero mean, normally distributed, constant variance, i.i.d ("independent and identically distributed")
    - _these ideas tend to get referred to as our "assumptions"_
    
--

- We will **never** know whether our residuals contain only randomness - we can never observe everything! 

---
# Checking Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  


]
.pull-right[

```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
df<-tibble(x=runif(1000,1,10),xr = round(x), y=1*x+rnorm(1000))
#df$y[df$x==6]<-6+rnorm(100,0,3)
df %>% group_by(xr) %>% summarise(m=mean(y), s=sd(y)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y",c(0,5,10), breaks=c(0,5,10))+
  themedapr3()
p2 <- ggplot(df, aes(x=xr,y=y))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y",c(0,5,10), breaks=c(0,5,10))+
  themedapr3()

p1 / p2
```

]

---
count:false 
# Checking Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- __mean of the residuals = zero across the predicted values on the linear predictor.__    

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  


]
.pull-right[


```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
tibble(x = runif(1000,1,10),
       xr = round(x),
       s = abs(5-x)*2,
       e = map_dbl(s,~rnorm(1,0,1)),
       y = x + s + e) -> df

df %>% group_by(xr) %>% summarise(m=mean(y), s=sd(y)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y",c(0,5,10,15), breaks=c(0,5,10,15))+
  themedapr3()
p2 <- ggplot(df, aes(x=xr,y=y))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y",c(0,5,10,15), breaks=c(0,5,10,15))+
  themedapr3()

p1 / p2
```

]

---
count:false 
# Checking Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- __spread of residuals is normally distributed and constant across the predicted values on the linear predictor.__  


]
.pull-right[

```{r echo=FALSE,fig.asp=.8}
library(ggdist)
library(tidyverse)
tibble(x = runif(1000,1,10),
       xr = round(x),
       s = abs(x)/2,
       e = map_dbl(s,~rnorm(1,0,.)),
       y = x + e) -> df

df %>% group_by(xr) %>% summarise(m=mean(y), s=sd(y)) -> dfd
p1 <- ggplot(df, aes(x=x,y=y))+
  geom_point()+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y",c(0,5,10), breaks=c(0,5,10))+
  themedapr3()
p2 <- ggplot(df, aes(x=xr,y=y))+
  geom_jitter(height=0,width=1, alpha=.3)+
  stat_dist_halfeye(inherit.aes=F,data=dfd, aes(x=xr,dist="norm",arg1=m,arg2=s),alpha=.6, fill="orange")+
  geom_smooth(method="lm",se=F, fullrange=T)+
  scale_x_continuous("x",1:10, breaks=seq(1,10,2))+
  scale_y_continuous("y",c(0,5,10), breaks=c(0,5,10))+
  themedapr3()

p1 / p2
```
]

---
count:false
# Checking Assumptions

.pull-left[

What does "zero mean and constant variance" look like?  

- mean of the residuals = zero across the predicted values on the linear predictor.  

- spread of residuals is normally distributed and constant across the predicted values on the linear predictor.  



]
.pull-right[
__`plot(model)`__

```{r echo=FALSE}
df<-tibble(x=runif(1000,1,10),xr = round(x), y=1*x+rnorm(1000))
```

```{r fig.asp=.8}
my_model <- lm(y ~ x, data = df)
plot(my_model, which = 1)
```

]


---
count:false
# Checking Assumptions
<br><br>
<center>
<div class="acronym">
L
</div> inearity<br>
<div class="acronym">
I
</div> ndependence<br>
<div class="acronym">
N
</div> ormality<br>
<div class="acronym">
E
</div> qual variance<br>
</center>
.footnote["Line without N is a Lie!" (Umberto)]

---

# What if our model doesn't meet assumptions?

- is our model mis-specified?  
  - is the relationship non-linear? higher order terms? (e.g. $y \sim x + x^2$)
  - is there an omitted variable or interaction term? 
  
  
--

- transform the outcome variable?
  - makes things look more "normal"
  - but can make things more tricky to interpret:  
    $y \sim x$ and $log(y) \sim x$ are quite different models

--

- bootstrap
  - do many times: resample (w/ replacement) your data, and refit your model.
  - obtain a distribution of parameter estimate of interest. 
  - compute a confidence interval for estimate
  - celebrate

--

But these don't help if we have violated our assumption of independence...

---
# Summary

- we can fit a linear regression model which takes the form $\color{red}{y} = \color{blue}{\mathbf{X} \boldsymbol{\beta}} + \boldsymbol{\varepsilon}$  

--

- in R, we fit this with `lm(y ~ x1 + .... xk, data = mydata)`.  

--

- we can extend this to different link functions to model outcome variables which follow different distributions.  

--

- when drawing inferences from a fitted model to the broader population, we rely on certain assumptions.  

--

  - one of these is that the errors are independent.


---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Linear Regression Refresh</h2>
<h2>Part 2: Clustered Data</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Possible Approaches</h2>
<h2 style="text-align: left;opacity:0.3;">Extra: ANOVA & Repeated Measures (brief)</h2>

---
# What is clustered data?

.pull-left[
- children within schools  

- patients within clinics  

- observations within individuals  
]
.pull-left[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/h2.png")
```
]
---
count:false
# What is clustered data?

--

.pull-left[
- children within classrooms within schools within districts etc...  

- patients within doctors within hospitals... 

- time-periods within trials within individuals
]
.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/h3.png")
```
]

--

.footnote[
Other relevant terms you will tend to see: "grouping structure", "levels", "hierarchies". 
]

---
# Why is clustering worth thinking about?  

Clustering will likely result in measurements on observational units within a given cluster being more similar to each other than to those in other clusters.  

--
<br>
  - For example, our measure of academic performance for children in a given class will tend to be more similar to one another (because of class specific things such as the teacher) than to children in other classes.

--

.pull-left[
A lot of the data you will come across will have clusters. 
  - multiple experimental trials per participant
  - patients in clinics
  - employees within departments
  - children within classes
]

--

.pull-right[
```{r echo=FALSE}
knitr::include_graphics("jk_img_sandbox/lev1.png")
```

]

---
# ICC (intra-class correlation coefficient)

Clustering is expressed in terms of the correlation among the measurements within the same cluster - known as the __intra-class correlation coefficient (ICC).__

--

There are various formulations of ICC, but the basic principle = ratio of *variance between groups* to *total variance*.  

<br>
$\rho = \frac{\sigma^2_{a}}{\sigma^2_{b} + \sigma^2_e} \\ \qquad \\\textrm{Where:} \\ \sigma^2_{b} = \textrm{variance between clusters} \\ \sigma^2_e = \textrm{variance within clusters (residual variance)} \\$

--

Can also be interpreted as the correlation between two observations from the same group. 

---
# Various values of $\rho$

The larger the ICC, the lower the variability is within the clusters (relative to the variability between clusters). The greater the correlation between two observations from the same group. 

.pull-left[
```{r echo=FALSE, fig.asp=.8, fig.align="center"}
iccgen <- function(j,n,e,icc,coef=0){
  v = (icc*e)/(1-icc)
  es = e/(v+e)
  v = if(is.infinite(v)){v=e}else{v/(v+e)}
  npj = n/j
  tibble(
    j = letters[1:j],
    zeta_j = rnorm(j,0,sqrt(v))
  ) %>%
    mutate(
      e_ij = map(j, ~rnorm(npj, 0, sqrt(es)))
    ) %>% unnest() %>%
    mutate(
      x = rnorm(n, 10, 5),
      y = 5 + coef*x + zeta_j + e_ij
    )
}
set.seed(3406)
sims = map_dfr(set_names(c(0,.5,.75,.95,.99,1)), 
        ~iccgen(j=10,n=100,e=1,icc=.,coef=0), .id="icc") %>%
  group_by(icc, j) %>%
  mutate(
    m = mean(y)
  ) %>% ungroup

ggplot(sims, aes(x=j, y=y))+
  geom_jitter(height=0, size=2,aes(col=j))+
  scale_y_continuous(NULL, labels=NULL)+
  stat_summary(geom="errorbar",aes(x=j,y=m,col=j),lwd=1)+
  facet_grid(icc~.)+
  guides(col=F)+
  themedapr3() +
  labs(x="cluster") -> p1

# ggplot(sims, aes(x=0, y=y))+
#   see::geom_violinhalf(aes(x=.5))+
#   geom_jitter(height=0,width=.5, size=2,aes(col=j), alpha=.5)+
#   scale_y_continuous(NULL, labels=NULL)+
#   scale_x_continuous(NULL, labels=NULL)+
#   #stat_summary(geom="errorbar",aes(x=j,y=m,col=j),lwd=1)+
#   facet_grid(icc~.)+
#   guides(col=F)+
#   themedapr3() -> p2
# p1 + p2 + plot_layout(widths=c(8,1))
p1
```

]

--

.pull-right[

```{r echo=FALSE, fig.asp=.8, fig.align="center"}
set.seed(875)
sims = map_dfr(set_names(c(0,.5,.75,.95,.99,1)), 
        ~iccgen(j=10,n=100,e=1,icc=.,coef=.1), .id="icc")

ggplot(sims, aes(x=x, y=y))+
  geom_point(aes(col=j))+
  geom_line(aes(col=j))+
  facet_grid(icc~.)+
  themedapr3()+
  guides(col=F)+
  scale_y_continuous(NULL, labels=NULL)+
  scale_x_continuous("X", labels=NULL)
```
]

---
# Clustered data & lm

.pull-left[
#### Why is it a problem?

Clustering is something **systematic** that our model should (arguably) take into account.  

- remember, $\varepsilon \sim N(0, \sigma) \textbf{ independently}$ 

]

--

.pull-right[
#### HOW is it a problem?  

Standard errors tend to be smaller than they should be, meaning that:  

  - confidence intervals will be too narrow 
  
  - $t$-statistics will be too large  
  
  - $p$-values will be misleadingly small

]

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Linear Regression Refresh</h2>
<h2><b style="opacity:0.4;">Part 2: Clustered Data</b><b>A Practical Comment on Data</b></h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Possible Approaches</h2>
<h2 style="text-align: left;opacity:0.3;">Extra: ANOVA & Repeated Measures (brief)</h2>


---
# Wide Data/Long Data

.pull-left[
__Wide Data__  

```{r echo=FALSE}
tibble(
  ID = sprintf("%03d",1:4), 
  age = rdunif(4, 18, 75),
  trial_1 = c(10, 7.5, 12, 10.5),
  trial_2 = c(12.5, 7, 14.5, 17),
  trial_3 = c(18, 5, 11, 14)
) -> wided
wided %>% rbind(.,"...")
```



]
.pull-right[
__Long Data__


```{r echo=FALSE}
tibble(
  ID = sprintf("%03d",1:4), 
  age = rdunif(4, 18, 75),
  trial_1 = c(10, 7.5, 12, 10.5),
  trial_2 = c(12.5, 7, 14.5, 17),
  trial_3 = c(18, 5, 11, 14)
) -> wided
pivot_longer(wided, 3:5, names_to="trial", values_to="score") -> longd
longd %>% rbind(.,"...")
```


]


---
# Wide Data/Long Data


```{r echo=FALSE, fig.cap="Source: Examples of wide and long representations of the same data. Source: Garrick Aden-Buie’s (@grrrck) [Tidy Animated Verbs](https://github.com/gadenbuie/tidyexplain)"}
knitr::include_graphics("https://www.fromthebottomoftheheap.net/assets/img/posts/tidyr-longer-wider.gif")
```


---
# Long is good for plotting. 

.pull-left[
__`group` aesthetic__  

```{r fig.asp=.5}
ggplot(longd, aes(x=trial,y=score, group=ID, col=ID))+
  geom_point(size=4)+
  geom_path()+
  themedapr3()
```

]
.pull-right[
__`facet_wrap()`__  
```{r fig.asp=.5}
ggplot(longd, aes(x=trial,y=score))+
  geom_point(size=4)+
  geom_path(aes(group=1))+
  themedapr3()+
  facet_wrap(~ID)
```
]

---
# Long is good for describing by ID

```{r}
longd %>% 
  group_by(ID) %>%
  summarise(
    ntrials = n_distinct(trial),
    meanscore = mean(score),
    sdscore = sd(score)
  )
```



---
# Summary

- Clustering can take many forms, and exist at many levels  

--

- Clustering is something systematic that we would want our model to take into account

  - Ignoring it can lead to incorrect statistical inferences

--

- Clustering is typically assessed using intra-class correlation coefficient (ICC) - the ratio of variance within clusters to the total variance $\rho = \frac{\sigma^2_b}{\sigma^2_b + \sigma^2_e}$

--

- Tidying your data and converting it to *long* format (one observational unit per row, and a variable identifying the cluster ID) is a good start. 

---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 2


---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Linear Regression Refresh</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Clustered Data</h2>
<h2>Part 3: Possible Approaches</h2>
<h2 style="text-align: left;opacity:0.3;">Extra: ANOVA & Repeated Measures (brief)</h2>

---
# Our Data
.pull-left[
> Sample of 115 birds from 12 gardens, information captured on the arrival time (hours past midnight) and number of worms caught by the end of the day. 

```{r}
worms_data <- read_csv("https://uoepsy.github.io/data/worms.csv")
head(worms_data)
```

```{r}
library(ICC)
ICCbare(x = gardenid, y = nworms, data = worms_data)
```
]
.pull-right[
```{r echo=FALSE, fig.align="center", fig.asp=.8}
ggplot(worms_data, aes(x=arrivalt,y=nworms))+
  geom_point(size=4)+
  geom_smooth(method="lm", se=F)+
  labs(x="",y="")+
  themedapr3() -> p1
ggplot(worms_data, aes(x=arrivalt,y=nworms, col=gardenid))+
  geom_point(size=4)+
  geom_smooth(method="lm", se=FALSE)+
  facet_wrap(~gardenid, scales="free_y")+
  guides(col=FALSE)+
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3() -> p2

p1 / p2 + plot_layout(heights=c(1,2))
```
]

---
# Ignore the clustering

.pull-left[
__(Complete pooling)__  

+ `lm(y ~ 1 + x, data = df)`  

+ Information from all clusters is pooled together to estimate over x  

```{r}
model <- lm(nworms ~ arrivalt, data = worms_data)
```
```{r echo=FALSE, out.width="300px"}
.pp(summary(model),l=list(c(10:12)))
```
]
.pull-right[
```{r echo=FALSE}
df <- worms_data %>% rename(y=nworms, x = arrivalt) %>% mutate(cluster_var = gsub("garden","cluster",gardenid))
model <- lm(y ~ x, data = df)
df %>% mutate(
  f = fitted(model)
) %>%
ggplot(.,aes(x=x,y=y))+geom_point(size=4)+
  geom_smooth(method="lm")+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.2) + 
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()
```
]

---
count:false
# Ignore the clustering

.pull-left[
__(Complete pooling)__  

+ `lm(y ~ 1 + x, data = df)`  

+ Information from all clusters is pooled together to estimate over x  

```{r}
model <- lm(nworms ~ arrivalt, data = worms_data)
```
```{r echo=FALSE, out.width="300px"}
.pp(summary(model),l=list(c(10:12)))
```

But different clusters show different patterns. 
Residuals are not independent.  
]
.pull-right[

```{r echo=FALSE}
library(ggfx)
library(ggforce)
model <- lm(y ~ x, data = df)
df %>% mutate(
  f = fitted(model)
) -> pdat 

ggplot(pdat,aes(x=x,y=y))+
  with_blur(geom_point(aes(col=cluster_var),size=4,alpha=.2), sigma = unit(0.7, 'mm')) + 
  geom_point(data = filter(pdat,cluster_var %in% c("cluster9","cluster4")),aes(col=cluster_var),size=4) + 
  
  geom_mark_ellipse(aes(label = gardenid, filter = cluster_var == "cluster4"),
                    con.colour  = "#526A83", con.cap = 0, 
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) + 
  geom_mark_ellipse(aes(label = gardenid, filter = cluster_var == "cluster9"),
                    con.colour  = "#526A83", con.cap = 0, 
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) + 
  
  
  guides(col=FALSE, alpha=FALSE)+
  geom_smooth(method="lm",se=F)+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.2) + 
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()
```
]

---
count:false
class: extra
exclude: `r params$show_extra`
# Lesser used approaches

__Cluster Robust Standard Errors__

.pull-left[
<!-- https://rlbarter.github.io/Practical-Statistics/2017/05/10/generalized-estimating-equations-gee/ -->

Don't include clustering as part of the model directly, but incorporate the dependency into our residuals term. 

$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error}^* \\
\end{align}
$$
.footnote[`*` Where errors are clustered]

]
--
.pull-right[
Simply adjusts our standard errors:
```{r}
library(plm)
clm <- plm(nworms ~ 1 + arrivalt, data=worms_data, 
           model="pooling", index="gardenid")
```
```{r echo=FALSE}
.pp(summary(clm),l=list(c(13:16)))
```
```{r}
sqrt(diag(vcovHC(clm, 
                 method='arellano', 
                 cluster='group')))
```
]


---
class: extra
exclude: `r params$show_extra`
count:false
# Lesser used approaches

__Generalised Estimating Equations (GEE)__  

.pull-left[
- Don't include clustering as part of the model directly, but incorporate the dependency into our residuals term. 

$$
\begin{align}
\color{red}{\textrm{outcome}} & = \color{blue}{(\textrm{model})} + \textrm{error}^* \\
\end{align}
$$

.footnote[`*` Where errors are clustered, and follow some form of correlational<br>structure within clusters (e.g. based on how many timepoints apart<br>two observations are).]


]

--
.pull-right[
Specifying a correlational structure for residuals within clusters can influence _what_ we are estimating
```{r}
library(geepack)
# needs to be arranged by cluster, 
# and for cluster to be numeric
worms_data <- 
  worms_data %>%
  mutate(
    cluster_id = as.numeric(as.factor(gardenid))
  ) %>% arrange(cluster_id)
# 
geemod  = geeglm(nworms ~ 1 + arrivalt, 
                 data = worms_data, 
                 corstr = 'exchangeable',
                 id = cluster_id)
```
```{r echo=FALSE}
.pp(summary(geemod),l=list(c(6:9)))
```

]

---
# Fixed effects

.pull-left[

__(No pooling)__  

- `lm(y ~ x * cluster, data = df)`  

- Information from a cluster contributes to estimate *for that cluster*, but information is not pooled to estimate an overall effect. 

```{r}
model <- lm(nworms ~ 1 + arrivalt * gardenid, 
            data = worms_data)
```
{{content}}
]
.pull-right[
```{r echo=FALSE}
model1 <- lm(y~x*cluster_var,df)
df %>% mutate(
  f = fitted(model1)
) %>%
ggplot(.,aes(x=x,y=y,col=cluster_var))+
  geom_point()+
  geom_line(aes(y=f,group=cluster_var))+
  geom_smooth(df %>% mutate(f = fitted(model1)) %>% filter(cluster_var == "cluster1"), method="lm", se=F, fullrange = T , mapping=aes(x=x,y=y,col=cluster_var), lty="dashed", lwd=.5) +
  guides(col=FALSE)+
  geom_segment(aes(y=y, yend = f, x = x, xend = x), alpha=.4) + 
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  scale_x_continuous(limits=c(0,9), breaks=0:9)+
  themedapr3()
```
]

--

+ Lots of estimates (separate for each cluster). 
+ Variance estimates constructed based on information *only* within each cluster. 
+ No overall estimate of effect over x. 
```{r echo=FALSE}
.pp(summary(model),l=list(c(10:34)))
```

---
# Random effects (MLM)

.pull-left[
__(Partial Pooling)__

- `lmer(y ~ 1 + x + (1 + x| cluster), data = df)`
- cluster-level variance in intercepts and slopes is modeled as randomly distributed around fixed parameters.
- effects are free to vary by cluster, but information from clusters contributes (according to cluster $n$ and outlyingness of cluster) to an overall fixed parameter. 

```{r}
library(lme4)
model <- lmer(nworms ~ 1 + arrivalt + 
                (1 + arrivalt | gardenid),
              data = worms_data)
summary(model)$coefficients
```


]
.pull-right[

```{r echo=FALSE}
model = lmer(y~x+(1+x|cluster_var),df)
df %>% mutate(
  fit = fitted(model)
) %>%
ggplot(., aes(x=x, y=y))+
  geom_line(aes(y=fit, group=cluster_var, col=cluster_var))+
  geom_abline(intercept=fixef(model)[1], slope=fixef(model)[2],lwd=2)+
  geom_point(aes(col=cluster_var),alpha=.3)+
  geom_segment(aes(y=y, yend = fit, x = x, xend = x, 
                   col=cluster_var), alpha=.3)+
  labs(x="arrival at garden\n(hrs past midnight)",y="number of worms caught")+
  themedapr3()+
  guides(col=FALSE) +
  scale_x_continuous(limits=c(0,9), breaks=0:9)+
  scale_y_continuous(limits=c(0,70))
```
]

---
count:false
# Random effects (MLM)

.pull-left[
__(Partial Pooling)__

- `lmer(y ~ 1 + x + (1 + x| cluster), data = df)`
- cluster-level variance in intercepts and slopes is modeled as randomly distributed around fixed parameters.
- effects are free to vary by cluster, but information from clusters contributes (according to cluster $n$ and outlyingness of cluster) to an overall fixed parameter. 

```{r}
library(lme4)
model <- lmer(nworms ~ 1 + arrivalt + 
                (1 + arrivalt | gardenid),
              data = worms_data)
summary(model)$coefficients
```


]
.pull-right[

```{r echo=FALSE}
library(ggforce)
library(ggfx)
model = lmer(y~x+(1+x|cluster_var),df)
dfm <- 
  df %>% group_by(cluster_var) %>%
  summarise(x = 0) %>% 
  mutate(mf = predict(model, newdata=.),
         ff = fixef(model)[1] + x*fixef(model)[2],
         cv = ifelse(mf>ff, -.2, .2))

dfi <- bind_rows(df,
                 df %>% group_by(cluster_var) %>% summarise(x = 0)) %>%
  mutate(fit2 = predict(model, newdata=.))


df %>% mutate(
  fit = fitted(model)
) -> pdat

dfs <- tibble(
  cluster_var = c("cluster5","cluster12"),
  yf = c(
    coef(model)$cluster_var["cluster5","(Intercept)"]+(3*fixef(model)[2]),
    coef(model)$cluster_var["cluster12","(Intercept)"]+(3*fixef(model)[2])
    ),
  yg = c(
    coef(model)$cluster_var["cluster5","(Intercept)"]+(3*coef(model)$cluster_var["cluster5","x"]), 
    coef(model)$cluster_var["cluster12","(Intercept)"]+(2.7*coef(model)$cluster_var["cluster12","x"])
  )
)

filter(pdat, cluster_var %in% c("cluster12","cluster5")) %>%
  group_by(cluster_var) %>%
  mutate(
    fitmc = mean(fit),
    xgc = x-mean(x),
    fix = fitmc + fixef(model)[2]*xgc,
  ) -> counterf


ggplot(pdat, aes(x=x, y=y))+
  geom_abline(intercept=fixef(model)[1], slope=fixef(model)[2],lwd=2)+
  with_blur(geom_line(aes(y=fit, group=cluster_var, col=cluster_var), alpha=.5), sigma = 2)+
  with_blur(geom_point(aes(col=cluster_var), alpha=.5), sigma = 2)+
  with_blur(geom_segment(aes(y=y, yend = fit, x = x, xend = x, col=cluster_var), alpha=.2), sigma = 2)+
  with_blur(geom_curve(data=dfm[dfm$mf>dfm$ff,], 
             aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),
             curvature = -.5, alpha=.5), sigma = 2)+
  with_blur(geom_curve(data=dfm[dfm$mf<dfm$ff,],
             aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),
             curvature = .5, alpha=.5), sigma = 2)+
  with_blur(geom_line(data=dfi, aes(y=fit2, group=cluster_var, col=cluster_var),
            lty = "dashed", alpha=.5), sigma = 2)+
  
  geom_line(data = filter(pdat, cluster_var %in% c("cluster12","cluster5")), aes(y=fit, group=cluster_var, col=cluster_var))+
  geom_point(data = filter(pdat, cluster_var %in% c("cluster12","cluster5")), aes(col=cluster_var))+
  geom_segment(data = filter(pdat, cluster_var %in% c("cluster12","cluster5")), aes(y=y, yend = fit, x = x, xend = x, col=cluster_var), alpha=.7)+
  geom_line(data=filter(dfi, cluster_var %in% c("cluster12","cluster5")), aes(y=fit2, group=cluster_var, col=cluster_var),lty = "dashed")+
  geom_curve(data=filter(dfm[dfm$mf>dfm$ff,], cluster_var %in% c("cluster12","cluster5")),aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),curvature = -.5)+
  geom_curve(data=filter(dfm[dfm$mf<dfm$ff,], cluster_var %in% c("cluster12","cluster5")),aes(x=x, xend=x, y=mf, yend=ff, col = cluster_var),curvature = .5)+
  
  geom_line(data=counterf, aes(y=fix,group=cluster_var), alpha=.5)+
  # geom_abline(intercept=coef(model)$cluster_var["cluster5","(Intercept)"], slope=fixef(model)[2],
  #             lwd=1,alpha=.2)+
  # geom_curve(data = dfs[1,], aes(x=3,xend=3,y=yf, yend=yg), lwd=.5, curvature=.5, alpha=.3)+
  # geom_abline(intercept=coef(model)$cluster_var["cluster12","(Intercept)"], slope=fixef(model)[2],
  #             lwd=1,alpha=.2)+
  # geom_curve(data = dfs[2,], aes(x=3,xend=2.7,y=yf, yend=yg), lwd=.5, curvature=-.5, alpha=.3)+
  
  
  guides(col = FALSE)+
  labs(x="x = arrival at garden\n(hrs past midnight)",y="y = number of worms caught")+
  themedapr3() + 
  geom_mark_ellipse(aes(label = gardenid, filter = cluster_var == "cluster5"),
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) +
  geom_mark_ellipse(aes(label = gardenid, filter = cluster_var == "cluster12"),
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE) +
  scale_x_continuous(limits=c(0,9), breaks=0:9)+
  scale_y_continuous(limits=c(0,70))+
  NULL

```
]


---
# Summary

With clustered data, there are many possible approaches. Some of the main ones are:  

- Ignore it (**complete pooling**)  
    - and make inappropriate inferences.  

--

- Completely partition out any variance due to clustering into fixed effects for each cluster (__no pooling__).  
    - and limit our estimates to being cluster specific and low power.  
    
--

- Model cluster-level variance as randomly distributed around fixed parameters, and partially pool information across clusters.  
    - best of both worlds?  
    

---
class: inverse, center, middle, animated, rotateInDownLeft

# End

---
exclude: `r params$show_extra`
class: extra, inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Linear Regression Refresh</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: Clustered Data</h2>
<h2 style="text-align: left;opacity:0.3;">Part 3: Possible Approaches</h2>
<h2>Extra: ANOVA & Repeated Measures (brief)</h2>


---
count:false
class: extra
exclude: `r params$show_extra`
# ANOVA revisited

- ANOVA in it's simplest form is essentially a linear model with categorical predictor(s). It is used to examines differences in group means/differences in differences in group means etc.  

- It achieves this by partitioning variance. We can calculate it by hand!  


- Still quite popular in psychology because we often design experiments with discrete conditions.
  - This means we can balance designs and examine mean differences between conditions. 

---
count:false
class: extra
exclude: `r params$show_extra`
# quick simulating data

.pull-left[
```{r include=F}
set.seed(9542)
```

```{r}
testdata <- 
  tibble(
    groups = rep(c("A","B"), each = 30),
    outcome = c(rnorm(30,10,5), rnorm(30,15,5))
  )
```
].pull-right[
```{r fig.asp=.8}
with(testdata, boxplot(outcome ~ groups))
```
]

---
count:false
class: extra
exclude: `r params$show_extra`
# lm vs anova

.pull-left[
```{r}
summary(lm(outcome ~ groups, data = testdata))
```
]

.pull-right[
```{r}
summary(aov(outcome ~ groups, data = testdata))
anova(lm(outcome ~ groups, data = testdata))
```
]

in this case (one predictor) the coefficient test $t = \sqrt{F}$
```{r}
sqrt(12.5)
```

---
count:false
class: extra
exclude: `r params$show_extra`
# more anova  
```{r echo=FALSE}
df <- read.table("~/Desktop/jk_codebits/wip/data/growth.txt", header=T) %>%
  transmute(y = gain, 
            x1 = fct_recode(factor(supplement), "horse"="control",
                            "cat"="supergain",
                            "dog"="supersupp",
                            "parrot"="agrimore"), 
            x2 = fct_recode(factor(diet),"scotland"="wheat",
                            "england"="oats",
                            "wales"="barley"),
            x3 = rnorm(n())
          ) %>% 
  sample_n(40)
```

- ANOVA tests whether several parameters (differences between group means) are simultaneously zero.   
- This is why it is referred to as the "omnibus"/"overall F" test. 

.pull-left[
```{r fig.asp=.8, echo=FALSE}
with(df, boxplot(y~x1))
```
]

.pull-right[
```{r highlight.output=c(5)}
anova(lm(y~x1,df))
```
]

---
count:false
class: extra
exclude: `r params$show_extra`
# more anova

- ANOVA tests whether several parameters (differences between group means) are simultaneously zero.   
- This is why it is referred to as the "omnibus"/"overall F" test. 

.pull-left[
```{r fig.asp=.8, echo=FALSE}
with(df, boxplot(y~x1))
```
]

.pull-right[
```{r highlight.output=c(20)}
summary(lm(y~x1,df))
```
]


---
count:false
class: extra
exclude: `r params$show_extra`
# partitioning variance

```{r fig.asp=.7, echo=FALSE}
knitr::include_graphics("jk_img_sandbox/anova.png")
```


.footnote[  
The terminology here can be a nightmare.  
$SS_{between}$ sometimes gets referred to as $SS_{model}$, $SS_{condition}$, $SS_{regression}$, or $SS_{treatment}$.  
Meanwhile $SS_{residual}$ also gets termed $SS_{error}$.  
To make it all worse, there are inconsistencies in the acronyms used, $SSR$ vs. $RSS$! 

]


---
count:false
class: extra
exclude: `r params$show_extra`
# more anova

.pull-left[

with multiple predictors, sums of squares can be calculated differently

1. Sequential Sums of Squares = Order matters
2. <p style="opacity:0.4">Partially Sequential Sums of Squares</p>
3. <p style="opacity:0.4">Partial Sums of Squares</p>

]
.pull-right[
__sequential SS__
```{r}
anova(lm(y~x1+x2,df))
anova(lm(y~x2+x1,df))
```
]

---
count:false
class: extra
exclude: `r params$show_extra`
# more anova

.pull-left[

with multiple predictors, sums of squares can be calculated differently

1. <p style="opacity:0.4">Sequential Sums of Squares</p>
2. <p style="opacity:0.4">Partially Sequential Sums of Squares</p>
3. Partial Sums of Squares = Each one calculated as if its the last one in sequential SS (order doesn't matter).

]
.pull-right[
__partial SS__
```{r}
car::Anova(lm(y~x1+x2,df), type="III")
car::Anova(lm(y~x2+x1,df), type="III")
```

]

---
class: extra
exclude: `r params$show_extra`
count:false
# Why use lm vs ANOVA?

ANOVA requires post-hoc test to compare specific differences, but it has the advantage of conducting fewer tests (initially). 

- ANOVA asks the question:
  - "*are there differences* between group means?
  - "*is there an effect* of x?"
  - "*does* x predict y?"  
  
- the coefficient tests from the linear model ask 
  - "*what are the differences* between group means?
  - "*what is the effect* of x?"
  - "*how does* x predict y?"

---
count:false
class: extra
exclude: `r params$show_extra`
# anova() as model comparison

.pull-left[
because ANOVA provides test of __set of parameters being simultaneously zero__, we can use this for model comparison. e.g.  
- "does the addition of $\textrm{<predictor(s)>}$ improve our model of y?"

```{r}
m0<-lm(y~1, df)
m1<-lm(y~x1, df)

m2<-lm(y~x1+x2, df)
m3<-lm(y~x1+x2+x3, df)
```
]
.pull-right[
`anova()` function to compare models. 
```{r}
anova(m1, m2)
anova(m1, m3)
```
]

---
count:false
class: extra
exclude: `r params$show_extra`
# Partitioning - ANOVA

```{r fig.asp=.7, echo=FALSE}
knitr::include_graphics("jk_img_sandbox/anova.png")
```

---
count:false
exclude: `r params$show_extra`
# Partitioning - ANOVA (Rpt Measures)

```{r fig.asp=.7, echo=FALSE}
knitr::include_graphics("jk_img_sandbox/rmanova.png")
```

---
count:false
class: extra
exclude: `r params$show_extra`
# Rpt Measures ANOVA in R

.pull-left[
```{r echo=FALSE, fig.asp=.9}
set.seed(853)
tibble(
  condition = letters[1:4],
  cmean = c(0,2,3,3),
  ceff = c(.5, 1, 3, .5),
  smeans = map(cmean, ~rnorm(10,.,1)),
  seffs = map(ceff, ~rnorm(10, .,1)),
  sid = map(condition, ~paste0(.,1:10)),
) %>% unnest(c(smeans,sid,seffs)) %>%
  mutate(
    data = map2(smeans,seffs, ~tibble(t = 1:3, y = .y*t+rnorm(3,.x,.1)))
  ) %>% unnest(data) -> df

df$subject = df$sid
df$t = factor(df$t)

ggplot(df, aes(x=t,y=y,col=sid))+
  geom_point()+
  geom_path(aes(group=sid))+
  guides(col=FALSE,fill=FALSE)+
  themedapr3() -> pwithin
ggplot(df,aes(x=condition,y=y))+
  geom_boxplot(aes(group=condition, col=condition),alpha=.3)+
  guides(col=FALSE,fill=FALSE)+
  themedapr3() -> pbetween
pwithin
```
]
.pull-right[
```{r}
library(ez)
ezANOVA(data = df, dv = y, wid = subject, 
        within = t)
```
]


---
count:false
class: extra
exclude: `r params$show_extra`
# Mixed ANOVA in R

.pull-left[
```{r echo=FALSE, fig.asp=.9}
pbetween / 
  pwithin + facet_wrap(~condition)
```
]
.pull-right[
```{r}
library(ez)
ezANOVA(data = df, dv = y, wid = subject, 
        within = t, between = condition)
```
]

---
count:false
exclude: `r params$show_extra`
class: extra, inverse, center, middle, animated, rotateInDownLeft

# End

