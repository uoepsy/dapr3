---
title: "<b>Centering Predictors<br>Generalisations</b>"
subtitle: "Data Analysis for Psychology in R 3"
author: "Josiah King, Umberto Noè, Tom Booth"
institute: "Department of Psychology<br/>The University of Edinburgh"
date: "AY 2021-2022"
output:
  xaringan::moon_reader:
    lib_dir: jk_libs/libs
    css: 
      - xaringan-themer.css
      - jk_libs/tweaks.css
    nature:
      beforeInit: "jk_libs/macros.js"
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
params: 
    show_extra: true
    finalcompile: FALSE
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
#knitr::opts_chunk$set(eval=FALSE)
options(htmltools.dir.version = FALSE)
options(digits=4,scipen=2)
options(knitr.table.format="html")
xaringanExtra::use_xaringan_extra(c("tile_view","animate_css","tachyons"))
xaringanExtra::use_tile_view()
xaringanExtra::use_extra_styles(
  mute_unhighlighted_code = FALSE
)
xaringanExtra::use_share_again()
library(knitr)
library(tidyverse)
library(ggplot2)
library(kableExtra)
library(patchwork)
knitr::opts_chunk$set(
  dev = "png",
  warning = FALSE,
  message = FALSE,
  cache = FALSE
)
themedapr3 = function(){
  theme_minimal() + 
    theme(text = element_text(size=20))
}
source("jk_source/jk_presfuncs.R")
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
style_mono_accent(
  # base_color = "#0F4C81", # DAPR1
  # base_color = "#BF1932", # DAPR2
  base_color = "#88B04B", # DAPR3 
  # base_color = "#FCBB06", # USMR
  # base_color = "#a41ae4", # MSMR
  header_color = "#000000",
  header_font_google = google_font("Source Sans Pro"),
  header_font_weight = 400,
  code_font_size = "0.7rem",
  text_font_google = google_font("Source Sans Pro", "400", "400i", "600", "600i"),
  code_font_google = google_font("Source Code Pro"),
  extra_css = list(".scroll-output" = list("height"="90%","overflow-y"="scroll"))
)
```

class: inverse, center, middle

<h2>Part 1: Centering Predictors</h2>
<h2 style="text-align: left;opacity:0.3;">Part 2: GLMM</h2>


---
# Centering predictors in LM

.pull-left[
```{r include=F}
library(lme4)
set.seed(934)
df <- tibble(
  x = rnorm(200,4,1),
  y = pmax(0.1,.3+.5*x + rnorm(200))
)
```

```{r}
m1 <- lm(y~x,data=df)
m2 <- lm(y~scale(x, center=T,scale=F),data=df)
m3 <- lm(y~scale(x, center=T,scale=F),data=df)
m4 <- lm(y~I(x-5), data=df)
anova(m1,m2,m3,m4)
```
]
--
.pull-right[
```{r echo=FALSE, fig.align="center"}
set.seed(934)
df <- tibble(
  x = rnorm(200,4,1),
  y = pmax(0.1,.3+.5*x + rnorm(200))
)

mod = lm(y~x,df)
p1 = ggplot(df, aes(x=x,y=y))+geom_point()+
  geom_smooth(method="lm")+
  geom_smooth(method="lm",fullrange=T,lty="dotted")+
  #ylim(0,max(df$y))+
  geom_segment(x=0,xend=0,y=0,yend=max(df$y))+
  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+
  geom_point(data=tibble(x=0,y=coef(mod)[1]),size=4)+
  labs(title="Raw X")+
  scale_x_continuous(limits=c(0,7),breaks=0:7)+
  themedapr3()

mod = lm(y~scale(x,scale=F),df)
p2 = ggplot(df, aes(x=x,y=y))+geom_point()+
  geom_smooth(method="lm")+
  geom_smooth(method="lm",fullrange=T,lty="dotted")+
  #ylim(0,max(df$y))+
  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=max(df$y))+
  geom_segment(x=0,xend=max(df$x),y=0,yend=0)+
  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=4)+
  scale_x_continuous(limits=c(0,7),breaks=map_dbl(seq(4,-4), ~mean(df$x)-.),
                     labels=seq(-4,4))+
  labs(title="Mean centered X")+
  themedapr3()
  
  
mod = lm(y~scale(x),df)
p3 = ggplot(df, aes(x=x,y=y))+geom_point()+
  geom_smooth(method="lm")+
  geom_smooth(method="lm",fullrange=T,lty="dotted")+
  #ylim(0,max(df$y))+
  geom_segment(x=mean(df$x),xend=mean(df$x),y=0,yend=max(df$y))+
  geom_segment(x=0,xend=30,y=0,yend=0)+
  geom_point(data=tibble(x=mean(df$x),y=coef(mod)[1]),size=4)+
  scale_x_continuous(limits=c(0,7),
                     breaks=c(mean(df$x)-(2*sd(df$x)), mean(df$x)-sd(df$x), 
                              mean(df$x), 
                              mean(df$x)+sd(df$x), mean(df$x)+(2*sd(df$x))),
                     labels=c(-2,-1,0,1,2))+
  labs(title="Scaled X")+
  themedapr3()



mod = lm(y~x,df %>% mutate(x=x-5))
p4 = ggplot(df, aes(x=x,y=y))+geom_point()+
  geom_smooth(method="lm")+
  geom_smooth(method="lm",fullrange=T,lty="dotted")+
  #ylim(0,max(df$y))+
  geom_segment(x=5,xend=5,y=0,yend=max(df$y))+
  geom_segment(x=0,xend=30,y=0,yend=0)+
  geom_point(data=tibble(x=5,y=coef(mod)[1]),size=4)+
  scale_x_continuous(limits=c(0,7),breaks=0:7, labels=c(0:7)-5)+
  labs(title="x-5")+
  themedapr3()
p1 + p2 + p3 + p4 
```
]

---
# Big-fish little pond.. 

__things are different when we have clustered data__  

```{r include=FALSE}
set.seed(843)
doit<-1
while(doit){
  df<-as.data.frame(c())
  Ngroups = round(rnorm(1,10,0))
  NperGroup = rdunif(Ngroups, 10, 20)
  N = sum(NperGroup)
  dd<-MASS::mvrnorm(n=Ngroups, mu = c(0,0), Sigma = matrix(c(1,0,0,1),byrow = T, nrow=2))
  igs = map(seq_along(NperGroup), ~rep(.,NperGroup[.])) %>% unlist
  xxm = rnorm(Ngroups,0,1)
  xxm = rdunif(Ngroups, 2, 10)
  xxm = map(1:Ngroups, ~rep(xxm[.], NperGroup[.])) %>% unlist
  xgc = map(1:Ngroups, ~rnorm(NperGroup[.], 0, 3)) %>% unlist
  #xx = map(1:Ngroups, ~rep(rnorm(1,3,.6), NperGroup[.]))%>% unlist
  xx = xxm+xgc 
  l2p = sample(1:4, Ngroups, replace=T)
  l2p = map(1:Ngroups, ~rep(l2p[.], NperGroup[.])) %>% unlist
  
  e = rnorm(N, sd = 1)
    
  y = 0 +
      dd[igs,1]+
      -2*xxm+
      2*xgc + 
      dd[igs,2]*xgc +
      0*l2p +
      e
  d = data.frame(y,xxm,xgc,igs,l2p)
  df<-rbind(df,d)
  
  lmer(y ~ xxm + xgc + l2p + (1+xgc | igs), data =df,
       control=lmerControl(optimizer = "bobyqa")) -> m 
  print(VarCorr(m))
  
  t1 = attributes(VarCorr(m)[[1]])$stddev
  t2 = attributes(VarCorr(m)[[1]])$correlation
  
  if(!isSingular(m)){
    doit <- 0
  }
}

df %>% transmute(
  pond = paste0("pond_",igs),
  tyoe = l2p,
  self_esteem = round(3+scale(y)[,1]*.6,2),
  fish_weight = round(31+scale(xxm+xgc)[,1]*11),
  image = c("jk_img_sandbox/fish-161320_1280.png"),
) -> bflp
```

```{r echo=FALSE, fig.asp=.8, fig.align="center"}
ggplot(bflp, aes(x=fish_weight, y=self_esteem))+
  geom_point()+
  #geom_line(aes(group=pond))+
  geom_smooth(method="lm")+
  labs(x="Fish Weight (kg)",y="Self Esteem Scale (1-5)")+
  themedapr3()
```


???
is it better to be a big fish?

---
count:false
# Big-fish little pond 

__things are different when we have clustered data__  

```{r echo=FALSE, fig.asp=.8, fig.align="center"}
library(ggforce)
library(ggfx)

ggplot(bflp, aes(x=fish_weight, y=self_esteem))+
  with_blur(geom_point(),sigma=3)+
  with_blur(geom_line(aes(group=pond),alpha=.5),sigma=3)+
  geom_point(data=filter(bflp, pond=="pond_2"))+
  geom_line(data=filter(bflp, pond=="pond_2"))+
  #geom_smooth(method="lm")+
  labs(x="Fish Weight (kg)",y="Self Esteem Scale (1-5)")+
  themedapr3()+
  geom_mark_ellipse(aes(label = pond, filter = pond == "pond_2"),
                    con.arrow = arrow(ends = "last",length = unit(0.5, "cm")),
                    show.legend = FALSE)
```

---
# Centering predictors in MLM

```{r include=FALSE}
library(lme4)
set.seed(86)
doit<-1
while(doit){
  Ngroup2s = 10
  dd2<-MASS::mvrnorm(n=Ngroup2s, mu = c(0,0), Sigma = matrix(c(1,0,0,1),byrow = T, nrow=2))
  cor(dd2)
  i = 2
  df<-as.data.frame(c())
  for(i in 1:Ngroup2s){
    Ngroups = round(rnorm(1,10,0))
    Nxgroups = 2
    
    #NperGroup = rep(Nxgroups*nrep,Ngroups)
    NperGroup = rdunif(Ngroups, 5, 10)*Nxgroups
    N = sum(NperGroup)
    
    dd<-MASS::mvrnorm(n=Ngroups, mu = c(0,0), Sigma = matrix(c(1,0,0,1),byrow = T, nrow=2))
    ddb<-MASS::mvrnorm(n=Ngroups, mu = c(0,0), Sigma = matrix(c(1,0,0,1),byrow = T, nrow=2))
    ddx<-MASS::mvrnorm(n=Nxgroups, mu = c(0,0), Sigma = matrix(c(1,0,0,1),byrow = T, nrow=2))
    
    igs = map(seq_along(NperGroup), ~rep(.,NperGroup[.])) %>% unlist
    xgs = map(1:Ngroups, ~rep(1:Nxgroups,NperGroup[.]/Nxgroups)) %>% unlist
    #x = map(1:Ngroups, ~rep(1:NperGroup[.],Nxgroups)) %>% unlist
    xxm = rnorm(Ngroups,50,10)
    xxm = map(1:Ngroups, ~rep(xxm[.], NperGroup[.])) %>% unlist
    xgc = map(1:Ngroups, ~rnorm(NperGroup[.], 0, 3)) %>% unlist
    #xx = map(1:Ngroups, ~rep(rnorm(1,3,.6), NperGroup[.]))%>% unlist
    xx = xxm+xgc 
    
    l2p = sample(1:4, Ngroups, replace=T, prob = c(.2,.5,.3,.1))
    l2p = map(1:Ngroups, ~rep(l2p[.], NperGroup[.])) %>% unlist
    
    l3p = i %% 2
    e = rnorm(N, sd = 15)
    
    y = 0 +
      dd[igs,1]+
      dd2[i,1]+
      #ddx[xgs, 1] + 
      -10*xxm+
      #-.05*(xx2 %in% c(1,2,4))*xgc+
      -3*xgc + 
      dd[igs,2]*xgc +
      ddb[igs,2]*xxm + 
      2*dd2[i,2]*xxm +
      -3*l2p +
      2*l3p +
      e
    d = data.frame(y,xxm,xgc,igs,xgs,i, l2p,l3p)
    #ggplot(d,aes(x=x,y=y,group=factor(igs)))+facet_wrap(~xgs)+geom_path()
    d$ng2 = i
    df<-rbind(df,d)
  }
  df %>% filter(xgs == 1) %>%
  lmer(y ~ xxm + xgc + l2p + l3p + (1+xgc | ng2/igs), data =.,
       control=lmerControl(optimizer = "bobyqa")) -> m 
  print(VarCorr(m))
  t1 = attributes(VarCorr(m)[[1]])$stddev
  t2 = attributes(VarCorr(m)[[1]])$correlation
  t3 = attributes(VarCorr(m)[[2]])$stddev
  t4 = attributes(VarCorr(m)[[2]])$correlation
  
  if(!isSingular(m) & all(t1 != 0) & !(t2[lower.tri(t2)] %in% c(0,1,-1)) & all(t3 != 0) & !(t4[lower.tri(t4)] %in% c(0,1,-1)) ){
    doit <- 0
  }
}

df %>% filter(xgs==1) %>% transmute(
  tgu = 19 + (scale(y)[,1]*3), 
  phys = round(xxm + xgc),
  hospital = ng2,
  patient = igs,
  prioritylevel = l2p,
  private = l3p
) %>% filter(hospital%in%c(5,6)) %>% mutate(hospital=paste0("Hospital_",hospital), 
                                     patient = paste0(hospital,patient))-> tgudat
```

<br> 

```{r echo=FALSE, fig.asp=.8}
ggplot(tgudat, aes(x=phys, y=tgu,color=patient)) +
  geom_point(alpha=.4)+
  geom_line(aes(group=patient), alpha=.4)+
  labs(x="Daily amount of Physiotherapy\n(minutes)", y="Time Get up and Go test (seconds)")+
  themedapr3()+
  guides(color="none")
```

---
count:false
# Centering predictors in MLM

+ grand mean centering - doesn't do anything

```{r echo=FALSE, fig.asp=.8}
ggplot(tgudat, aes(x=scale(phys,scale=F), y=tgu,color=patient)) +
  geom_point(alpha=.4)+
  geom_line(aes(group=patient), alpha=.4)+
  labs(x="Daily amount of Physiotherapy\nbelow/above the recommended 60 minutes", y="Time Get up and Go test (seconds)")+
  themedapr3()+
  guides(color="none")
```

---
count:false
# Centering predictors in MLM

+ groups may have different mean values for a predictor.

```{r echo=FALSE, fig.asp=.8}
tgudat %>% group_by(patient) %>% summarise(s=sd(phys), phys = mean(phys), tgu = mean(tgu)) %>% ungroup -> sdff
ggplot(tgudat, aes(x=phys, y=tgu,color=patient)) +
  geom_point(alpha=.2)+
  geom_line(aes(group=patient), alpha=.1)+
  geom_point(data=sdff,size=4)+
  geom_errorbarh(data=sdff,aes(xmin=phys-(2*s), xmax=phys+(2*s)))+
  labs(x="Daily amount of Physiotherapy\n(minutes)", y="Time Get up and Go test (seconds)")+
  themedapr3()+guides(color="none")+xlim(c(20,85))
```

---
# Centering predictors in MLM

+ groups may have different mean values for a predictor.

```{r echo=FALSE, fig.asp=.8}
tgudat %>% group_by(patient) %>% summarise(s=sd(phys), phys = mean(phys), tgu = mean(tgu)) %>% ungroup -> sdff
tgudat %>% group_by(patient) %>% mutate(m=mean(phys),gc=phys-mean(phys)) %>% ungroup %>%
ggplot(., aes(x=gc, y=tgu,color=patient)) +
  geom_point(alpha=.7)+
  geom_line(aes(group=patient), alpha=.1)+
  geom_point(data=sdff,x=0,size=4)+
  geom_errorbarh(data=sdff,aes(x=0,xmin=0-(2*s), xmax=0+(2*s)), alpha=.5)+
  labs(x="Daily amount of Physiotherapy\n(*minutes from patient average*)", y="Time Get up and Go test (seconds)")+
  themedapr3()+guides(color="none")
```

---
# Separating out within and between effects

.pull-left[

<center>__ $\bar{x}_i$ __</center>
```{r echo=FALSE, fig.asp=.8, fig.align="center"}
tgudat %>% group_by(patient) %>% mutate(
  xbar = mean(phys),
  xbari = phys - mean(phys)
) -> tgudat

ggplot(tgudat, aes(x=xbar, y=tgu,color=patient)) +
  geom_point(alpha=.4)+
  geom_line(aes(group=patient), alpha=.4)+
  labs(x="*Average* daily amount of Physiotherapy\n(minutes)", y="Time Get up and Go test (seconds)")+
  themedapr3()+guides(color="none")
```
]
.pull-right[
<center>__ $x_{ij} - \bar{x}_i$ __</center> 
```{r echo=FALSE, fig.asp=.8, fig.align="center"}
tgudat %>% group_by(patient) %>% mutate(
  xbar = mean(phys),
  xbari = phys - mean(phys)
) -> tgudat

ggplot(tgudat, aes(x=xbari, y=tgu,color=patient)) +
  geom_point(alpha=.4)+
  geom_line(aes(group=patient), alpha=.4)+
  labs(x="Daily amount of Physiotherapy\n(*deviations from patient average* - minutes)", y="Time Get up and Go test (seconds)")+
  themedapr3()+guides(color="none")
```
]

---
# Separating out within and between effects
### Big-fish little pond 

```{r echo=FALSE, fig.height=5, fig.width=12, fig.align="center"}
bflp %>% group_by(pond) %>% mutate(
  xbar = mean(fish_weight),
  xgc = fish_weight - mean(fish_weight)
) %>% ungroup -> bflpa

ggplot(bflpa, aes(x=xgc, y=self_esteem))+
  geom_point()+
  #geom_line(aes(group=pond),alpha=.5)+
  geom_smooth(method="lm")+
  labs(x="Weight difference from pond average (kg)",y="Self Esteem Scale (1-5)")+
  themedapr3() +
ggplot(bflpa, aes(x=xbar, y=self_esteem))+
  geom_point()+
  geom_smooth(method="lm")+
  labs(x="Pond Average Fish Weight (kg)",y="Self Esteem Scale (1-5)")+
  themedapr3()
```



---
# Separating out within and between effects

.pull-left[
**RE model**  
$$
\begin{align}
y_{ij} &= \beta_{0i} + \beta_{1}(x_j) + \varepsilon_{ij} \\
\beta_{0i} &= \gamma_{00} + \zeta_{0i} \\
... \\
\end{align}
$$


```{r}
rem <- lmer(tgu ~ phys + 
              (1 | patient), data=tgudat)
fixef(rem)
```


]

.pull-right[
**Within-between model**  
$$
\begin{align}
y_{ij} &= \beta_{0i} + \beta_{1}(\bar{x}_i) + \beta_2(x_{ij} - \bar{x}_i)+ \varepsilon_{ij} \\
\beta_{0i} &= \gamma_{00} + \zeta_{0i} \\
... \\
\end{align}
$$

```{r}
tgudat <- 
  tgudat %>% group_by(patient) %>%
    mutate(
      physgrpm = mean(phys),
      physgrpc = phys - mean(phys)
    ) %>% ungroup

wbm <- lmer(tgu ~ physgrpm + physgrpc + 
              (1 | patient), data=tgudat)
fixef(wbm)
```



]

???
Problems, however, arise if you fail to include the group means in the model when using the raw scale or grand-mean centered predictor. If you do that, you will get a mish mosh effect estimate for the Level 1 predictor that represents neither represents the between-group nor the within-group effect. Instead, it confounds these two effects together into a single value that may not resemble either. To make matters worse, this mish mosh also doesn’t represent the total effect, as it weights the within- and between-group effects differently. The obtained estimate is difficult to interpret, outside of a few special cases.

---
# Separating out within and between effects

.pull-left[
**Within-between model**  
$$
\begin{align}
y_{ij} &= \beta_{0i} + \beta_{1}(\bar{x}_i) + \beta_2(x_{ij} - \bar{x}_i)+ \varepsilon_{ij} \\
\beta_{0i} &= \gamma_{00} + \zeta_{0i} \\
... \\
\end{align}
$$

```{r}
tgudat <- 
  tgudat %>% group_by(patient) %>%
    mutate(
      physgrpm = mean(phys),
      physgrpc = phys - mean(phys)
    ) %>% ungroup

wbm <- lmer(tgu ~ physgrpm + physgrpc + 
              (1 | patient), data=tgudat)
fixef(wbm)
```


]

.pull-right[
```{r}
broom.mixed::augment(wbm) %>%
  ggplot(.,aes(x=physgrpm+physgrpc, y=.fitted, group=patient))+
  geom_line() +
  themedapr3()
```
]

---
# Summary

---
class: inverse, center, middle, animated, rotateInDownLeft

# End of Part 1

---
class: inverse, center, middle

<h2 style="text-align: left;opacity:0.3;">Part 1: Centering Predictors</h2>
<h2>Part 2: GLMM</h2>

---
# Recall lm() and glm()

.pull-left[
lm() is for gaussian/"normal" outcomes

$y ~ xb + $

]
.pull-right[

glm() is more generalised. 

glm(y~x, family = gaussian) = lm()
glm(y~x, family = binomial(link = "logit"))

link function to model outcomes different distributions

]

---
# lmer() and glmer()

.pull-left[
lmer() is for gaussian/"normal" outcomes

$y ~ xb + e$  

] 
.pull-right[

glmer() is more generalised. 

glmer(y~x, family = gaussian) = lmer
glmer(y~x, family = binomial(link = "logit"))

link function to model outcomes different distributions

]

---
# fitting a glmer()

```{r include=F}
set.seed(5)
doit<-1
while(doit){
  df<-as.data.frame(c())
  Ngroups = round(rnorm(1,15,0))
  NperGroup = rdunif(Ngroups, 7, 15)
  N = sum(NperGroup)
  dd<-MASS::mvrnorm(n=Ngroups, mu = c(0,0), Sigma = matrix(c(1,0,0,1),byrow = T, nrow=2))
  igs = map(seq_along(NperGroup), ~rep(.,NperGroup[.])) %>% unlist
  x = map(1:Ngroups, ~rnorm(NperGroup[.],0,1)) %>% unlist
  #x = map(1:Ngroups, ~sample(0:1, size=NperGroup[.],replace=T)) %>% unlist
  l2p = map(1:Ngroups, ~rep(sample(0:1,1,prob=c(.8,.2)), NperGroup[.])) %>% unlist
  e = rnorm(N, sd = 1)
    
  lp = 0 +
    dd[igs,1]+
    .9*x + 
    #dd[igs,2]*x +
    1*l2p
  ybin = rbinom(N, size = 1, prob = plogis(lp))
  
  d = data.frame(ybin,x,igs,l2p,lp=lp+rnorm(N,0,.01))
  
  df<-rbind(df,d)
  
  glmer(ybin ~ x + l2p + (1+x | igs), data =df, family="binomial",
       control=glmerControl(optimizer = "bobyqa")) -> m 
  print(VarCorr(m))
  
  t1 = attributes(VarCorr(m)[[1]])$stddev
  t2 = attributes(VarCorr(m)[[1]])$correlation
  
  if(!isSingular(m)){
    doit <- 0
  }
}

```






---
# glmer fits can be tricky

scale parameters may help
why? think of a bowl, trying to find min point.


---
# interpretation

- model is non-linear (logit)
- effects are conditional upon random effects

???
this means that effects are cluster-specific. 
effects for subjects with the same random effect value


---
# Population average vs Cluster Specific  

- **Population average** effects are the association between $x$ and $y$, averaged over clusters

- **Cluster specific** effects are the association between $x$ and $y$, *holding the cluster constant*. 

<!-- cluster specific:  -->
<!-- represents the odds of the person being employed if married compared with the odds of the SAME person being employed if not married. -->

<!-- population average: -->
<!-- represents the odds of an AVERAGE married person being employed compared with the odds of an AVERAGE unmarried person being employed. -->

<!-- Rather than saying “AVERAGE”, sometimes I speak loosely and say the odds of a married person “picked at random” being employed compared with the odds of another unmarried person “picked at random” being employed. -->

---
# why?

consider..

identity link function   
`lmer(respiratory_rate ~ treatment + (1|hospital))`

patient $j$ from hospital $i$ = control  
patient $j'$ from hospital $i'$ = treatment  

how does our model explain difference in outcome between patient $j$ and patient $j'$?  
outcome for patient $j'$ should be the "the effect of having treatment" plus the random effect differences between hospitals i and i'  

model for patient $j$ from hospital $i$  
$\hat{y}_{ij} = (\gamma_{00} + \zeta_{0i}) + \beta_1 (Treatment_{ij} = 0)$

model for patient $j'$ from hospital $i'$  
$\hat{y}_{i'j'} = (\gamma_{00} + \zeta_{0i'}) + \beta_1 (Treatment_{i'j'} = 1)$

difference:  
$\hat{y}_{i'j'} - \hat{y}_{ij} = \beta_1 + (\zeta_{0i'} - \zeta_{0i})$



???
the zeta differences here will be, on average 0. 
hist(replicate(1000, mean(map_dbl(combn(rnorm(100),2, simplify=F), diff))),breaks=20)


---
# why?

consider..

non-linear link function  
`glmer(needs_op ~ treatment + (1|hospital), family="binomial")`

patient j from hospital i = control  
patient j' from hospital i' = treatment  
we're now modeling difference __in probability of outcome__ between patient j and patient j'?  
probability of outcome for patient j' is "the effect of having treatment" plus the random effect differences between hospitals i and i'  

model for patient X from hospital $i$  
$log \left( \frac{p_{ij}}{1 - p_{ij}} \right)  = (\gamma_{00} + \zeta_{0i}) + \beta_1 (Treatment_{ij} = 0)$

model for patient Y from hospital $i'$  
$log \left( \frac{p_{i'j'}}{1 - p_{i'j'}} \right) = (\gamma_{00} + \zeta_{0i'}) + \beta_1 (Treatment_{i'j'} = 1)$

difference (log odds):  
$log \left( \frac{p_{i'j'}}{1 - p_{i'j'}} \right) - log \left( \frac{p_{ij}}{1 - p_{ij}} \right) = \beta_1 + (\zeta_{0i'} - \zeta_{0i})$

---
# why?

consider..

non-linear link function 
`glmer(needs_op ~ treatment + (1|hospital), family="binomial")`

patient j from hospital i = control  
patient j' from hospital i' = treatment  
we want the difference between patients x and y to be "the effect of having treatment", plus the adjustment between hospitals i and i'  

model for patient X from hospital $i$  
$log \left( \frac{p_{ij}}{1 - p_{ij}} \right)  = (\gamma_{00} + \zeta_{0i}) + \beta_1 (Treatment_{ij} = 0)$

model for patient Y from hospital $i'$  
$log \left( \frac{p_{i'j'}}{1 - p_{i'j'}} \right) = (\gamma_{00} + \zeta_{0i'}) + \beta_1 (Treatment_{i'j'} = 1)$

difference (odds ratio):  
$\frac{p_{i'j'}/(1 - p_{i'j'})}{p_{ij}/(1 - p_{ij})} = \exp(\beta_1 + (\zeta_{0i'} - \zeta_{0i})) \neq \exp(\beta_1)$

```{r eval=F,echo=F}
#fixed effect is 1.2
gamma00 = 1.2
# 10 groups. random effects are:
zetas = rnorm(10)
# the difference between random chosen observation from group i with x = 0, 
# and randomly chosen observation from group i' with x = 1
# is gamma_00 + (ranef_i' - ranef_i)
# these are all the (ranef_i' - ranef_i)'s
map_dbl(combn(zetas, 2, simplify=F),diff)
# so the expected value, because we assume zetas are N(0,s), is gamma00:
hist(replicate(1e4, mean(gamma00 + map_dbl(combn(rnorm(10),2, simplify=F), diff))), breaks=20)
# hence beta (e.g. gamma + zeta) is the effect of x "averaged across clusters"

##### 
# BUT WAIT!
# glmm says "what about me?"
# logistic model means gamma00 and zetas are all in log-odds. 
# the exponent of gamma_00 + (ranef_i' - ranef_i) is not the exponent of gamma_00
gamma00 = 1.2 # equivalent to odds ratio of 3.3
zetas = rnorm(10) # random deviations (in log odds) around gamma 
hist(replicate(1e4, mean(exp(gamma00 + map_dbl(combn(rnorm(10),2, simplify=F), diff)))), breaks=20)
```


---
# interpreting coefficients in glmer

`glmer(needs_op ~ treatment + (1|hospital), family="binomial")` 

Hence, the interpretation of $e^{\beta_1}$ is not the odds ratio for the effect of treatment averaged over hospitals, but rather for patients from the same hospital. 

---
# coefficients with an identity link (lm/lmer)

coef(lm(y~x,d))
$\beta$ denotes the change in the average $y$ when $x$ is increased by one unit and all other covariates are fixed.

 in lmer()
$\beta$ denotes the change in the average $y$ when $x$ is increased by one unit.

Advantageous feature: population + cluster-specific predictions
 $\gamma$ describes mean changes in the population
 $\gamma + \zeta_i$ describes individual cluster effects
 
in glmm
 $e^{\gamma + \zeta_i}$ describes individual cluster effects
 $e^\gamma$ describes mean effect in the population, holding cluster constant
 
 

```{r echo=FALSE}

library(ggExtra)

crq <- read_csv("https://uoepsy.github.io/data/crqdata.csv") %>% rename(slope=crq)
m <- lmer(emot_dysreg ~ slope + (1 + slope| schoolid), data = crq)
fixef(m)


as.data.frame(coef(m)$schoolid) %>%
  ggplot(.,aes(x=`(Intercept)`, y = slope)) + 
  geom_vline(xintercept = fixef(m)[1], lwd=1,lty="longdash", color="#88B04B") +
  geom_hline(yintercept = fixef(m)[2], lwd=1,lty="longdash",color="#88B04B") + 
  geom_point(size=2) +
  geom_rug(sides="tr") + 
  themedapr3() -> p
ggMarginal(p, color="#88B04B", lwd=2)
```


---
# Summary


---
class: inverse, center, middle, animated, rotateInDownLeft

# End

 