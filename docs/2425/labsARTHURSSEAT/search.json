[
  {
    "objectID": "02ex.html",
    "href": "02ex.html",
    "title": "W2 Exercises: Introducing MLM",
    "section": "",
    "text": "These first set of exercises are not “how to do analyses with multilevel models” - they are designed to get you thinking, and help with an understanding of how these models work.\n\n\nQuestion 1\n\n\nRecall the data from last week’s exercises. Instead of looking at the roles A, B and C, we’ll look in more fine grained detail at the seniority. This is mainly so that we have a continuous variable to work with as it makes this illustration easier.\nThe chunk of code below shows a function for plotting that you might not be familiar with - stat_summary(). This takes the data in the plot and “summarises” the Y-axis variable into the mean at every unique value on the x-axis. So below, rather than having points for every observation, we have the mean of wp at every value of seniority:\n\nlibrary(tidyverse)\njsup &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\nggplot(jsup, aes(x = seniority, y = wp, col = role)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nBelow is some code that fits a model of the workplace-pride predicted by seniority level. Line 2 then gets the ‘fitted’ values from the model and adds them as a new column to the dataset, called pred_lm. The fitted values are what the model predicts for every individual observation.\nLines 4-7 then plot the data, split up by each department, and adds lines showing the model fitted values.\nRun the code and check that you get a plot. What do you notice about the lines?\n\nlm_mod &lt;- lm(wp ~ seniority, data = jsup)\njsup$pred_lm &lt;- predict(lm_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_lm), col = \"red\")\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. We should get something like this:\n\nlm_mod &lt;- lm(wp ~ seniority, data = jsup)\njsup$pred_lm &lt;- predict(lm_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_lm), col = \"red\")\n\n\n\n\n\n\n\n\nNote that the lines are exactly the same for each department. This makes total sense, because the model (which is where we’ve got the lines from) completely ignores the department variable!\n\n\n\n\nQuestion 2\n\n\nBelow are 3 more code chunks that all 1) fit a model, then 2) add the fitted values of that model to the plot.\nThe first model is a ‘no-pooling’ approach, similar to what we did in last week’s exercises - adding in dept as a predictor.\nThe second and third are multilevel models. The second fits random intercepts by-department, and the third fits random intercepts and slopes of seniority.\nCopy each chunk and run through the code. Pay attention to how the lines differ.\n\n\nCode\nfe_mod &lt;- lm(wp ~ dept + seniority, data = jsup)\njsup$pred_fe &lt;- predict(fe_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_fe), col = \"blue\")\n\n\n\n\nCode\nlibrary(lme4)\nri_mod &lt;- lmer(wp ~ seniority + (1|dept), data = jsup)\njsup$pred_ri &lt;- predict(ri_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_ri), col = \"green\")\n\n\n\n\nCode\nrs_mod &lt;- lmer(wp ~ seniority + (1 + seniority|dept), data = jsup)\njsup$pred_rs &lt;- predict(rs_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_rs), col = \"orange\")\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. The first model has an adjustment for each department (we can see this in the coefficients if we want). What this means is that the line for each department is shifted up or down. We can see that the lines are now shifted up for departments like UKSA and FSA, and down for UKSC and OFSTED:\n\nfe_mod &lt;- lm(wp ~ dept + seniority, data = jsup)\njsup$pred_fe &lt;- predict(fe_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_fe), col = \"blue\")\n\n\n\n\n\n\n\n\nThis next one looks very similar to the previous one, but it is conceptually doing something a bit different. Rather than separating out every individual department, we are modelling a distribution of deviations for each department from some average.\n\nlibrary(lme4)\nri_mod &lt;- lmer(wp ~ seniority + (1|dept), data = jsup)\njsup$pred_ri &lt;- predict(ri_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_ri), col = \"green\")\n\n\n\n\n\n\n\n\nFinally, we can add in the random slopes of seniority. In this model, we are not only allowing departments to vary in their average workplace-pride, but we are also allowing them to vary in the association between seniority and workplace pride. Some departments (OFQUAL, OFSTED, ORR) have a negative association, some have a flatter association (e.g, FSA, UKSA etc).\n\nrs_mod &lt;- lmer(wp ~ seniority + (1 + seniority|dept), data = jsup)\njsup$pred_rs &lt;- predict(rs_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_rs), col = \"orange\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFrom the previous questions you should have a model called ri_mod.\nBelow is a plot of the fitted values from that model. Rather than having a separate facet for each department as we did above, I have put them all on one plot. The thick black line is the average intercept and slope of the departments lines.\nIdentify the parts of the plot that correspond to A1-4 in the summary output of the model below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nChoose from these options:\n\nwhere the black line cuts the y axis (at x=0)\n\nthe slope of the black line\n\nthe standard deviation of the distances from all the individual datapoints (employees) to the line for the department in which it works.\n\nthe standard deviation of the distances from all the individual department lines to the black line\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1 = the standard deviation of the distances from all the individual department lines to the black line\n\nA2 = the standard deviation of the distances from all the individual datapoints (employees) to the line for the department in which it works.\nA3 = where the black line cuts the y axis\n\nA4 = the slope of the black line\n\n\n\n\n\nOptional Extra\n\n\nBelow is the model equation for the ri_mod model.\nIdentify the part of the equation that represents each of A1-4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n\\text{For Employee }j\\text{ from Dept }i & \\\\\n\\text{Level 1 (Employee):}& \\\\\n\\text{wp}_{ij} &= b_{0i} + b_1 \\cdot \\text{seniority}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (Dept):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{Where:}& \\\\\n\\zeta_{0i} &\\sim N(0,\\sigma_{0}) \\\\\n\\varepsilon &\\sim N(0,\\sigma_{e}) \\\\\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nChoose from:\n\n\\(\\sigma_{\\varepsilon}\\)\n\n\\(b_{1}\\)\n\n\\(\\sigma_{0}\\)\n\n\\(\\gamma_{00}\\)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nA1 = \\(\\sigma_{0}\\)\n\nA2 = \\(\\sigma_{\\varepsilon}\\)\n\nA3 = \\(\\gamma_{00}\\)\n\nA4 = \\(b_{1}\\)",
    "crumbs": [
      "W2 Exercises: Introducing MLM"
    ]
  },
  {
    "objectID": "02ex.html#footnotes",
    "href": "02ex.html#footnotes",
    "title": "W2 Exercises: Introducing MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is like taking predict() from the model, and then then grouping by age, and calculating the mean of those predictions. However, we can do this more easily using augment() and then some fancy stat_summary() in ggplot↩︎\nprovided that the confidence intervals and p-values are constructed using the same methods↩︎",
    "crumbs": [
      "W2 Exercises: Introducing MLM"
    ]
  },
  {
    "objectID": "r07_qdata.html",
    "href": "r07_qdata.html",
    "title": "R7: Questionnaire Data Wrangling",
    "section": "",
    "text": "Questionnaire data often comes to us in ‘wide’ format, which is often how we want it for many of the analytical methods we use with questionnaire data. However, working with data in the wide format comes with some specific challenges that generally arise because we have lots and lots of variables.\nBelow we will walk through some of the common ways we want to wrangle and clean questionnaire data.",
    "crumbs": [
      "Week 7",
      "R7: Questionnaire Data Wrangling"
    ]
  },
  {
    "objectID": "r07_qdata.html#footnotes",
    "href": "r07_qdata.html#footnotes",
    "title": "R7: Questionnaire Data Wrangling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically this is pronounced “LICK-URT” and not “LIE-KURT”. It’s named after Dr Rensis Likert, and that’s how he pronounced his name!↩︎",
    "crumbs": [
      "Week 7",
      "R7: Questionnaire Data Wrangling"
    ]
  },
  {
    "objectID": "05ex.html",
    "href": "05ex.html",
    "title": "W5 Exercises: Bringing it all together",
    "section": "",
    "text": "Take your pick!\n\nQuestion 1\n\n\nYou can find all the datasets that we have seen (and more!) as an additional doc in the readings page.\nFor each one, there is a quick explanation of the study design which also details the research aims of the project.\nPick one of the datasets and, in your groups:\n\nexplore the data, and do any required cleaning (most of them are clean already)\nconduct an analysis to address the research aims\nwrite a short description of the sample data (see Chapter 11 #the-sample-data)\nwrite a short explanation of your methods (see Chapter 11 #the-methods)\nwrite a short summary of your results, along with suitable visualisations and tables (see Chapter 11 #the-results)\nPost some of your writing on Piazza and we can collectively discuss it!\n\n\nEach of the datasets contains some tags that give an indication of the type of study. Anything with either “#binomial-outcome” or “#non-linear” you can ignore as we have not covered this in DAPR3.\n\n\n\n\n\n\n\n\n\n\n\nFlashcards: lm to lmer\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\n\nFormula:\n\n\\(y_i = b_0 + b_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + is always there unless we specify otherwise (e.g., by using 0 +).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustered (multi-level) data\n\n\n\n\n\nWhen our data is clustered (or ‘grouped’) such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nIf we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nBy including a random-intercept term, we are letting our model estimate random variability around an average parameter (represented by the fixed effects) for the clusters.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = b_{0i} + b_{1} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(b_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nWe can now see that the intercept estimate \\(b_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has an adjustment for each subject.\nEach subject’s line has been moved up or down accordingly.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPooling & Shrinkage\n\n\n\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different:\n\n\n\n\n\n\n\n\n\nWhy? One of the benefits of multilevel models is that our cluster-level estimates are shrunk towards the average depending on a) the amount of across-cluster variation and b) the number of datapoints in clusters.\nIn another way, we can think of the multilevel model as “borrowing strength” from what we know about e.g., clusters 1-19 to inform what we think about cluster 20. This also gets termed “partial pooling” because we are partially combining all the information across clusters to get an average, but still allowing those clusters to vary.\n\n\n\n\n\n\n\n\nmodel\npooling\nexplanation\n\n\n\n\nlm(y~x)\ncomplete\nall information across clusters is combined (pooled) together and a line is fitted\n\n\nlm(y~group + x)\nno\ninformation is split up into clusters, and cluster differences are estimated. observations from cluster \\(i\\) contribute only to estimates about cluster \\(i\\)\n\n\nlmer(y~x+(1|group))\npartial\ninformation is combined (pooled) but cluster-level variation is modelled. cluster level estimates are shrunk towards the average depending upon how distinct the clustering is, and how much data a cluster has\n\n\n\n\n\n\n\n\n\n\n\n\nRandom slopes\n\n\n\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = b_{0i} + b_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(b_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(b_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\nAs with the intercept \\(b_{0i}\\), the slope of the predictor \\(b_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel parameters: Fixed effects\n\n\n\n\n\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the groups all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n    405.790      -0.672 \n\n\nThe fixed effects are the estimated intercept and slopes for the average group, around which groups vary.\n\n\n\n\n\n\n\n\n\nModel parameters: Variance components\n\n\n\n\n\nAs well as estimating the fixed effects, multilevel models are also defined by the “variance components”. These are the variances and covariances of the random effects. Looking at these we can ask: how much do groups vary in around the fixed intercept? and around the fixed slope? Do groups with higher intercepts also have higher slopes (this is the correlation).\nWe can think of these as the width of the distributions of group deviations from each fixed effect\n\n\n\n\n\n\n\n\n\nWe can extract these using the VarCorr() function, and we can also see them in the “random effects” part of the summary() output from a model.\n\nVarCorr(random_slopes_model)\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 72.72         \n          x1           1.36    -0.35\n Residual             25.74         \n\n\n\nRemember, variance is just standard deviation squared!\n\n\n\n\n\n\n\n\n\n\nGroup-specific random effects\n\n\n\n\n\nThe plots below show the fitted values for each subject from each model that we have gone through in these expandable boxes (simple linear regression, random intercept, and random intercept & slope):\n\n\n\n\n\n\n\n\n\nIn the random-intercept model (center panel), the differences from each of the subjects’ intercepts to the fixed intercept (thick green line) have mean 0 and standard deviation \\(\\sigma_0\\). The standard deviation (and variance, which is \\(\\sigma_0^2\\)) is what we see in the random effects part of our model summary (or using the VarCorr() function).\nRandom effects:\n Groups   Name        Variance Std.Dev.\n subject  (Intercept) 5207     72.2    \n Residual             1512     38.9    \nNumber of obs: 185, groups:  subject, 20\n\n\n\n\n\n\n\n\n\nIn the random-slope model (right panel), the same is true for the differences from each subjects’ slope to the fixed slope. As our fixed effects are:\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n    405.790      -0.672 \n\n\nWe can extract the deviations for each group from the fixed effect estimates using the ranef() function. These are the deviations from the overall intercept (\\(\\widehat \\gamma_{00} = 405.79\\)) and slope (\\(\\widehat \\gamma_{10} = -0.672\\)) for each subject \\(i\\).\nSo the first entry, sub_308, has an intercept 31.33 above the fixed intercept of 405.79, and has a slope that is -1.44 below the fixed slope of -0.672.\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)      x1\nsub_308       31.33 -1.4400\nsub_309      -28.83  0.4184\nsub_310        2.71  0.0599\nsub_330       59.40  0.3853\nsub_331       74.96  0.1739\nsub_332       91.09 -0.2346\nsub_333       97.85 -0.1906\nsub_334      -54.19 -0.5585\nsub_335      -16.90  0.9207\nsub_337       52.22 -1.1660\nsub_349      -67.76 -0.6844\nsub_350       -5.82 -1.2379\nsub_351       61.20  0.0550\nsub_352       -7.91 -0.6650\nsub_369      -47.64 -0.4681\nsub_370      -33.12 -1.1100\nsub_371       77.58 -0.2040\nsub_372      -36.39 -0.4583\nsub_373     -197.58  1.7990\nsub_374      -52.20  4.6051\n\nwith conditional variances for \"subject\" \n\n\n\n\n\n\n\n\n\n\n\nGroup-specific coefficients\n\n\n\n\n\nWe can see the estimated intercept and slope for each subject \\(i\\) specifically, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)     x1\nsub_308         437 -2.112\nsub_309         377 -0.254\nsub_310         409 -0.612\nsub_330         465 -0.287\nsub_331         481 -0.498\nsub_332         497 -0.907\nsub_333         504 -0.863\nsub_334         352 -1.231\nsub_335         389  0.248\nsub_337         458 -1.838\nsub_349         338 -1.357\nsub_350         400 -1.910\nsub_351         467 -0.617\nsub_352         398 -1.337\nsub_369         358 -1.140\nsub_370         373 -1.782\nsub_371         483 -0.876\nsub_372         369 -1.131\nsub_373         208  1.127\nsub_374         354  3.933\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nNotice that the above are the fixed effects + random effects estimates, i.e. the overall intercept and slope + deviations for each subject.\n\ncbind(\n  int = fixef(random_slopes_model)[1] + \n    ranef(random_slopes_model)$subject[,1],\n  slope = fixef(random_slopes_model)[2] + \n    ranef(random_slopes_model)$subject[,2]\n)\n\n      int  slope\n [1,] 437 -2.112\n [2,] 377 -0.254\n [3,] 409 -0.612\n [4,] 465 -0.287\n [5,] 481 -0.498\n [6,] 497 -0.907\n [7,] 504 -0.863\n [8,] 352 -1.231\n [9,] 389  0.248\n[10,] 458 -1.838\n[11,] 338 -1.357\n[12,] 400 -1.910\n[13,] 467 -0.617\n[14,] 398 -1.337\n[15,] 358 -1.140\n[16,] 373 -1.782\n[17,] 483 -0.876\n[18,] 369 -1.131\n[19,] 208  1.127\n[20,] 354  3.933\n\n\n\n\n\n\n\n\n\n\n\nPlotting random effects\n\n\n\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms &lt;- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssumptions, Influence\n\n\n\n\n\nIn the simple linear model \\(\\color{red}{y} = \\color{blue}{b_0 + b_1(x)} \\color{black}{+ \\varepsilon}\\), we distinguished between the systematic model part \\(b_0 + b_1(x)\\), around which observations randomly vary (the \\(\\varepsilon\\) part) - i.e. \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}}\\) \\(+ \\text{error}\\).\nIn the multi-level model, our random effects are another source of random variation: \\(\\color{red}{\\text{outcome}}\\) = \\(\\color{blue}{\\text{model}}\\) \\(+ \\text{group-error} + \\text{individual-error}\\). As such, random effects are another form of residual, and our assumptions of zero mean constant variance apply at both levels of residuals (see Figure 1).\n\n\n\n\n\n\n\n\nFigure 1: The black dashed lines show our model assumptions.\n\n\n\n\n\n\nWe can assess these normality of both resid(model) and ranef(model) by constructing plots using functions such as hist(), qqnorm() and qqline().\n\nWe can also use plot(model, type=c(\"p\",\"smooth\")) to give us our residuals vs fitted plot (smooth line should be horizontal at approx zero, showing zero mean).\n\nplot(model, form = sqrt(abs(resid(.))) ~ fitted(.), type = c(\"p\",\"smooth\")) will give us our scale-location plot (smooth line should be horizontal, showing constant variance).\n\nWe can also use the check_model() function from the performance package to get lots of info at once:\n\nlibrary(performance)\ncheck_model(random_slopes_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\nTo get p-values for our coefficients, there are lots of different ways (see Optional Chapter 3 if you’re interested).\nFor DAPR3, we are recommending using the “Satterthwaite” method, which can be done by re-fitting the model the lmerTest package:\n\nrandom_slopes_model2 &lt;- lmerTest::lmer( outcome ~ 1 + x1 + (1+x1|subject), data=dat)\nsummary(random_slopes_model2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: outcome ~ 1 + x1 + (1 + x1 | subject)\n   Data: dat\n\nREML criterion at convergence: 1862\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.069 -0.417 -0.014  0.431  5.226 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n subject  (Intercept) 5287.68  72.72         \n          x1             1.86   1.36    -0.35\n Residual              662.33  25.74         \nNumber of obs: 185, groups:  subject, 20\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)  405.790     16.666  18.045   24.35    3e-15 ***\nx1            -0.672      0.313  16.757   -2.15    0.047 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n   (Intr)\nx1 -0.370\n\n\nWe can also conduct model comparisons by doing a likelihood ratio test. This can be useful to test multiple coefficients at once. The clearest way to think about this is to start with the full model and remove the bits you want to test.\nTypically, as we are interested in testing the fixed part, our ‘restricted model’ might have random slopes of predictors that are not in the fixed effects, which looks weird but is okay because we’re just using that model as a comparison point:\n\nmodel2 &lt;- lmer( outcome ~ 1 + x1 + x2+ (1+x1|subject), data=dat)\nmodel2.0 &lt;- lmer( outcome ~ 1 + (1+x1|subject), data=dat)\n\nanova(model2.0, model2)\n\n\n\n\n\n\n\n\n\n\nVisualising Model Fitted values\n\n\n\n\n\nThe model fitted (or “model predicted”) values can be obtained using predict() (returning just the values) or broom.mixed::augment() (returning the values attached to the data that is inputted to the model).\nTo plot, them, we would typically like to plot the fitted values for each group (e.g. subject)\n\nlibrary(broom.mixed)\naugment(random_slopes_model) |&gt;\n  ggplot(aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Fixed Effects\n\n\n\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. A good option is to use the effects package to construct a dataframe of the linear prediction across the values of a predictor, plus confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\nWe might then want to combine this with out plot of fitted values to make a plot that shows both the estimates for the average group (this is the fixed effects part) and the amount to which groups vary around that average (this we can see with the fitted values plot)\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\n\naugment(random_slopes_model) |&gt;\n  ggplot(aes(x=x1))+\n  geom_line(aes(y=.fitted,group=subject), alpha=.1) + \n  geom_line(data = ef, aes(y=fit))+\n  geom_ribbon(data = ef, aes(y=fit,ymin=lower,ymax=upper), \n              col=\"red\", fill=\"red\",alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested and Crossed structures\n\n\n\n\n\nThe same principle we have seen for one level of clustering can be extended to clustering at different levels (for instance, observations are clustered within subjects, which are in turn clustered within groups).\nConsider the example where we have observations for each student in every class within a number of schools:\n\n\n\n\n\n\n\n\n\nQuestion: Is “Class 1” in “School 1” the same as “Class 1” in “School 2”?\nNo.\nThe classes in one school are distinct from the classes in another even though they are named the same.\nThe classes-within-schools example is a good case of nested random effects - one factor level (one group in a grouping varible) appears only within a particular level of another grouping variable.\nIn R, we can specify this using:\n(1 | school) + (1 | class:school)\nor\n(1 | school) + (1 | school:class)\nConsider another example, where we administer the same set of tasks at multiple time-points for every participant.\nQuestion: Are tasks nested within participants?\nNo.\nTasks are seen by multiple participants (and participants see multiple tasks).\nWe could visualise this as the below:\n\n\n\n\n\n\n\n\n\nIn the sense that these are not nested, they are crossed random effects.\nIn R, we can specify this using:\n(1 | subject) + (1 | task)\n\nNested vs Crossed\nNested: Each group belongs uniquely to a higher-level group.\nCrossed: Not-nested.\n\nNote that in the schools and classes example, had we changed data such that the classes had unique IDs (e.g., see below), then the structures (1 | school) + (1 | class) and (1 | school) + (1 | school:class) would give the same results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\n\n\n\n\nMLM allows us to model effects in the linear model as varying between groups. Our coefficients we remember from simple linear models (the \\(b\\)’s) are modelled as a distribution that has an overall mean around which our groups vary. We can see this in Figure 2, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 2 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\n\n\n\nFigure 2: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(b\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{b_{0i} \\cdot 1 + b_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{b_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{b_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is $= ({0i}, {1i}):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho_{01}1 \\\\\n        \\rho_{01} & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\nWe fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\nlibrary(lme4)\nm1 &lt;- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 638\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4945 -0.5722 -0.0135  0.6254  2.3912 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.262    1.504        \n          x           0.796    0.892    0.55\n Residual             4.367    2.090        \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    1.726      0.967    1.78\nx              1.151      0.297    3.88\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)",
    "crumbs": [
      "W5 Exercises: Bringing it all together"
    ]
  },
  {
    "objectID": "09ex.html",
    "href": "09ex.html",
    "title": "W9 Exercises: EFA",
    "section": "",
    "text": "Data: Conduct Problems\nA researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours:\n\nBreaking curfew\nVandalism\nSkipping school\nBullying\nSpreading malicious rumours\nFighting\nLying\nUsing a weapon\nStealing\nThreatening others\n\nOur task is to use the dimension reduction techniques we learned about in the lecture to help inform how to organise the items she has developed into subscales.\nThe data can be found at https://uoepsy.github.io/data/conduct_probs_scale.csv\n\n\nQuestion 1\n\n\nRead in the dataset.\nCreate a correlation matrix for the items, and inspect the items to check their suitability for exploratory factor analysis\n\n\n\n\n\n\nHints\n\n\n\n\n\nTake a look at Reading 9# Initial Checks.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\ncpdata &lt;- read.csv(\"https://uoepsy.github.io/data/conduct_probs_scale.csv\")\n# discard the first column\ncpdata &lt;- cpdata[,-1]\n\nHere’s a correlation matrix. There’s no obvious blocks of items here, but we can see that there are some fairly high correlations, as well as some weaker ones. All are positive.\n\nlibrary(ggcorrplot)\nggcorrplot(cor(cpdata))\n\n\n\n\n\n\n\n\nThe Bartlett’s test comes out with a p-value of 0 (which isn’t possible, but it’s been rounded for some reason). This suggests that we reject the null of this test (that our correlation matrix is proportional to the identity matrix). This is good. It basically means “we have some non-zero correlations”!\n\nlibrary(psych)\ncortest.bartlett(cor(cpdata), n=450)\n\n$chisq\n[1] 2238\n\n$p.value\n[1] 0\n\n$df\n[1] 45\n\n\nThe overall sampling adequacy is 0.87, which is pretty good! (or rather, which is ‘meritorious’!). MSA for all items is &gt;.8\n\nKMO(cpdata)  \n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cpdata)\nOverall MSA =  0.87\nMSA for each item = \n   breaking_curfew          vandalism    skipping_school           bullying \n              0.84               0.88               0.92               0.82 \n spreading_rumours           fighting              lying       using_weapon \n              0.81               0.94               0.88               0.95 \n          stealing threatening_others \n              0.90               0.94 \n\n\nFinally, all the relationships here look fairly linear:\n\npairs.panels(cpdata)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nHow many dimensions should be retained?\nThis question can be answered in the same way as we did for PCA - use a scree plot, parallel analysis, and MAP test to guide you.\n\n\n\n\n\nScree\n\n\n\nSolution 2. The scree plot shows a kink at 3, which suggests retaining 2 components.\n\nscree(cpdata)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAP\n\n\n\nSolution 3. The MAP suggests retaining 2 factors. I’m just extracting the actual map values here to save having to show all the other output. We can see that the 2nd entry is the smallest:\n\nVSS(cpdata, plot = FALSE, n = ncol(cpdata))$map\n\n [1] 0.1058 0.0338 0.0576 0.1035 0.1494 0.2520 0.3974 0.4552 1.0000     NA\n\n\n\n\n\n\n\nParallel Analysis\n\n\n\nSolution 4. Parallel analysis suggests 2 factors as well:\n\nfa.parallel(cpdata, fa = \"both\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  2 \n\n\n\n\n\n\n\nMaking a decision\n\n\n\nSolution 5. Again, a quite clear picture that 2 factors is preferred:\n\n\n\n\n\n\n\n\nguides\nsuggestion\n\n\n\n\nScree\n2\n\n\nMAP\n2\n\n\nParallel Analysis\n2\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nUse the function fa() from the psych package to conduct and EFA to extract 2 factors (this is what we suggest based on the various tests above, but you might feel differently - the ideal number of factors is subjective!). Use a suitable rotation (rotate = ?) and extraction method (fm = ?).\n\n\n\n\n\n\nHints\n\n\n\n\n\nWould you expect factors to be correlated? If so, you’ll want an oblique rotation.\nSee R9#doing-an-efa.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. For example, you could choose an oblimin rotation to allow factors to correlate. Let’s use MLE as the estimator.\n\nconduct_efa &lt;- fa(cpdata, nfactors=2, rotate='oblimin', fm=\"ml\")\n\n\n\n\n\nQuestion 4\n\n\nInspect your solution. Make sure to look at and think about the loadings, the variance accounted for, and the factor correlations (if estimated).\n\n\n\n\n\n\nHints\n\n\n\n\n\nJust printing an fa object:\n\nmyfa &lt;- fa(data, ..... )\nmyfa\n\nWill give you lots and lots of information.\nYou can extract individual parts using:\n\nmyfa$loadings for the loadings\nmyfa$Vaccounted for the variance accounted for by each factor\nmyfa$Phi for the factor correlation matrix\n\nYou can find a quick guide to reading the fa output here: efa_output.pdf.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Things look pretty good here. Each item has a clear primary loading on to one of the factors, and the complexity for all items is 1 (meaning they’re clearly link to just one of the factors). The h2 column is showing that the 2 factor solution is explaining 39%+ of the variance in each item. Both factors are well determined, having a at least 3 salient loadings.\nThe 2 factors together explain 57% of the variance in the data - both factors explain a similar amount (29% for factor 1, 28% for factor 2).\nWe can also see that there is a moderate correlation between the two factors. Use of an oblique rotation was appropriate - if the correlation had been very weak, then it might not have differed much from if we used an orthogonal rotation.\n\nconduct_efa\n\n\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                     ML1   ML2   h2   u2 com\nbreaking_curfew    -0.05  0.88 0.74 0.26   1\nvandalism           0.05  0.69 0.51 0.49   1\nskipping_school    -0.01  0.67 0.44 0.56   1\nbullying            0.90  0.00 0.81 0.19   1\nspreading_rumours   0.93 -0.02 0.85 0.15   1\nfighting            0.65 -0.02 0.42 0.58   1\nlying               0.02  0.77 0.60 0.40   1\nusing_weapon        0.63  0.09 0.45 0.55   1\nstealing            0.04  0.70 0.51 0.49   1\nthreatening_others  0.62 -0.01 0.38 0.62   1\n\n                       ML1  ML2\nSS loadings           2.91 2.80\nProportion Var        0.29 0.28\nCumulative Var        0.29 0.57\nProportion Explained  0.51 0.49\nCumulative Proportion 0.51 1.00\n\n With factor correlations of \n     ML1  ML2\nML1 1.00 0.43\nML2 0.43 1.00\n\nMean item complexity =  1\n\n\n\n\n\n\nQuestion 5\n\n\nLook back to the description of the items, and suggest a name for your factors based on the patterns of loadings.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo sort the loadings, you can use\n\nprint(myfa$loadings, sort = TRUE)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. You can inspect the loadings using:\n\nprint(conduct_efa$loadings, sort=TRUE)\n\n\nLoadings:\n                   ML1    ML2   \nbullying            0.899       \nspreading_rumours   0.931       \nfighting            0.653       \nusing_weapon        0.629       \nthreatening_others  0.621       \nbreaking_curfew            0.878\nvandalism                  0.694\nskipping_school            0.671\nlying                      0.767\nstealing                   0.696\n\n                 ML1   ML2\nSS loadings    2.890 2.782\nProportion Var 0.289 0.278\nCumulative Var 0.289 0.567\n\n\nWe can see that, ordered like this, we have five items that have high loadings for one factor and another five items that have high loadings for the other.\nThe five items for factor 2 all have in common that they are non-aggressive forms of conduct problems. The five items for factor 1 are all more aggressive behaviours. We could, therefore, label our factors: ‘aggressive’ and ‘non-aggressive’ conduct problems.\n\n\n\n\nQuestion 6\n\n\nCompare three different solutions:\n\nyour current solution from the previous questions\none where you fit 1 more factor\none where you fit 1 fewer factors\n\nWhich one looks best?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’re looking here to assess:\n\nhow much variance is accounted for by each solution\ndo all factors load on 3+ items at a salient level?\n\ndo all items have at least one loading at a salient level?\nare there any “Heywood cases” (communalities or standardised loadings that are &gt;1)?\nshould we perhaps remove some of the more complex items?\nis the factor structure (items that load on to each factor) coherent, and does it make theoretical sense?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. The 1-factor model explains 37% of the variance (as opposed to the 57% explained by the 2 factor solution), and all items load fairly high on the factor. The downside here is that we’re not discerning between different types of conduct problems that we did in the 2 factor solution.\n\nconduct_1 &lt;- fa(cpdata, nfactors=1, fm=\"ml\")\nconduct_1\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 1, fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML1   h2   u2 com\nbreaking_curfew    0.42 0.18 0.82   1\nvandalism          0.42 0.18 0.82   1\nskipping_school    0.35 0.13 0.87   1\nbullying           0.89 0.79 0.21   1\nspreading_rumours  0.90 0.81 0.19   1\nfighting           0.64 0.41 0.59   1\nlying              0.43 0.18 0.82   1\nusing_weapon       0.68 0.46 0.54   1\nstealing           0.42 0.17 0.83   1\nthreatening_others 0.61 0.38 0.62   1\n\n                ML1\nSS loadings    3.69\nProportion Var 0.37\n\nMean item complexity =  1\nTest of the hypothesis that 1 factor is sufficient.\n\ndf null model =  45  with the objective function =  5.03 with Chi Square =  2238\ndf of  the model are 35  and the objective function was  1.78 \n\nThe root mean square of the residuals (RMSR) is  0.19 \nThe df corrected root mean square of the residuals is  0.22 \n\nThe harmonic n.obs is  450 with the empirical chi square  1465  with prob &lt;  1.9e-285 \nThe total n.obs was  450  with Likelihood Chi Square =  789  with prob &lt;  3.4e-143 \n\nTucker Lewis Index of factoring reliability =  0.557\nRMSEA index =  0.219  and the 90 % confidence intervals are  0.206 0.232\nBIC =  575\nFit based upon off diagonal values = 0.8\nMeasures of factor score adequacy             \n                                                   ML1\nCorrelation of (regression) scores with factors   0.96\nMultiple R square of scores with factors          0.92\nMinimum correlation of possible factor scores     0.84\n\n\nThe 3-factor model explains 60% of the variance (only 3% more than the 2-factor model). Notably, the third factor is not very clearly defined - it only has 1 salient loading (possibly 2 if we consider the 0.3 to be salient, but that item is primarily loaded on the 2nd factor).\n\nconduct_3 &lt;- fa(cpdata, nfactors=3, rotate='oblimin', fm=\"ml\")\nconduct_3\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 3, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                     ML1   ML2   ML3   h2   u2 com\nbreaking_curfew    -0.02  0.61  0.31 0.71 0.29 1.5\nvandalism           0.06  0.12  0.74 0.72 0.28 1.1\nskipping_school     0.00  0.56  0.14 0.44 0.56 1.1\nbullying            0.90  0.09 -0.10 0.82 0.18 1.0\nspreading_rumours   0.92 -0.02  0.03 0.85 0.15 1.0\nfighting            0.65 -0.13  0.14 0.43 0.57 1.2\nlying               0.02  0.85 -0.06 0.67 0.33 1.0\nusing_weapon        0.63  0.08  0.02 0.45 0.55 1.0\nstealing            0.06  0.69  0.02 0.53 0.47 1.0\nthreatening_others  0.62 -0.08  0.09 0.39 0.61 1.1\n\n                       ML1  ML2  ML3\nSS loadings           2.93 2.14 0.94\nProportion Var        0.29 0.21 0.09\nCumulative Var        0.29 0.51 0.60\nProportion Explained  0.49 0.36 0.16\nCumulative Proportion 0.49 0.84 1.00\n\n With factor correlations of \n     ML1  ML2  ML3\nML1 1.00 0.39 0.32\nML2 0.39 1.00 0.68\nML3 0.32 0.68 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  45  with the objective function =  5.03 with Chi Square =  2238\ndf of  the model are 18  and the objective function was  0.02 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  450 with the empirical chi square  3.98  with prob &lt;  1 \nThe total n.obs was  450  with Likelihood Chi Square =  10.5  with prob &lt;  0.91 \n\nTucker Lewis Index of factoring reliability =  1.01\nRMSEA index =  0  and the 90 % confidence intervals are  0 0.016\nBIC =  -99.5\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML3\nCorrelation of (regression) scores with factors   0.96 0.93 0.88\nMultiple R square of scores with factors          0.93 0.86 0.77\nMinimum correlation of possible factor scores     0.85 0.72 0.53\n\n\n\n\n\n\nQuestion 7\n\n\nWrite a brief paragraph or two that summarises your method and the results from your chosen optimal factor structure for the 10 conduct problems.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWrite about the process that led you to the number of factors. Discuss the patterns of loadings and provide definitions of the factors.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. The main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description).\nAn example summary would be:\n\nFirst, the data were checked for their suitability for factor analysis. Normality was checked using visual inspection of histograms, linearity was checked through the inspection of the linear and lowess lines for the pairwise relations of the variables, and factorability was confirmed using a KMO test, which yielded an overall KMO of \\(.87\\) with no variable KMOs \\(&lt;.50\\). An exploratory factor analysis was conducted to inform the structure of a new conduct problems test. Inspection of a scree plot alongside parallel analysis (using principal components analysis; PA-PCA) and the MAP test were used to guide the number of factors to retain. All three methods suggested retaining two factors; however, a one-factor and three-factor solution were inspected to confirm that the two-factor solution was optimal from a substantive and practical perspective, e.g., that it neither blurred important factor distinctions nor included a minor factor that would be better combined with the other in a one-factor solution. These factor analyses were conducted using maximum likelihood estimation and (for the two- and three-factor solutions) an oblimin rotation, because it was expected that the factors would correlate. Inspection of the factor loadings and correlations reinforced that the two-factor solution was optimal: both factors were well-determined, including 5 loadings \\(&gt;|0.3|\\) and the one-factor model blurred the distinction between different forms of conduct problems. The factor loadings are provided in Table 11. Based on the pattern of factor loadings, the two factors were labelled ‘aggressive conduct problems’ and ‘non-aggressive conduct problems’. These factors had a correlation of \\(r=.43\\). Overall, they accounted for 57% of the variance in the items, suggesting that a two-factor solution effectively summarised the variation in the items.\n\n\n\n\nTable 1: Factor Loadings\n\n\n\n\n\n\n\nML1\nML2\n\n\n\n\nspreading_rumours\n0.93\n\n\n\nbullying\n0.90\n\n\n\nfighting\n0.65\n\n\n\nusing_weapon\n0.63\n\n\n\nthreatening_others\n0.62\n\n\n\nvandalism\n\n0.88\n\n\nstealing\n\n0.77\n\n\nlying\n\n0.70\n\n\nskipping_school\n\n0.69\n\n\nbreaking_curfew\n\n0.67",
    "crumbs": [
      "Week 9",
      "W9 Exercises: EFA"
    ]
  },
  {
    "objectID": "09ex.html#footnotes",
    "href": "09ex.html#footnotes",
    "title": "W9 Exercises: EFA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou should provide the table of factor loadings. It is conventional to omit factor loadings \\(&lt;|0.3|\\); however, be sure to ensure that you mention this in a table note.↩︎",
    "crumbs": [
      "Week 9",
      "W9 Exercises: EFA"
    ]
  },
  {
    "objectID": "04ex.html",
    "href": "04ex.html",
    "title": "W4 Exercises: Centering",
    "section": "",
    "text": "Hangry\n\nData: hangry1.csv\nThe study is interested in evaluating whether levels of hunger are associated with levels of irritability (i.e., “the hangry hypothesis”). 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week.\nThe data are available at: https://uoepsy.github.io/data/hangry1.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRemember that what we’re interested in is “whether levels of hunger are associated with levels of irritability (i.e.,”the hangry hypothesis”)“.\nRead in the data and fit the model below. How well does it address the research question?\n\nmod1 &lt;- lmer(q_irritability ~ q_hunger + \n                (1 + q_hunger | ppt), \n                data = hangry)\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nAlways plot your data! It’s tempting to just go straight to interpreting coefficients of this model, but in order to understand what a model says we must have a theory about how the data are generated.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\nhangry &lt;- read_csv(\"https://uoepsy.github.io/data/hangry1.csv\")\n\nmod1 &lt;- lmer(q_irritability ~ q_hunger + \n                (1 + q_hunger | ppt), \n                data = hangry)\n\nThe model above will give us that same old formulaic expression of “for people on average, a 1 unit increase in q_hunger is associated with a 0.17 increase in q_irritability”.\nThe problem is that in trying to estimate what do as q_hunger increases, we’re ignoring the fact that people tend to have different average levels of q_hunger:\n\nggplot(hangry, aes(x = q_hunger, y = q_irritability, group = ppt)) +\n  geom_point() +\n  geom_line(alpha=.4) + \n  facet_wrap(~ppt)\n\n\n\n\n\n\n\n\nSo the interpretation of the fixed effect of our model above as “what happens to a persons’ irritability when they are 1 more hungry?”, we’re not accurately estimating this because our model doesn’t account for the fact that the numbers in q_hunger mean very different things for different people - for person 1 a hunger score of 60 might be “I’m really hungry”, but for person 2 (who is usually in the 80s or 90s) it could mean “I’m not very hungry at all”.\n\n\n\n\nQuestion 2\n\n\n\n\n\n\n\n\nwithin effects, between effects, and smushed effects\n\n\n\n\n\nResearch Question: “whether levels of hunger are associated with levels of irritability (i.e.,”the hangry hypothesis”)“.\nThink about the relationship between irritability and hunger. How should we interpret this research aim?\nIs it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a composite of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that this isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it a good estimation of how person \\(i\\) changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are.).\n\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll find group_by() |&gt; mutate() very useful here, as seen in Chapter 10 #group-mean-centering.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\nhangry &lt;- \n    hangry |&gt; group_by(ppt) |&gt;\n        mutate(\n            avg_hunger = mean(q_hunger),\n            hunger_gc = q_hunger - avg_hunger\n        )\nhead(hangry)\n\n# A tibble: 6 × 5\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   avg_hunger hunger_gc\n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1             42       52 N2p1        39.2     12.8 \n2             24       47 N2p1        39.2      7.8 \n3             17        8 N2p1        39.2    -31.2 \n4             26       47 N2p1        39.2      7.8 \n5             27       42 N2p1        39.2      2.80\n6             17       48 N2p2        39.6      8.4 \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people?\n\nDoes it look like when people are more hungry than normal, they are more irritable?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou might find stat_summary() useful here for plotting the between effect (see Chapter 10 #group-mean-centering)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. We might find it easier to look at a plot where each participant is represented as their mean plus an indication of their range of irritability scores:\n\nggplot(hangry,aes(x=avg_hunger,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nIt’s hard to see any clear relationship between a persons’ average hunger and their irritability scores here.\nIt is also a bit difficult to get at the relationship between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data.\n\nggplot(hangry,aes(x=hunger_gc,y=q_irritability, group=ppt)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2)\n\n\n\n\n\n\n\n\nIt looks like most of these lines are going upwards, but there’s a fair bit of variation in them.\nSo we can actually make a guess at what we’re going to see when we model. We’ll probably have a positive fixed effect of hunger_gc (i.e. A below will be positive), and there by-participant variation in these slopes will be quite large relative to the fixed effect (i.e B below will be quite large in comparison to A)\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ppt      (Intercept)      ...      ... \n          hunger_gc        ...      *B*\n Residual                  ...      ...    \n\n...\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)      ...        ...     ...\nhunger_gc        *A*        ...     ...\n...              ...        ...     ...\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation level deviations from those averages), that represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables as predictors, instead of the raw hunger scores.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWe can only put one of these variables in the random effects (1 + hunger | participant). Think about the fact that each participant has only one value for their average hungriness.\n\nIf the model fails to converge, and it’s a fairly simple model (i.e one or two random slopes), then often you can switch optimizer (see Chapter 2 #convergence-warnings-singular-fits). For instance, try adding control = lmerControl(optimizer = \"bobyqa\") to the model.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. With the defaults, this model doesn’t converge\n\nhangrywb &lt;- lmer(q_irritability ~ avg_hunger + hunger_gc + \n                (1 + hunger_gc | ppt), \n                data = hangry)\n\nChanging the optimizer helps:\n\nhangrywb &lt;- lmer(q_irritability ~ avg_hunger + hunger_gc + \n                (1 + hunger_gc | ppt), \n                data = hangry,\n                control = lmerControl(optimizer = \"bobyqa\"))\n\n\n\n\n\n\n\noptional - why change the optimizer?\n\n\n\n\n\nNote that the max|grad| convergence error of the initial model was very close to the tolerance (see Chapter 8 #non-convergence for an explanation of what this tolerance is).\nThe fact that it is close indicates that we may be quite close to a solution, so it’s worth investigating if this is simply an optimizer problem.\nOne other thing to do would be to consider all available optimizers, see which ones converge, and compare estimates across them. If the estimates are the same (or pretty close), and some of these converge, then it gives us more trust in our model. We can do this with the code below. We can see that 5 optimizers don’t give error messages, and that they all give pretty much the same estimated fixed effects. We can go further and compare random effects variances too, but we won’t do that here.\n\n# fit with all optimizers\nallopts = allFit(hangrywb)\n\nbobyqa : [OK]\nNelder_Mead : [OK]\nnlminbwrap : [OK]\nnloptwrap.NLOPT_LN_NELDERMEAD : [OK]\nnloptwrap.NLOPT_LN_BOBYQA : \n\n\n[OK]\n\n# error messages from each optimizer \n# (NULL here means no message, which is good)\nsummary(allopts)$msgs\n\n$bobyqa\nNULL\n\n$Nelder_Mead\nNULL\n\n$nlminbwrap\nNULL\n\n$nloptwrap.NLOPT_LN_NELDERMEAD\nNULL\n\n$nloptwrap.NLOPT_LN_BOBYQA\n[1] \"Model failed to converge with max|grad| = 0.0027028 (tol = 0.002, component 1)\"\n\n# fixed effect estimates for all optimizers\nsummary(allopts)$fixef\n\n                              (Intercept) avg_hunger hunger_gc\nbobyqa                               17.6   -0.00644     0.187\nNelder_Mead                          17.6   -0.00644     0.187\nnlminbwrap                           17.6   -0.00644     0.187\nnloptwrap.NLOPT_LN_NELDERMEAD        17.6   -0.00647     0.187\nnloptwrap.NLOPT_LN_BOBYQA            17.6   -0.00648     0.187\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nWrite down an interpretation of each of the fixed effects\n\n\n\n\n\nSolution\n\n\n\nSolution 5. Here they are\n\nfixef(hangrywb)\n\n(Intercept)  avg_hunger   hunger_gc \n   17.62005    -0.00644     0.18663 \n\n\n\n\n\n\n\n\n\n\nterm\nest\ninterpretation\n\n\n\n\n(Intercept)\n17.620\nestimated irritability score for someone with an average hunger of 0, and not deviating from that average (i.e. hunger_gc = 0)\n\n\navg_hunger\n-0.006\nestimated difference in irritability between two people who differ in average hunger level by 1 (e.g., a person with average hunger of 11 vs someone with average hunger level of 10), when they are at their average (hunger_gc = 0)\n\n\nhunger_gc\n0.187\nestimated change in irritability score for every 1 more hungry a person is than they normally are\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nHave a go at also writing an explanation for yourself of the random effects part of the output.\nThere’s no formulaic way to interpret these, but have a go at describing in words what they represent, and how that adds to the picture your model describes.\nDon’t worry about making it read like a report - just write yourself an explanation!\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\nVarCorr(hangrywb)\n\n Groups   Name        Std.Dev. Corr \n ppt      (Intercept) 6.992         \n          hunger_gc   0.366    -0.08\n Residual             4.772         \n\n\n\n\n\n\n\n\n\n\nterm\nest\ninterpretation\n\n\n\n\nsd__(Intercept)\n6.992\nParticipant level variability in irritability when they are at their average hunger level - i.e. when everybody is at their own average level of hunger, they vary in their irritability scores with a standard deviation of 7.\n\n\nsd__hunger_gc\n0.366\nParticipants vary quite a bit in how deviations from hunger are associated with irritability. They vary around the fixed effect of 0.19 with a standard deviation of 0.37. To think about what this means, imagine a normal distribution that is centered on 0.19 and has a standard deviation of 0.36. A fairly large portion of that distribution would fall below zero (i.e. have a negative slope). And we would also expect some slopes that are e.g., .5, .6 etc\n\n\ncor__(Intercept).hunger_gc\n-0.080\nthis estimate is basically zero, but represents the relationship between participants relative standing at the intercept and their relative standing on the slopes. So participants who are more irritable than others when at their average hunger, tend to have slightly lower slopes\n\n\nsd__Observation\n4.772\nthe residual variance doesn't really have much of an interpretation - it really just represents all the leftover stuff that the model doesn't explain. If we imagine all of the individual participant lines, this represents how spread out the individual observations are around those lines\n\n\n\n\n\n\n\n\n\n\n\n\nHangry 2\n\nQuestion 7\n\n\nA second dataset on the same variables is available at: https://uoepsy.github.io/data/hangry2.csv.\nThese data are from people who were following a five-two diet, while the original dataset were from people who were not folling any diet.\nCombine the datasets together so we can fit a model to see if the hangry effect differs between people on diets vs those who aren’t.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nSomething like bind_rows() might help here. If you’ve not seen it before, remember that you can look up the help documentation in the bottom-right panel of RStudio\nBe sure to keep an indicator of which group the data are in!!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Here are our two datasets:\n\nhangry1 &lt;- read_csv(\"https://uoepsy.github.io/data/hangry1.csv\")\nhangry2 &lt;- read_csv(\"https://uoepsy.github.io/data/hangry2.csv\")\n\nWe could simply bind them together using bind_rows()\n\nhangryfull &lt;- \n  bind_rows(\n    hangry1, \n    hangry2\n  )\n\nbut then we wouldn’t know who was from which group! So we’ll need to add a variable to each one first:\n\nhangryfull &lt;- \n  bind_rows(\n    hangry1 |&gt; mutate(diet = \"N\"), \n    hangry2 |&gt; mutate(diet = \"Y\")\n  )\nhead(hangryfull)\n\n# A tibble: 6 × 4\n  q_irritability q_hunger ppt   diet \n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1             42       52 N2p1  N    \n2             24       47 N2p1  N    \n3             17        8 N2p1  N    \n4             26       47 N2p1  N    \n5             27       42 N2p1  N    \n6             17       48 N2p2  N    \n\n\n\n\n\n\nQuestion 8\n\n\nDoes the relationship between hunger and irritability depend on whether or not people are following the five-two diet?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWhich relationship between hunger and irritability are we talking about? The between effect or the within effect? It could be both!\nWe’re going to need to create those two variables again for this combined dataset.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nhangryfull &lt;- \n    hangryfull |&gt; group_by(ppt) |&gt;\n        mutate(\n            avg_hunger = mean(q_hunger),\n            hunger_gc = q_hunger - avg_hunger\n        )\n\nhangrywbdiet &lt;- lmer(q_irritability ~ (avg_hunger + hunger_gc) * diet + \n                (1 + hunger_gc | ppt), \n                data = hangryfull,\n                control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(hangrywbdiet)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: q_irritability ~ (avg_hunger + hunger_gc) * diet + (1 + hunger_gc |  \n    ppt)\n   Data: hangryfull\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 2735\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4138 -0.5906 -0.0454  0.5426  2.3954 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n ppt      (Intercept) 48.083   6.934         \n          hunger_gc    0.145   0.381    -0.01\n Residual             23.305   4.828         \nNumber of obs: 405, groups:  ppt, 81\n\nFixed effects:\n                  Estimate Std. Error t value\n(Intercept)       17.13095    5.14648    3.33\navg_hunger         0.00386    0.10533    0.04\nhunger_gc          0.18577    0.07560    2.46\ndietY            -10.85470    6.53568   -1.66\navg_hunger:dietY   0.46590    0.13354    3.49\nhunger_gc:dietY    0.38141    0.10139    3.76\n\nCorrelation of Fixed Effects:\n            (Intr) avg_hn hngr_g dietY  avg_:Y\navg_hunger  -0.971                            \nhunger_gc   -0.002  0.000                     \ndietY       -0.787  0.765  0.002              \navg_hngr:dY  0.766 -0.789  0.000 -0.968       \nhngr_gc:dtY  0.002  0.000 -0.746 -0.002  0.000\n\n\n\n\n\n\nQuestion 9\n\n\nConstruct two plots showing the two model estimated interactions. This model is a bit of a confusing one, so plotting may help a bit with understanding what those interactions represent.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\neffects(terms, mod) |&gt; as.data.frame() |&gt; ggplot(.....)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. The xlevels bit here just gives us the little dataframe to plot with more levels at it, so that it gives us smoother lines. Try it with and without to see what I mean!\n\nlibrary(effects)\neffect(\"avg_hunger*diet\", hangrywbdiet, xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=avg_hunger, y=fit,col=diet))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=diet),alpha=.4)+\n  labs(x=\"participants' average hunger level\")\n\n\n\n\n\n\n\n\nWe saw in our original model that for the reference level of diet, the “N” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. In our full model this is the avg_hunger coefficient.\nWe also saw the interaction avg_hunger:dietY indicates that irritability is estimated to increase by 0.47 more for those in the diet than it does for those not on the diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\neffect(\"hunger_gc*diet\", hangrywbdiet, xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=hunger_gc, y=fit,col=diet))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=diet),alpha=.4)+\n  labs(x=\"increase from participants' average hunger level\")\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_gc we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re in the diet “N” group). This is the slope of the red line - the hunger_gc coefficient from our full model.\nThe interaction hunger_gc:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive which matches with the fact that the blue line is steeper in this plot.\n\n\n\n\nQuestion 10\n\n\nProvide tests of the fixed effects and write-up the results.\n\n\n\n\n\nSolution\n\n\n\nSolution 10. \n\nhangrywbdiet.p &lt;- lmerTest::lmer(q_irritability ~ (avg_hunger + hunger_gc) * diet + \n                (1 + hunger_gc | ppt), \n                data = hangryfull,\n                control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(hangrywbdiet.p)$coefficients\n\n                  Estimate Std. Error   df t value Pr(&gt;|t|)\n(Intercept)       17.13095     5.1465 77.0  3.3287 0.001341\navg_hunger         0.00386     0.1053 77.0  0.0367 0.970834\nhunger_gc          0.18577     0.0756 65.4  2.4573 0.016659\ndietY            -10.85470     6.5357 77.0 -1.6608 0.100813\navg_hunger:dietY   0.46590     0.1335 77.0  3.4888 0.000806\nhunger_gc:dietY    0.38141     0.1014 68.5  3.7617 0.000352\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on a restricted diet such as the five-two, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes of relative-hunger level were included for participants. The model was fitting with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser from the lme4. \\(P\\)-values were obtained using the Satterthwaite approximation for degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(b = 0.19,\\ SE = 0.08,\\ t(65.41) = 2.46,\\ p=0.017\\)), and that this was increased for those following the five-two diet (\\(b = 0.38,\\ SE = 0.1,\\ t(68.49) = 3.76,\\ p&lt;0.001\\)). Although for those not on a specific diet there was no evidence for an association between irritability and being generally a more hungry person (\\(p=0.971\\)), there a significant interaction was found between average hunger and being on the five-two diet (\\(b = 0.47,\\ SE = 0.13,\\ t(77) = 3.49,\\ p&lt;0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association of both between-person hunger and within-person hunger with irritability.",
    "crumbs": [
      "W4 Exercises: Centering"
    ]
  },
  {
    "objectID": "11exPINE.html",
    "href": "11exPINE.html",
    "title": "W11 Exercises",
    "section": "",
    "text": "Q1 - EFA [14 marks]\n\n\nBelow are the results of a data reduction of a set of 12 items assessing environmental conscientiousness. Participants are asked to respond to each statement on a 5-point likert scale from “Strongly Disagree” to “Strongly Agree”.\nBased on the results and the item descriptions below, provide an interpretation of the factor solution. Your description should include:\n\nComment on the numerical solution [6 marks]\nDiscussion of suitability of the selected number of factors [6 marks]\nAn attempt to define the factors [2 marks]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nitem\nwording\n\n\n\n\ny1\nI recycle regularly\n\n\ny2\nI use eco-friendly transportation\n\n\ny3\nI buy sustainable products\n\n\ny4\nI know how to reduce my carbon footprint\n\n\ny5\nProtecting resources matters to me\n\n\ny6\nI care about protecting the environment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nitem\nwording\n\n\n\n\ny7\nI feel responsible for my environmental impact\n\n\ny8\nI am worried about climate change effects\n\n\ny9\nI know about the harm of single-use plastics\n\n\ny10\nI know how deforestation affects climate change\n\n\ny11\nI know about relevant environmental policies\n\n\ny12\nWildlife destruction concerns me deeply\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12 items, 4 factors extracted. explains 29% variance\nFactors ML2 & ML1 both have \\(\\geq 3\\) salient/primary loadings\n\nsalient = \\(\\geq |0.3|\\)\n\nFactors ML3 & ML4: both have only 1 item with salient loading\n3rd factor explaining only 6%, 4th factor only 2%\nSSloadings for ML3 & ML4 are &lt;1\n3 complex items (y4, y5, y6)\nsome items (y5,y6) have no salient loadings\nFactors ML1,2,3 correlated weak-moderate\nFactor ML4 not strongly correlated with others\nprobably overextracting (too many factors)\nmain pointer = low variance expl of ML3 & ML4, plus not enough items\ncomplex items y5 & y6 are spread across ML3 & ML4 - a 3 factor solution may well make more sense\nitem y4 = one to keep an eye on\nML1 = “environmental knowledge”\nML2 = “environmental behaviours”\nML3/4 combined look like “environmental concern”\n\n\n\n\n\nQ2 - SSloadings [6 marks]\n\n\nCalculate the 6 values missing from the table below: SSloadings [2 marks] and proportion variance [2 marks] & cumulative variance [2 marks].\n\n\n       PC1   PC2\nitem1 0.90  0.00\nitem2 0.90 -0.29\nitem3 0.90  0.30\nitem4 0.70  0.70\nitem5 0.81 -0.50\n\n\n               PC1  PC2 \nSS loadings     __   __ \nProportion Var  __   __ \nCumulative Var  __   __ \n\n\n\n\n\n\nTable filled in:\n\n\n                 PC1   PC2\nSS loadings    3.576 0.914\nProportion Var 0.715 0.183\nCumulative Var 0.715 0.898\n\n\nHow?\nstart by squaring all the numbers, and sum the columns to give us SSloadings:\n\n\n        PC1    PC2\nitem1 0.810 0.0000\nitem2 0.810 0.0841\nitem3 0.810 0.0900\nitem4 0.490 0.4900\nitem5 0.656 0.2500\nSum   3.576 0.9141\n\n\ndivide SSloadings by 5 (because 5 observed variables) to get proportion variance\n\n\n  PC1   PC2 \n0.715 0.183 \n\n\nthose two numbers are then cumulatively summed for cumulative variance:\n\n\n  PC1   PC2 \n0.715 0.898 \n\n\n\n\n\n\nQ3 - MLM [10 marks]\n\n\nA company that makes “6-minute journals” is undertaking some research to showcase the effectiveness of their product in helping to alleviate unwanted feelings. They recruited 166 people signing up to one of 10 “anger management classes” in different cities, and asked them if they would like to have a free journal to help with reflection. 88 participants chose to take a journal, and 78 did not. Each participant filled out weekly assessments of anger levels for 10 weeks. Scores on the anger measure can range from 0 to 15, with changes of 3 being considered ‘clinically meaningful’.\nTo investigate if having a journal helps to reduce anger levels, the company fit a multilevel model to the data, with anger levels being modelled by week number (0 to 9, with 0 representing the first week participants filled in the anger assessment), whether the journal was used (“no”/“yes”, with “no” as the reference level).\nProvide an interpretation of each of:\n\nthe fixed effects [4 marks]\nthe random effects [4 marks]\nthe relevance of the findings, considering the context of the study design and researchers’ aims [2 marks]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nanger for someone who doesn’t journal, at start (in “week 1”, or “week 0” is fine here, give benefit of doubt) is 10.22\nno significant change over the study period for those who don’t journal\npeople to take the journal have lower anger at the start by -0.3\njournal group decreases in anger over the study by an additional -0.24 compared to no-journal group\nboth participants and classes vary in starting anger levels and in change in anger over study period\nparticipants vary (both intercept and slopes of change) much more than classes\nhigh level of ppt variability relative to fixed slope\nppts who start more angry decrease less (positive correlation intercepts and slopes)\ntake journal = significant reduction in anger\neffect is small - over 10 weeks they only go down by -1.622\ndifference in two groups at outset suggests two groups are not comparable\nself-selecting journal - maybe all we’re doing is splitting up people who do/don’t want to change\n\n\n\n\n\nQ4 - hierarchical data structures [3 marks]\n\n\nProvide example levels for each of the three types of study: Cross-Sectional, Repeated Measures, Longitudinal [3 marks]\n\n\n\nlevel\ncross-sectional\nrepeated measures\nlongitudinal\n\n\n\n\n2\n…\n…\n…\n\n\n1\n…\n…\n…\n\n\n\n\n\n\n\nanything that makes sense here, obvious ones are:\n\n\n\nlevel\ncross-sec\nrpt measures\nlongitudinal\n\n\n\n\n2\ndepartment\npeople\npeople\n\n\n1\npeople\nexperimental items\ntimepoints"
  },
  {
    "objectID": "10ex.html",
    "href": "10ex.html",
    "title": "W10 Exercises: EFA, replicability, reliability",
    "section": "",
    "text": "Data: petmoods2.csv\nA pet food company has conducted a questionnaire on the internet (\\(n = 620\\)) to examine whether owning a pet influences low mood. They asked 16 questions on a Likert scale (1-7, detailed below) followed by a simple Yes/No question concerning whether the respondent owned a pet.\nThe researchers don’t really know much about the theory of mood disorders, but they looked at some other questionnaires and copied the questions they saw often appearing. They think that they are likely picking up on multiple different types of “low mood”, so they want do an EFA to examine this. They then want to look at group differences (pet owners vs not pet owners) in the dimensions of low mood.\nThe data are available at https://uoepsy.github.io/data/petmoods2.csv\n\n\n\n\n\nQuestionNumber\nOver the last 2 weeks, how much have you had/have you been...\n\n\n\n\nitem1\nLittle interest or pleasure in doing things?\n\n\nitem2\nFeeling down, depressed, or hopeless?\n\n\nitem3\nTrouble falling or staying asleep, or sleeping too much?\n\n\nitem4\nFeeling tired or having little energy?\n\n\nitem5\nPoor appetite or overeating?\n\n\nitem6\nFeeling bad about yourself - or that you are a failure or have let yourself or your family down?\n\n\nitem7\nTrouble concentrating on things, such as reading the newspaper or watching television?\n\n\nitem8\nMoving or speaking so slowly that other people could have noticed? Or the opposite - being so fidgety or restless that you have been moving around a lot more than usual?\n\n\nitem9\nA lack of motivation to do anything at all?\n\n\nitem10\nFeeling nervous, anxious or on edge?\n\n\nitem11\nNot being able to stop or control worrying?\n\n\nitem12\nWorrying too much about different things?\n\n\nitem13\nTrouble relaxing?\n\n\nitem14\nBeing so restless that it is hard to sit still?\n\n\nitem15\nBecoming easily annoyed or irritable?\n\n\nitem16\nFeeling afraid as if something awful might happen?\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data, check the suitability for conducting factor analysis, and then examine what underlying dimensions best explain the observed relationships between the 16 mood-related questions in the survey.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nIt will be very handy to rename the columns to something easier to work with (see R7-Questionnaire Data Wrangling #variable-names).\n\nMake sure to fit the EFA on only the mood questions\n\n\n\n\n\n\n\n\n\n1 - variable names\n\n\n\nSolution 1. \n\nlibrary(tidyverse)\nlibrary(psych)\npetdata &lt;- read_csv(\"https://uoepsy.github.io/data/petmoods2.csv\")\n\n\nnames(petdata)\n\n [1] \"ppt_id\"                                                                                                                                                               \n [2] \"do_you_own_a_pet\"                                                                                                                                                     \n [3] \"little_interest_or_pleasure_in_doing_things\"                                                                                                                          \n [4] \"feeling_down_depressed_or_hopeless\"                                                                                                                                   \n [5] \"trouble_falling_or_staying_asleep_or_sleeping_too_much\"                                                                                                               \n [6] \"feeling_tired_or_having_little_energy\"                                                                                                                                \n [7] \"poor_appetite_or_overeating\"                                                                                                                                          \n [8] \"feeling_bad_about_yourself_or_that_you_are_a_failure_or_have_let_yourself_or_your_family_down\"                                                                        \n [9] \"trouble_concentrating_on_things_such_as_reading_the_newspaper_or_watching_television\"                                                                                 \n[10] \"moving_or_speaking_so_slowly_that_other_people_could_have_noticed_or_the_opposite_being_so_fidgety_or_restless_that_you_have_been_moving_around_a_lot_more_than_usual\"\n[11] \"a_lack_of_motivation_to_do_anything_at_all\"                                                                                                                           \n[12] \"feeling_nervous_anxious_or_on_edge\"                                                                                                                                   \n[13] \"not_being_able_to_stop_or_control_worrying\"                                                                                                                           \n[14] \"worrying_too_much_about_different_things\"                                                                                                                             \n[15] \"trouble_relaxing\"                                                                                                                                                     \n[16] \"being_so_restless_that_it_is_hard_to_sit_still\"                                                                                                                       \n[17] \"becoming_easily_annoyed_or_irritable\"                                                                                                                                 \n[18] \"feeling_afraid_as_if_something_awful_might_happen\"                                                                                                                    \n\n\nI’m going to subset out the mood data for now, to make it easier to work with:\n\nmoodq &lt;- petdata %&gt;% \n    select(-ppt_id, -do_you_own_a_pet)\n\nAnd then rename the variables:\n\nnames(moodq) &lt;- paste0(\"item\", 1:ncol(moodq))\nhead(moodq)\n\n# A tibble: 6 × 16\n  item1 item2 item3 item4 item5 item6 item7 item8 item9 item10 item11 item12\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     6     4     4     5     5     5     4     3     2      4      6      4\n2     3     5     5     6     3     5     3     5     3      5      4      3\n3     6     7     7     6     5     4     7     6     6      5      3      6\n4     5     4     4     4     4     2     5     2     5      3      6      2\n5     6     6     5     5     5     4     7     2     2      6      6      3\n6     4     5     5     5     3     4     5     3     3      5      4      4\n# ℹ 4 more variables: item13 &lt;dbl&gt;, item14 &lt;dbl&gt;, item15 &lt;dbl&gt;, item16 &lt;dbl&gt;\n\n\n\n\n\n\n\n2 - suitability for EFA\n\n\n\nSolution 2. \n\npheatmap::pheatmap(cor(moodq))\n\n\n\n\n\n\n\ncor(moodq)[lower.tri(cor(moodq))] |&gt; hist()\n\n\n\n\n\n\n\ncut(abs(cor(moodq)[lower.tri(cor(moodq))]),c(0,.2,.5,.8,1)) |&gt;\n  table() |&gt;\n  barplot()\n\n\n\n\n\n\n\nKMO(moodq)\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = moodq)\nOverall MSA =  0.87\nMSA for each item = \n item1  item2  item3  item4  item5  item6  item7  item8  item9 item10 item11 \n  0.93   0.80   0.88   0.91   0.92   0.92   0.88   0.94   0.91   0.80   0.90 \nitem12 item13 item14 item15 item16 \n  0.91   0.86   0.93   0.84   0.87 \n\n\n\n\n\n\n\n3 - how many factors?\n\n\n\nSolution 3. \n\nscree(moodq)\n\n\n\n\n\n\n\nVSS(moodq, plot=FALSE)$map\n\n[1] 0.0484 0.0123 0.0189 0.0278 0.0385 0.0520 0.0741 0.1044\n\nfa.parallel(moodq)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  2 \n\n\neverything points to 2!\n\n\n\n\n\n4 - doing EFA\n\n\n\nSolution 4. We would expect any different dimensions of low mood to be correlated i think:\n\nmood.fa1 &lt;- fa(moodq, nfactors=1)\nmood.fa2 &lt;- fa(moodq, nfactors=2, rotate=\"oblimin\",fm=\"ml\")\nmood.fa3 &lt;- fa(moodq, nfactors=3, rotate=\"oblimin\",fm=\"ml\")\n\nhere i’ve pulled out the variance explained by each solution:\n\nmood.fa1$Vaccounted[2,1]\n\n[1] 0.239\n\nmood.fa2$Vaccounted[3,2]\n\n[1] 0.385\n\nmood.fa3$Vaccounted[3,3]\n\n[1] 0.401\n\n\n3 factor solution only explains 2% more than the 2 factor solution.\nthe 3rd factor has no items with it as the primary loading. Pretty clear sign of over-extraction here.\n\nmood.fa3$loadings\n\n\nLoadings:\n       ML1    ML2    ML3   \nitem1   0.392              \nitem2   0.956              \nitem3   0.840              \nitem4   0.712              \nitem5   0.356         0.122\nitem6   0.446         0.182\nitem7   0.653        -0.254\nitem8   0.376  0.256       \nitem9   0.398              \nitem10         0.908       \nitem11         0.390  0.246\nitem12         0.323       \nitem13         0.785       \nitem14  0.217  0.378       \nitem15         0.826       \nitem16         0.347 -0.218\n\n                 ML1   ML2   ML3\nSS loadings    3.385 2.711 0.252\nProportion Var 0.212 0.169 0.016\nCumulative Var 0.212 0.381 0.397\n\n\nThe 1 factor solution explains quite a bit less variance than the 2 factor solution (24% vs 39%).\nWe have a lot more items with no salient loading in the 1-factor solution, whereas the 2-factor solution every item has a loading \\(\\geq |.3|\\)\n\n\n\nprint(mood.fa1$loadings, cutoff=.3)\n\n\nLoadings:\n       MR1  \nitem1  0.334\nitem2  0.822\nitem3  0.760\nitem4  0.639\nitem5  0.325\nitem6  0.408\nitem7  0.569\nitem8  0.522\nitem9  0.345\nitem10 0.497\nitem11      \nitem12      \nitem13 0.505\nitem14 0.442\nitem15 0.456\nitem16      \n\n                 MR1\nSS loadings    3.825\nProportion Var 0.239\n\n\n\n\nprint(mood.fa2$loadings, cutoff=.3)\n\n\nLoadings:\n       ML1    ML2   \nitem1   0.398       \nitem2   0.963       \nitem3   0.827       \nitem4   0.706       \nitem5   0.340       \nitem6   0.422       \nitem7   0.680       \nitem8   0.382       \nitem9   0.388       \nitem10         0.902\nitem11         0.411\nitem12         0.323\nitem13         0.784\nitem14         0.382\nitem15         0.833\nitem16         0.324\n\n                 ML1  ML2\nSS loadings    3.370 2.71\nProportion Var 0.211 0.17\nCumulative Var 0.211 0.38\n\n\n\n\n\n\n\n\n\n5 - final solution and interpretation\n\n\n\nSolution 5. \n\nmood.fa2\n\nFactor Analysis using method =  ml\nCall: fa(r = moodq, nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n         ML1   ML2   h2    u2 com\nitem1   0.40 -0.05 0.15 0.849 1.0\nitem2   0.96 -0.02 0.92 0.082 1.0\nitem3   0.83  0.02 0.69 0.307 1.0\nitem4   0.71  0.01 0.50 0.499 1.0\nitem5   0.34  0.02 0.12 0.881 1.0\nitem6   0.42  0.04 0.19 0.812 1.0\nitem7   0.68 -0.03 0.45 0.547 1.0\nitem8   0.38  0.25 0.25 0.746 1.7\nitem9   0.39 -0.02 0.15 0.853 1.0\nitem10 -0.02  0.90 0.81 0.194 1.0\nitem11 -0.03  0.41 0.16 0.836 1.0\nitem12  0.05  0.32 0.11 0.886 1.0\nitem13  0.05  0.78 0.63 0.366 1.0\nitem14  0.21  0.38 0.23 0.772 1.6\nitem15 -0.04  0.83 0.68 0.320 1.0\nitem16 -0.01  0.32 0.10 0.896 1.0\n\n                       ML1  ML2\nSS loadings           3.41 2.75\nProportion Var        0.21 0.17\nCumulative Var        0.21 0.38\nProportion Explained  0.55 0.45\nCumulative Proportion 0.55 1.00\n\n With factor correlations of \n     ML1  ML2\nML1 1.00 0.24\nML2 0.24 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 2 factors are sufficient.\n\ndf null model =  120  with the objective function =  5.47 with Chi Square =  3352\ndf of  the model are 89  and the objective function was  0.16 \n\nThe root mean square of the residuals (RMSR) is  0.03 \nThe df corrected root mean square of the residuals is  0.03 \n\nThe harmonic n.obs is  620 with the empirical chi square  101  with prob &lt;  0.19 \nThe total n.obs was  620  with Likelihood Chi Square =  101  with prob &lt;  0.19 \n\nTucker Lewis Index of factoring reliability =  0.995\nRMSEA index =  0.014  and the 90 % confidence intervals are  0 0.027\nBIC =  -472\nFit based upon off diagonal values = 0.99\nMeasures of factor score adequacy             \n                                                   ML1  ML2\nCorrelation of (regression) scores with factors   0.97 0.95\nMultiple R square of scores with factors          0.94 0.90\nMinimum correlation of possible factor scores     0.89 0.80\n\n\n\nBoth factors load on 3+ items at a salient level (\\(\\geq |.3|\\)).\nThere are no Heywood cases.\nFactors positively correlated 0.24\nItems 8 and 14 are a bit more complex.\n\nThe solution explains 38% of the variance.\n\nHere are the wordings of items for factor 1:\n\nnames(petdata)[3:11]\n\n[1] \"little_interest_or_pleasure_in_doing_things\"                                                                                                                          \n[2] \"feeling_down_depressed_or_hopeless\"                                                                                                                                   \n[3] \"trouble_falling_or_staying_asleep_or_sleeping_too_much\"                                                                                                               \n[4] \"feeling_tired_or_having_little_energy\"                                                                                                                                \n[5] \"poor_appetite_or_overeating\"                                                                                                                                          \n[6] \"feeling_bad_about_yourself_or_that_you_are_a_failure_or_have_let_yourself_or_your_family_down\"                                                                        \n[7] \"trouble_concentrating_on_things_such_as_reading_the_newspaper_or_watching_television\"                                                                                 \n[8] \"moving_or_speaking_so_slowly_that_other_people_could_have_noticed_or_the_opposite_being_so_fidgety_or_restless_that_you_have_been_moving_around_a_lot_more_than_usual\"\n[9] \"a_lack_of_motivation_to_do_anything_at_all\"                                                                                                                           \n\n\nand for factor 2:\n\nnames(petdata)[12:18]\n\n[1] \"feeling_nervous_anxious_or_on_edge\"               \n[2] \"not_being_able_to_stop_or_control_worrying\"       \n[3] \"worrying_too_much_about_different_things\"         \n[4] \"trouble_relaxing\"                                 \n[5] \"being_so_restless_that_it_is_hard_to_sit_still\"   \n[6] \"becoming_easily_annoyed_or_irritable\"             \n[7] \"feeling_afraid_as_if_something_awful_might_happen\"\n\n\nit kind of makes sense that these two are have higher cross-loadings - they seem to be tapping into something about restlessness:\n\nnames(petdata)[c(10,16)]\n\n[1] \"moving_or_speaking_so_slowly_that_other_people_could_have_noticed_or_the_opposite_being_so_fidgety_or_restless_that_you_have_been_moving_around_a_lot_more_than_usual\"\n[2] \"being_so_restless_that_it_is_hard_to_sit_still\"                                                                                                                       \n\n\n\n\n\n\n\n\nQuestion 2\n\n\nSplitting the dataset in two, calculate congruence coefficients for your factor solution.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe most reliable way to split a dataset is actually to create a sample of rows.\nFor example, if I had 100 rows, then I can split it into two groups of 50 using:\n\nidx &lt;- sample(1:100, 50) # sample 50 rows\nsplit1 &lt;- data[idx, ] # choose those rows\nsplit2 &lt;- data[-idx, ] # exclude those rows\n\nTo calculate congruence coefficients, fit the same factor solution to both datasets and then use fa.congruence()\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. We have 620, so:\n\nidx &lt;- sample(1:620, 310)\ndf1 &lt;- moodq[idx,]\ndf2 &lt;- moodq[-idx,]\n\nf1 &lt;- fa(df1, nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\nf2 &lt;- fa(df2, nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\nfactor.congruence(f1,f2)\n\n     ML1  ML2\nML1 0.99 0.06\nML2 0.04 0.99\n\n\n\n\n\n\nQuestion 3\n\n\nIdeally, we would split a dataset in two right at the start, develop our model on the “exploratory” half, and not touch the second half of the data until we want to assess congruence.\nIf we had unlimited time and resources, we would just collect another, completely independent, sample.\nSo let’s pretend that’s exactly what we’ve done!\nYou can find a 2nd dataset at https://uoepsy.github.io/data/petmoods_conf.csv that contains the same questions.\nCompute congruence coefficients for your factor solution across the two dataset (the first one with n=620 and this one with n=203).\n\n\n\n\n\nSolution\n\n\n\nSolution 7. The congruence here is lower, especially for the second factor.\nNote that, for instance, item11 is no longer loading onto this factor.\n\ntestdat &lt;- read_csv(\"https://uoepsy.github.io/data/petmoods_conf.csv\")\nnames(testdat)[1:16]&lt;-paste0(\"item\",1:16)\n\ntestfa &lt;- fa(testdat[,-17], nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\n\nfactor.congruence(mood.fa2, testfa)\n\n     ML1  ML2\nML1 0.96 0.11\nML2 0.04 0.85\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nCalculate two measures of reliability for each factor - \\(alpha\\) and \\(omega\\). How do they differ?\n(If you’re thinking “which one should I use?” then there’s not really a right answer - they rely on assuming different measurement models. If you’re going to use mean/sum scores, then reporting reliabilty as \\(\\alpha\\) will make more sense)\n\n\n\n\n\n\nHints\n\n\n\n\n\nMake sure to do this separately for each factor, because both \\(\\alpha\\) and \\(\\omega_{total}\\) assume unidimensionality\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nalpha(moodq[,1:9])$total\n\n raw_alpha std.alpha G6(smc) average_r  S/N    ase mean    sd median_r\n     0.803      0.81   0.817     0.321 4.25 0.0117 4.09 0.703    0.293\n\nalpha(moodq[,10:16])$total\n\n raw_alpha std.alpha G6(smc) average_r  S/N    ase mean    sd median_r\n     0.761     0.764   0.769     0.316 3.23 0.0144 4.05 0.735    0.294\n\nlibrary(MBESS)\nci.reliability(moodq[,1:9])$est\n\n[1] 0.816\n\nci.reliability(moodq[,10:16])$est\n\n[1] 0.785\n\n\ndifferences in reliability here are quite small!",
    "crumbs": [
      "Week 10",
      "W10 Exercises: EFA, replicability, reliability"
    ]
  },
  {
    "objectID": "10ex.html#replicability",
    "href": "10ex.html#replicability",
    "title": "W10 Exercises: EFA, replicability, reliability",
    "section": "",
    "text": "Question 2\n\n\nSplitting the dataset in two, calculate congruence coefficients for your factor solution.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe most reliable way to split a dataset is actually to create a sample of rows.\nFor example, if I had 100 rows, then I can split it into two groups of 50 using:\n\nidx &lt;- sample(1:100, 50) # sample 50 rows\nsplit1 &lt;- data[idx, ] # choose those rows\nsplit2 &lt;- data[-idx, ] # exclude those rows\n\nTo calculate congruence coefficients, fit the same factor solution to both datasets and then use fa.congruence()\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. We have 620, so:\n\nidx &lt;- sample(1:620, 310)\ndf1 &lt;- moodq[idx,]\ndf2 &lt;- moodq[-idx,]\n\nf1 &lt;- fa(df1, nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\nf2 &lt;- fa(df2, nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\nfactor.congruence(f1,f2)\n\n     ML1  ML2\nML1 0.99 0.06\nML2 0.04 0.99\n\n\n\n\n\n\nQuestion 3\n\n\nIdeally, we would split a dataset in two right at the start, develop our model on the “exploratory” half, and not touch the second half of the data until we want to assess congruence.\nIf we had unlimited time and resources, we would just collect another, completely independent, sample.\nSo let’s pretend that’s exactly what we’ve done!\nYou can find a 2nd dataset at https://uoepsy.github.io/data/petmoods_conf.csv that contains the same questions.\nCompute congruence coefficients for your factor solution across the two dataset (the first one with n=620 and this one with n=203).\n\n\n\n\n\nSolution\n\n\n\nSolution 7. The congruence here is lower, especially for the second factor.\nNote that, for instance, item11 is no longer loading onto this factor.\n\ntestdat &lt;- read_csv(\"https://uoepsy.github.io/data/petmoods_conf.csv\")\nnames(testdat)[1:16]&lt;-paste0(\"item\",1:16)\n\ntestfa &lt;- fa(testdat[,-17], nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\n\nfactor.congruence(mood.fa2, testfa)\n\n     ML1  ML2\nML1 0.96 0.11\nML2 0.04 0.85",
    "crumbs": [
      "Week 10",
      "W10 Exercises: EFA, replicability, reliability"
    ]
  },
  {
    "objectID": "10ex.html#reliability",
    "href": "10ex.html#reliability",
    "title": "W10 Exercises: EFA, replicability, reliability",
    "section": "",
    "text": "Question 4\n\n\nCalculate two measures of reliability for each factor - \\(alpha\\) and \\(omega\\). How do they differ?\n(If you’re thinking “which one should I use?” then there’s not really a right answer - they rely on assuming different measurement models. If you’re going to use mean/sum scores, then reporting reliabilty as \\(\\alpha\\) will make more sense)\n\n\n\n\n\n\nHints\n\n\n\n\n\nMake sure to do this separately for each factor, because both \\(\\alpha\\) and \\(\\omega_{total}\\) assume unidimensionality\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nalpha(moodq[,1:9])$total\n\n raw_alpha std.alpha G6(smc) average_r  S/N    ase mean    sd median_r\n     0.803      0.81   0.817     0.321 4.25 0.0117 4.09 0.703    0.293\n\nalpha(moodq[,10:16])$total\n\n raw_alpha std.alpha G6(smc) average_r  S/N    ase mean    sd median_r\n     0.761     0.764   0.769     0.316 3.23 0.0144 4.05 0.735    0.294\n\nlibrary(MBESS)\nci.reliability(moodq[,1:9])$est\n\n[1] 0.816\n\nci.reliability(moodq[,10:16])$est\n\n[1] 0.785\n\n\ndifferences in reliability here are quite small!",
    "crumbs": [
      "Week 10",
      "W10 Exercises: EFA, replicability, reliability"
    ]
  },
  {
    "objectID": "07ex.html",
    "href": "07ex.html",
    "title": "W7 Exercises: Questionnaire Data & Scale Scores",
    "section": "",
    "text": "Dataset: boxbreathe.csv\nResearchers are interested in different methods for reducing stress. They recruit 522 participants. All participants first filled out a 6-question measure of stress that is aimed to capture feelings of immediate stress and panic. All questions were scored on a 5-point likert scale from “Strongly Disagree” (1) to “Strongly Agree” (5). To obtain an overall measure of stress, participants’ scores on the 6 questions are added together.\nAfter completing the initial stress measure, participants then completed one of three 5 minute tasks. One third of participants sat in silence for 5 minutes, one third played a picture-matching game on their phone for 5 minutes, and the remaining third completed 5 minutes of “box breathing” (inhale for 6, hold for 4, exhale for 6, hold for 4). After the 5 minutes, all participants filled out the same 6-item measure of stress.\nResearchers would like to know whether the different tasks are associated with differences in reduction in stress.\nDataset: https://uoepsy.github.io/data/boxbreathe.csv\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nt1_q1\n(Time1) I feel a bit on edge right now.\n\n\nt1_q2\n(Time1) I find it hard to focus because of how I'm feeling.\n\n\nt1_q3\n(Time1) I feel like things are getting a little out of control.\n\n\nt1_q4\n(Time1) I feel calm and steady in this moment.\n\n\nt1_q5\n(Time1) I feel capable of managing the situation right now.\n\n\nt1_q6\n(Time1) I feel somewhat restless or unsettled at the moment.\n\n\ntask\nTask completed (nothing / game / boxbreathing)\n\n\nt2_q1\n(Time2) I feel a bit on edge right now.\n\n\nt2_q2\n(Time2) I find it hard to focus because of how I'm feeling.\n\n\nt2_q3\n(Time2) I feel like things are getting a little out of control.\n\n\nt2_q4\n(Time2) I feel calm and steady in this moment.\n\n\nt2_q5\n(Time2) I feel capable of managing the situation right now.\n\n\nt2_q6\n(Time2) I feel somewhat restless or unsettled at the moment.\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and have a look at it.\n\nWhat does each row represent?\n\nWhat measurement(s) show us a person’s stress?\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. Here’s the data:\n\nbbdat &lt;- read_csv(\"https://uoepsy.github.io/data/boxbreathe.csv\")\nhead(bbdat)\n\n# A tibble: 6 × 13\n  t1_q1  t1_q2 t1_q3 t1_q4 t1_q5 t1_q6 task  t2_q1 t2_q2 t2_q3 t2_q4 t2_q5 t2_q6\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 Disag… Neit… Disa… Neit… Neit… Neit… noth… Stro… Neit… Disa… Neit… Neit… Neit…\n2 Neith… Agree Agree Disa… Stro… Stro… noth… Agree Agree Agree Disa… Stro… Stro…\n3 Neith… Agree Agree Neit… Disa… Agree noth… Agree Agree Agree Neit… Neit… Agree\n4 Neith… Disa… Neit… Neit… Neit… Disa… noth… Agree Neit… Neit… Neit… Neit… Neit…\n5 Neith… Neit… Neit… Disa… Neit… Neit… noth… Neit… Neit… Neit… Neit… Neit… Agree\n6 Stron… Neit… Neit… Disa… Neit… Neit… noth… Stro… Neit… Neit… Neit… Neit… Neit…\n\n\nEach row is a participant, and we have their stress measured at two time points. We can see that for each person there are 6 columns all measuring the construct of “stress” at each time point.\nAnd for each of those columns, there’s a whole load of words in there!\n\n\n\n\nQuestion 2\n\n\nFirst things first, our questionnaire software has given us the responses all in the descriptors used for each point of the likert scale, which is a bit annoying.\nConvert them all to numbers, which we can then work with.\n\n\n\nWhat we have\nWhat we want\n\n\n\n\nStrongly Agree\n5\n\n\nAgree\n4\n\n\nAgree\n4\n\n\nStrongly Disagree\n1\n\n\nNeither Disagree nor Agree\n3\n\n\nAgree\n4\n\n\nDisagree\n2\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee R7#variable-recoding.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. We want to turn all of the variables from t1_q1 to t1_q6 and from t2_q1 to t2_q6, into numbers.\nTo do it with one variable:\n\nbbdat |&gt; mutate(\n  t1_q1 = case_match(t1_q1,\n                     \"Strongly Disagree\" ~ 1,\n                     \"Disagree\" ~ 2,\n                     \"Neither Disagree nor Agree\" ~ 3,\n                     \"Agree\" ~ 4,\n                     \"Strongly Agree\" ~ 5\n  )\n)\n\nAnd we can do it to all at once with across().\nNote we have to specify two sets of columns because there’s a column in the middle (the task column) that we don’t want to do anything to.\n\nbbdat &lt;- bbdat |&gt; mutate(\n  across(c(t1_q1:t1_q6, t2_q1:t2_q6),\n         ~case_match(.,\n                     \"Strongly Disagree\" ~ 1,\n                     \"Disagree\" ~ 2,\n                     \"Neither Disagree nor Agree\" ~ 3,\n                     \"Agree\" ~ 4,\n                     \"Strongly Agree\" ~ 5\n         ))\n  )\n\nhead(bbdat)\n\n# A tibble: 6 × 13\n  t1_q1 t1_q2 t1_q3 t1_q4 t1_q5 t1_q6 task   t2_q1 t2_q2 t2_q3 t2_q4 t2_q5 t2_q6\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     2     3     2     3     3     3 nothi…     1     3     2     3     3     3\n2     3     4     4     2     1     5 nothi…     4     4     4     2     1     5\n3     3     4     4     3     2     4 nothi…     4     4     4     3     3     4\n4     3     2     3     3     3     2 nothi…     4     3     3     3     3     3\n5     3     3     3     2     3     3 nothi…     3     3     3     3     3     4\n6     5     3     3     2     3     3 nothi…     5     3     3     3     3     3\n\n\n\n\n\n\nQuestion 3\n\n\nJust looking at the data at time 1, create a correlation matrix of the various items that measure stress.\nWhat do you notice? Does it make sense given the wording of the questions?\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\ncor(bbdat[,1:6])\n\n       t1_q1  t1_q2  t1_q3  t1_q4  t1_q5  t1_q6\nt1_q1  1.000  0.216  0.427 -0.341 -0.369  0.459\nt1_q2  0.216  1.000  0.757 -0.709 -0.699  0.723\nt1_q3  0.427  0.757  1.000 -0.564 -0.735  0.718\nt1_q4 -0.341 -0.709 -0.564  1.000  0.715 -0.568\nt1_q5 -0.369 -0.699 -0.735  0.715  1.000 -0.807\nt1_q6  0.459  0.723  0.718 -0.568 -0.807  1.000\n\n\nCorrelations are all positive except for those with Q4 and Q5. Q4 and Q5 are positively related, but they are negatively related to the other questions.\nThis makes sense given the way the questions are worded - if people are feeling stressed, they will be more likely to disagree to Q4 and Q5, but agree with the others:\n\nqitems\n\n[1] \"I feel a bit on edge right now.\"                        \n[2] \"I find it hard to focus because of how I'm feeling.\"    \n[3] \"I feel like things are getting a little out of control.\"\n[4] \"I feel calm and steady in this moment.\"                 \n[5] \"I feel capable of managing the situation right now.\"    \n[6] \"I feel somewhat restless or unsettled at the moment.\"   \n\n\n\n\n\n\nQuestion 4\n\n\nReverse score questions 4 and 5.\nWe’ll need to do this for both the data at time 1 and at time 2.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nSee R7#reverse-coding\nBe careful!! if you have some code that reverse scores a question, and you run it twice, you will essentially reverse-reverse score the question, and it goes back to the original ordering!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. There’s only 4, so let’s do this individually for each question:\n\nbbdat &lt;- bbdat |&gt; \n  mutate(\n    t1_q4 = 6 - t1_q4,\n    t1_q5 = 6 - t1_q5,\n    t2_q4 = 6 - t2_q4,\n    t2_q5 = 6 - t2_q5\n)\nhead(bbdat)\n\n# A tibble: 6 × 13\n  t1_q1 t1_q2 t1_q3 t1_q4 t1_q5 t1_q6 task   t2_q1 t2_q2 t2_q3 t2_q4 t2_q5 t2_q6\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     2     3     2     3     3     3 nothi…     1     3     2     3     3     3\n2     3     4     4     4     5     5 nothi…     4     4     4     4     5     5\n3     3     4     4     3     4     4 nothi…     4     4     4     3     3     4\n4     3     2     3     3     3     2 nothi…     4     3     3     3     3     3\n5     3     3     3     4     3     3 nothi…     3     3     3     3     3     4\n6     5     3     3     4     3     3 nothi…     5     3     3     3     3     3\n\n\n\n\n\n\nQuestion 5\n\n\nTake a look at the correlation of the time 1 stress measures again.\nWhat has changed?\n\n\n\n\n\nSolution\n\n\n\nSolution 5. The negative correlations are now positive!\n\ncor(bbdat[,1:6])\n\n      t1_q1 t1_q2 t1_q3 t1_q4 t1_q5 t1_q6\nt1_q1 1.000 0.216 0.427 0.341 0.369 0.459\nt1_q2 0.216 1.000 0.757 0.709 0.699 0.723\nt1_q3 0.427 0.757 1.000 0.564 0.735 0.718\nt1_q4 0.341 0.709 0.564 1.000 0.715 0.568\nt1_q5 0.369 0.699 0.735 0.715 1.000 0.807\nt1_q6 0.459 0.723 0.718 0.568 0.807 1.000\n\n\n\n\n\n\nQuestion 6\n\n\nWe’re finally getting somewhere! Let’s create a score for “stress” at time 1, and a score for “stress” at time 2.\nThe description of the questionnaire says that we should take the sum of the scores on each question, to get an overall measure of stress.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe function rowSums() should help us here! See an example in R7#row-scoring\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\nbbdat$t1_stress &lt;- rowSums(bbdat[,1:6])\nbbdat$t2_stress &lt;- rowSums(bbdat[,8:13])\n\n\n\n\n\nQuestion 7\n\n\nMake a new column that represents the change in stress for each person between the two timepoints.\n\n\n\n\n\nSolution\n\n\n\nSolution 7. \n\nbbdat$stress_change &lt;- bbdat$t2_stress - bbdat$t1_stress\n\n\n\n\n\nQuestion 8\n\n\nProvide some descriptive statistics for the stress scores at time 1 and at time 2, and of the ‘change in stress’ measure.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe describe() function from the psych package is often pretty useful for this kind of thing\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nlibrary(psych)\nbbdat |&gt; \n  select(t1_stress, t2_stress, stress_change) |&gt;\n  describe()\n\n              vars   n  mean   sd median trimmed  mad min max range  skew\nt1_stress        1 522 17.84 4.61     18   17.88 4.45   7  30    23 -0.06\nt2_stress        2 522 17.77 4.55     18   17.78 4.45   6  30    24 -0.03\nstress_change    3 522 -0.07 1.02      0   -0.04 1.48  -3   3     6 -0.11\n              kurtosis   se\nt1_stress        -0.32 0.20\nt2_stress        -0.40 0.20\nstress_change    -0.08 0.04\n\n\n\n\n\n\nQuestion 9\n\n\nPlot the stress-change for each group of participants.\nFit a linear model to investigate whether the different techniques (the timer game and the box-breathing) are associated with differences in stress change.\n\n\n\n\n\nSolution\n\n\n\nSolution 9. It makes more sense to think of “nothing” as the reference level, so let’s make that happen:\n\nbbdat &lt;- bbdat |&gt;\n  mutate(\n    task = factor(task, levels=c(\"nothing\",\"game\",\"boxbreathing\"))\n  )\n\nmod1 &lt;- lm(stress_change ~ task, data = bbdat) \n\nsummary(mod1)\n\n\nCall:\nlm(formula = stress_change ~ task, data = bbdat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6954 -0.6954  0.0632  0.8333  2.8333 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         0.167      0.076    2.19    0.029 *  \ntaskgame           -0.230      0.107   -2.14    0.033 *  \ntaskboxbreathing   -0.471      0.107   -4.39  1.4e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1 on 519 degrees of freedom\nMultiple R-squared:  0.0357,    Adjusted R-squared:  0.032 \nF-statistic: 9.62 on 2 and 519 DF,  p-value: 7.9e-05\n\n\nWe can make a nice plot of the data, alongside our model estimates. We can actually use the effects() package here too, just like we did for lmer().\n\n# plot the data\nggplot(bbdat, aes(x = task, y = stress_change)) + \n  # jittered points\n  geom_jitter(width=.15, height=0, alpha=.2, size = 3) +\n  # plot the model estimated means and CIs:\n  geom_pointrange(\n    data = effects::effect(\"task\", mod1) |&gt; as.data.frame(),\n    aes(y=fit,ymin=lower,ymax=upper),\n    position = position_nudge(x=.25)\n  )",
    "crumbs": [
      "Week 7",
      "W7 Exercises: Questionnaire Data & Scale Scores"
    ]
  },
  {
    "objectID": "r09_rotato.html",
    "href": "r09_rotato.html",
    "title": "optional rotations",
    "section": "",
    "text": "The two plots below show an unrotated and a rotated solution to the same data:\n\nNo rotation\n\n\n\n\n\n\n\n\nFigure 1: unrotated solution\n\n\n\n\n\n\nRotation\n\n\n\n\n\n\n\n\nFigure 2: rotated solution"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to the DAPR3 Lab Workbook",
    "section": "",
    "text": "Welcome to the Data Analysis for Psychology in R 3 (DAPR3) lab workbook. Exercises for each week can be found in the menu to the left."
  },
  {
    "objectID": "00prereq.html",
    "href": "00prereq.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Install/Update R & RStudio\nMake sure you have installed both R and RStudio on your computer. You may have done this previously for DAPR2, in which case it is probably worth doing some updates.\nPlease make sure to read and follow the instructions below slowly and carefully!!\n\nFor instructions on how to install R and RStudio, click here\nFor instructions on how to update R and RStudio, click here\n\n\n\nUpdate Packages\nIt’s worth keeping packages up to date, so it might be worth updating all your packages.\nRunning this code will update all your packages. Just put it into the console (bottom left bit of RStudio):\n\noptions(pkgType = \"binary\")\nupdate.packages(ask = FALSE)\n\n\n\nNew packages!\nNow it is probably worth installing a few of the packages that we will be using in DAPR3. There are a few that we will need. For each one, check whether you have it already installed, because there’s not much point wasting time re-installing something you already have!\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models",
    "crumbs": [
      "Prerequisites"
    ]
  },
  {
    "objectID": "r08_eggs.html",
    "href": "r08_eggs.html",
    "title": "optional eigen decomp",
    "section": "",
    "text": "Doing data reduction can feel a bit like magic, and in part that’s just because it’s quite complicated.\nThe intuition\nConsider one way we might construct a correlation matrix - as the product of vector \\(\\mathbf{f}\\) with \\(\\mathbf{f'}\\) (f transposed): \\[\n\\begin{equation*}\n\\mathbf{f} =\n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix}\n\\qquad\n\\mathbf{f} \\mathbf{f'} =\n\\begin{bmatrix}\n0.9 \\\\\n0.8 \\\\\n0.7 \\\\\n0.6 \\\\\n0.5 \\\\\n0.4 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n0.9, 0.8, 0.7, 0.6, 0.5, 0.4 \\\\\n\\end{bmatrix}\n\\qquad = \\qquad\n\\begin{bmatrix}\n0.81, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 0.64, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 0.49, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 0.36, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 0.25, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 0.16 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nBut we constrain this such that the diagonal has values of 1 (the correlation of a variable with itself is 1), and lets call it R. \\[\n\\begin{equation*}\n\\mathbf{R} =\n\\begin{bmatrix}\n1.00, 0.72, 0.63, 0.54, 0.45, 0.36 \\\\\n0.72, 1.00, 0.56, 0.48, 0.40, 0.32 \\\\\n0.63, 0.56, 1.00, 0.42, 0.35, 0.28 \\\\\n0.54, 0.48, 0.42, 1.00, 0.30, 0.24 \\\\\n0.45, 0.40, 0.35, 0.30, 1.00, 0.20 \\\\\n0.36, 0.32, 0.28, 0.24, 0.20, 1.00 \\\\\n\\end{bmatrix}\n\\end{equation*}\n\\]\nPCA is about trying to determine a vector f which generates the correlation matrix R. a bit like unscrambling eggs!\nin PCA, we express \\(\\mathbf{R = CC'}\\), where \\(\\mathbf{C}\\) are our principal components.\nIf \\(n\\) is number of variables in \\(R\\), then \\(i^{th}\\) component \\(C_i\\) is the linear sum of each variable multiplied by some weighting:\n\\[\nC_i = \\sum_{j=1}^{n}w_{ij}x_{j}\n\\]\nHow do we find \\(C\\)?\nThis is where “eigen decomposition” comes in.\nFor the \\(n \\times n\\) correlation matrix \\(\\mathbf{R}\\), there is an eigenvector \\(x_i\\) that solves the equation \\[\n\\mathbf{x_i R} = \\lambda_i \\mathbf{x_i}\n\\] Where the vector multiplied by the correlation matrix is equal to some eigenvalue \\(\\lambda_i\\) multiplied by that vector.\nWe can write this without subscript \\(i\\) as: \\[\n\\begin{align}\n& \\mathbf{R X} = \\mathbf{X \\lambda} \\\\\n& \\text{where:} \\\\\n& \\mathbf{R} = \\text{correlation matrix} \\\\\n& \\mathbf{X} = \\text{matrix of eigenvectors} \\\\\n& \\mathbf{\\lambda} = \\text{vector of eigenvalues}\n\\end{align}\n\\] the vectors which make up \\(\\mathbf{X}\\) must be orthogonal (\\(\\mathbf{XX' = I}\\)), which means that \\(\\mathbf{R = X \\lambda X'}\\)\nWe can actually do this in R manually. Creating a correlation matrix\n\n# lets create a correlation matrix, as the produce of ff'\nf &lt;- seq(.9,.4,-.1)\nR &lt;- f %*% t(f)\n#give rownames and colnames\nrownames(R)&lt;-colnames(R)&lt;-paste0(\"V\",seq(1:6))\n#constrain diagonals to equal 1\ndiag(R)&lt;-1\nR\n\n     V1   V2   V3   V4   V5   V6\nV1 1.00 0.72 0.63 0.54 0.45 0.36\nV2 0.72 1.00 0.56 0.48 0.40 0.32\nV3 0.63 0.56 1.00 0.42 0.35 0.28\nV4 0.54 0.48 0.42 1.00 0.30 0.24\nV5 0.45 0.40 0.35 0.30 1.00 0.20\nV6 0.36 0.32 0.28 0.24 0.20 1.00\n\n\nEigen Decomposition\n\n# do eigen decomposition\ne &lt;- eigen(R)\nprint(e, digits=2)\n\neigen() decomposition\n$values\n[1] 3.16 0.82 0.72 0.59 0.44 0.26\n\n$vectors\n      [,1]   [,2]   [,3]  [,4]   [,5]   [,6]\n[1,] -0.50 -0.061  0.092  0.14  0.238  0.816\n[2,] -0.47 -0.074  0.121  0.21  0.657 -0.533\n[3,] -0.43 -0.096  0.182  0.53 -0.675 -0.184\n[4,] -0.39 -0.142  0.414 -0.78 -0.201 -0.104\n[5,] -0.34 -0.299 -0.860 -0.20 -0.108 -0.067\n[6,] -0.28  0.934 -0.178 -0.10 -0.067 -0.045\n\n\nThe eigenvectors are orthogonal (\\(\\mathbf{XX' = I}\\)):\n\nround(e$vectors %*% t(e$vectors),2)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,]    1    0    0    0    0    0\n[2,]    0    1    0    0    0    0\n[3,]    0    0    1    0    0    0\n[4,]    0    0    0    1    0    0\n[5,]    0    0    0    0    1    0\n[6,]    0    0    0    0    0    1\n\n\nThe Principal Components \\(\\mathbf{C}\\) are the eigenvectors scaled by the square root of the eigenvalues:\n\n#eigenvectors\ne$vectors\n\n       [,1]    [,2]    [,3]   [,4]    [,5]    [,6]\n[1,] -0.496 -0.0611  0.0923  0.139  0.2385  0.8155\n[2,] -0.468 -0.0743  0.1210  0.214  0.6566 -0.5327\n[3,] -0.433 -0.0963  0.1820  0.530 -0.6751 -0.1842\n[4,] -0.390 -0.1416  0.4143 -0.778 -0.2006 -0.1036\n[5,] -0.340 -0.2992 -0.8604 -0.197 -0.1076 -0.0669\n[6,] -0.282  0.9338 -0.1784 -0.100 -0.0667 -0.0452\n\n#scaled by sqrt of eigenvalues\ndiag(sqrt(e$values))\n\n     [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] 1.78 0.000 0.000 0.000 0.000 0.000\n[2,] 0.00 0.906 0.000 0.000 0.000 0.000\n[3,] 0.00 0.000 0.848 0.000 0.000 0.000\n[4,] 0.00 0.000 0.000 0.769 0.000 0.000\n[5,] 0.00 0.000 0.000 0.000 0.664 0.000\n[6,] 0.00 0.000 0.000 0.000 0.000 0.512\n\nC &lt;- e$vectors %*% diag(sqrt(e$values))\nC\n\n       [,1]    [,2]    [,3]    [,4]    [,5]    [,6]\n[1,] -0.883 -0.0554  0.0782  0.1070  0.1584  0.4174\n[2,] -0.833 -0.0673  0.1025  0.1648  0.4361 -0.2727\n[3,] -0.770 -0.0873  0.1542  0.4077 -0.4483 -0.0943\n[4,] -0.694 -0.1284  0.3512 -0.5987 -0.1332 -0.0530\n[5,] -0.604 -0.2712 -0.7293 -0.1514 -0.0715 -0.0342\n[6,] -0.502  0.8464 -0.1513 -0.0771 -0.0443 -0.0231\n\n\nAnd we can reproduce our correlation matrix, because \\(\\mathbf{R = CC'}\\).\n\nC %*% t(C)\n\n     [,1] [,2] [,3] [,4] [,5] [,6]\n[1,] 1.00 0.72 0.63 0.54 0.45 0.36\n[2,] 0.72 1.00 0.56 0.48 0.40 0.32\n[3,] 0.63 0.56 1.00 0.42 0.35 0.28\n[4,] 0.54 0.48 0.42 1.00 0.30 0.24\n[5,] 0.45 0.40 0.35 0.30 1.00 0.20\n[6,] 0.36 0.32 0.28 0.24 0.20 1.00\n\n\nNow lets imagine we only consider 1 principal component.\nWe can do this with the principal() function:\n\nlibrary(psych)\npc1&lt;-principal(R, nfactors = 1, covar = FALSE, rotate = 'none')\npc1\n\nPrincipal Components Analysis\nCall: principal(r = R, nfactors = 1, rotate = \"none\", covar = FALSE)\nStandardized loadings (pattern matrix) based upon correlation matrix\n    PC1   h2   u2 com\nV1 0.88 0.78 0.22   1\nV2 0.83 0.69 0.31   1\nV3 0.77 0.59 0.41   1\nV4 0.69 0.48 0.52   1\nV5 0.60 0.37 0.63   1\nV6 0.50 0.25 0.75   1\n\n                PC1\nSS loadings    3.16\nProportion Var 0.53\n\nMean item complexity =  1\nTest of the hypothesis that 1 component is sufficient.\n\nThe root mean square of the residuals (RMSR) is  0.09 \n\nFit based upon off diagonal values = 0.95\n\n\nLook familiar? It looks like the first component we computed manually. The first column of \\(\\mathbf{C}\\):\n\ncbind(pc1$loadings, C=C[,1])\n\n     PC1      C\nV1 0.883 -0.883\nV2 0.833 -0.833\nV3 0.770 -0.770\nV4 0.694 -0.694\nV5 0.604 -0.604\nV6 0.502 -0.502\n\n\nWe can now ask “how well does this component (on its own) recreate our correlation matrix?”\n\nC[,1] %*% t(C[,1])\n\n      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]\n[1,] 0.780 0.735 0.680 0.613 0.534 0.444\n[2,] 0.735 0.693 0.641 0.578 0.503 0.418\n[3,] 0.680 0.641 0.592 0.534 0.465 0.387\n[4,] 0.613 0.578 0.534 0.481 0.419 0.348\n[5,] 0.534 0.503 0.465 0.419 0.365 0.304\n[6,] 0.444 0.418 0.387 0.348 0.304 0.252\n\n\nIt looks close, but not quite. How much not quite? Measurably so!\n\nR - (C[,1] %*% t(C[,1]))\n\n        V1      V2      V3      V4      V5      V6\nV1  0.2200 -0.0154 -0.0498 -0.0727 -0.0838 -0.0836\nV2 -0.0154  0.3067 -0.0809 -0.0976 -0.1033 -0.0982\nV3 -0.0498 -0.0809  0.4075 -0.1140 -0.1153 -0.1066\nV4 -0.0727 -0.0976 -0.1140  0.5187 -0.1193 -0.1085\nV5 -0.0838 -0.1033 -0.1153 -0.1193  0.6346 -0.1036\nV6 -0.0836 -0.0982 -0.1066 -0.1085 -0.1036  0.7477\n\n\nNotice the values on the diagonals of \\(\\mathbf{c_1}\\mathbf{c_1}'\\).\n\ndiag(C[,1] %*% t(C[,1]))\n\n[1] 0.780 0.693 0.592 0.481 0.365 0.252\n\n\nThese aren’t 1, like they are in \\(R\\). But they are proportional: this is the amount of variance in each observed variable that is explained by this first component. Sound familiar?\n\npc1$communality\n\n   V1    V2    V3    V4    V5    V6 \n0.780 0.693 0.592 0.481 0.365 0.252 \n\n\nAnd likewise the 1 minus these is the unexplained variance:\n\n1 - diag(C[,1] %*% t(C[,1]))\n\n[1] 0.220 0.307 0.408 0.519 0.635 0.748\n\npc1$uniquenesses\n\n   V1    V2    V3    V4    V5    V6 \n0.220 0.307 0.408 0.519 0.635 0.748"
  },
  {
    "objectID": "08ex.html",
    "href": "08ex.html",
    "title": "W8 Exercises: PCA",
    "section": "",
    "text": "Relevant packages\n\npsych\n\n\n\nExercises: Police Performance\n\nData: police_performance.csv\nThe dataset available at https://uoepsy.github.io/data/police_performance.csv contains records on fifty police officers who were rated in six different categories as part of an HR procedure. The rated skills were:\n\ncommunication skills: commun\nproblem solving: probl_solv\nlogical ability: logical\nlearning ability: learn\nphysical ability: physical\nappearance: appearance\n\nThe data also contains information on each police officer’s arrest rate (proportion of arrests that lead to criminal charges).\nWe are interested in if the skills ratings by HR are a good set of predictors of police officer success (as indicated by their arrest rate).\n\n\nQuestion 1\n\n\nLoad the job performance data into R and call it job. Check whether or not the data were read correctly into R - do the dimensions correspond to the description of the data above?\n\n\n\n\n\nSolution\n\n\n\nSolution 1. Let’s load the data:\n\nlibrary(tidyverse)\n\njob &lt;- read_csv('https://uoepsy.github.io/data/police_performance.csv')\ndim(job)\n\n[1] 50  7\n\n\nThere are 50 observations on 6 variables.\nThe top 6 rows in the data are:\n\nhead(job)\n\n# A tibble: 6 × 7\n  commun probl_solv logical learn physical appearance arrest_rate\n   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n1     12         52      20    44       48         16      0.613 \n2     12         57      25    45       50         16      0.419 \n3     12         54      21    45       50         16      0     \n4     13         52      21    46       51         17      0.645 \n5     14         54      24    46       51         17      0.0645\n6     14         48      20    47       51         18      0.645 \n\n\n\n\n\n\nQuestion 2\n\n\nProvide descriptive statistics for each variable in the dataset.\n\n\n\n\n\nSolution\n\n\n\nSolution 2. We now inspect some descriptive statistics for each variable in the dataset:\n\n# Quick summary\nsummary(job)\n\n     commun       probl_solv      logical       learn         physical   \n Min.   :12.0   Min.   :48.0   Min.   :20   Min.   :44.0   Min.   :48.0  \n 1st Qu.:16.0   1st Qu.:52.2   1st Qu.:22   1st Qu.:48.0   1st Qu.:52.2  \n Median :18.0   Median :54.0   Median :24   Median :50.0   Median :54.0  \n Mean   :17.7   Mean   :54.2   Mean   :24   Mean   :50.3   Mean   :54.2  \n 3rd Qu.:19.8   3rd Qu.:56.0   3rd Qu.:26   3rd Qu.:52.0   3rd Qu.:56.0  \n Max.   :24.0   Max.   :59.0   Max.   :31   Max.   :56.0   Max.   :59.0  \n   appearance    arrest_rate   \n Min.   :16.0   Min.   :0.000  \n 1st Qu.:19.0   1st Qu.:0.371  \n Median :21.0   Median :0.565  \n Mean   :21.1   Mean   :0.512  \n 3rd Qu.:23.0   3rd Qu.:0.669  \n Max.   :28.0   Max.   :0.935  \n\n\nOPTIONAL\nIf you wish to create a nice looking table for a report, you could try the following code. However, I should warn you: this code is quite difficult to understand - have a go at running sections at a time - slowly adding each function in the pipe to see how it changes.\n\nlibrary(gt)\n\n# Mean and SD of each variable\njob |&gt;\n  summarise(across(everything(), list(M = mean, SD = sd))) |&gt;\n  pivot_longer(everything()) |&gt;\n  mutate(\n    value = round(value, 2),\n    name = str_replace(name, '_M', '.M'),\n    name = str_replace(name, '_SD', '.SD')\n  ) |&gt;\n  separate(name, into = c('variable', 'summary'), sep = '\\\\.') |&gt;\n  pivot_wider(names_from = summary, values_from = value) |&gt;\n  gt()\n\n\n\n\n\n\n\nvariable\nM\nSD\n\n\n\n\ncommun\n17.68\n2.74\n\n\nprobl_solv\n54.16\n2.41\n\n\nlogical\n24.02\n2.49\n\n\nlearn\n50.28\n2.84\n\n\nphysical\n54.16\n2.41\n\n\nappearance\n21.06\n2.99\n\n\narrest_rate\n0.51\n0.23\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nWorking with only the skills ratings (not the arrest rate - we’ll come back to that right at the end), investigate whether or not the variables are highly correlated and explain whether or not you PCA might be useful in this case.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe only have 6 variables here, but if we had many, how might you visualise cor(job)? Try the below:\n\nlibrary(pheatmap)\npheatmap(cor(data))\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. Let’s start by looking at the correlation matrix of the data:\n\nlibrary(pheatmap)\n\njob_skills &lt;- job |&gt; select(-arrest_rate)\n\nR &lt;- cor(job_skills)\n\npheatmap(R, breaks = seq(-1, 1, length.out = 100))\n\n\n\n\n\n\n\nFigure 1: Correlation between the variables in the ``Job’’ dataset\n\n\n\n\n\nThe correlation between the variables seems to be quite large (it doesn’t matter about direction here, only magnitude; if negative correlations were present, we would think in absolute value).\nThere appears to be a group of highly correlated variables comprising physical ability, appearance, communication skills, and learning ability which are correlated among themselves but uncorrelated with another group of variables. The second group comprises problem solving and logical ability.\nThis suggests that PCA might be useful in this problem to reduce the dimensionality without a significant loss of information.\n\n\n\n\nQuestion 4\n\n\nLook at the variance of the skills ratings in the data set. Do you think that PCA should be carried on the covariance matrix or the correlation matrix? Or does it not matter?\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee Reading 8: Performing PCA.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. Let’s have a look at the standard deviation of each variable:\n\njob_skills |&gt; \n  summarise(across(everything(), sd))\n\n# A tibble: 1 × 6\n  commun probl_solv logical learn physical appearance\n   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1   2.74       2.41    2.49  2.84     2.41       2.99\n\n\nAs the standard deviations appear to be fairly similar (and so will the variances) we can perform PCA using the covariance matrix if we want and it probably won’t differ too much from the correlation matrix.\n\n\n\n\nQuestion 5\n\n\nUsing the principal() function from the psych package, we conduct a PCA of the job skills.\n\n\n\n\n\n\nHints\n\n\n\n\n\nReading 8: Performing PCA shows an example of how to use the principal() function.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. \n\nlibrary(psych)\n\njob_pca &lt;- principal(job_skills, nfactors = ncol(job_skills), \n                     rotate = 'none')\n\n\n\n\n\nQuestion 6\n\n\nLooking at the PCA output, how many principal components would you keep if you were following the cumulative proportion of explained variance criterion?\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee Reading 8: How many components to keep? for an explanation of various criteria for deciding how many components we should keep.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. Let’s look again at the PCA summary:\n\njob_pca$loadings\n\n\nLoadings:\n           PC1    PC2    PC3    PC4    PC5    PC6   \ncommun      0.984 -0.120                0.101       \nprobl_solv  0.223  0.810  0.543                     \nlogical     0.329  0.747 -0.578                     \nlearn       0.987 -0.110                       0.105\nphysical    0.988                      -0.110       \nappearance  0.979 -0.125         0.161              \n\n                 PC1   PC2   PC3   PC4   PC5   PC6\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nProportion Var 0.673 0.210 0.105 0.006 0.004 0.003\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n\n\nThe following part of the output tells us that the first two components explain 88.3% of the total variance.\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\nAccording to this criterion, we should keep 2 principal components.\n\n\n\n\nQuestion 7\n\n\nLooking again at the PCA output, how many principal components would you keep if you were following Kaiser’s criterion?\n\n\n\n\n\nSolution\n\n\n\nSolution 7. \n\njob_pca$loadings\n\n\nLoadings:\n           PC1    PC2    PC3    PC4    PC5    PC6   \ncommun      0.984 -0.120                0.101       \nprobl_solv  0.223  0.810  0.543                     \nlogical     0.329  0.747 -0.578                     \nlearn       0.987 -0.110                       0.105\nphysical    0.988                      -0.110       \nappearance  0.979 -0.125         0.161              \n\n                 PC1   PC2   PC3   PC4   PC5   PC6\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nProportion Var 0.673 0.210 0.105 0.006 0.004 0.003\nCumulative Var 0.673 0.883 0.988 0.994 0.997 1.000\n\n\nThe eigenvalues are shown in the row\nSS loadings    4.035 1.261 0.631 0.035 0.022 0.016\nFrom the result we see that only the first two principal components have eigenvalues greater than 1, so this rule suggests to keep 2 PCs only.\n\n\n\n\nQuestion 8\n\n\nAccording to a scree plot, how many principal components would you retain?\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nscree(cor(job_skills))\n\n\n\n\n\n\n\n\nThis criterion could suggest 1, or maybe 3?\n\n\n\n\nQuestion 9\n\n\nHow many components should we keep according to the MAP method?\n\n\n\n\n\nSolution\n\n\n\nSolution 9. \n\nVSS(job_skills, plot=FALSE, method=\"pc\", n = ncol(job_skills))\n\n\nVery Simple Structure\nCall: vss(x = x, n = n, rotate = rotate, diagonal = diagonal, fm = fm, \n    n.obs = n.obs, plot = plot, title = title, use = use, cor = cor, \n    method = \"pc\")\nVSS complexity 1 achieves a maximimum of 0.95  with  3  factors\nVSS complexity 2 achieves a maximimum of 0.98  with  3  factors\n\nThe Velicer MAP achieves a minimum of 0.12  with  2  factors \nBIC achieves a minimum of  -24.1  with  1  factors\nSample Size adjusted BIC achieves a minimum of  -2.87  with  2  factors\n\nStatistics by number of factors \n  vss1 vss2  map dof   chisq prob sqresid  fit RMSEA BIC SABIC complex  eChisq\n1 0.89 0.00 0.17   9 1.1e+01 0.27   2.046 0.89 0.065 -24   4.1     1.0 1.1e+01\n2 0.92 0.96 0.12   4 2.3e-01 0.99   0.802 0.96 0.000 -15  -2.9     1.1 1.5e-03\n3 0.95 0.98 0.29   0 1.5e-01   NA   0.044 1.00    NA  NA    NA     1.1 1.8e-04\n4 0.92 0.96 0.50  -3 9.2e-07   NA   0.777 0.96    NA  NA    NA     1.1 1.6e-09\n5 0.92 0.96 1.00  -5 5.1e-11   NA   0.788 0.96    NA  NA    NA     1.1 1.5e-13\n6 0.92 0.96   NA  -6 5.0e-11   NA   0.788 0.96    NA  NA    NA     1.1 1.5e-13\n     SRMR  eCRMS eBIC\n1 8.5e-02 0.1096  -24\n2 1.0e-03 0.0019  -16\n3 3.5e-04     NA   NA\n4 1.0e-06     NA   NA\n5 9.8e-09     NA   NA\n6 9.8e-09     NA   NA\n\n\nAccording to the MAP criterion we should keep 2 principal components.\n\n\n\n\nQuestion 10\n\n\nHow many components should we keep according to parallel analysis?\n\n\n\n\n\nSolution\n\n\n\nSolution 10. \n\nfa.parallel(job_skills, fa=\"pc\", n.iter = 500)\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1 \n\n\nParallel analysis suggests to keep 1 principal component only as there is only one PC with an eigenvalue higher than the simulated random ones in red.\n\n\n\n\nQuestion 11\n\n\nBased on all of the criteria above, make a decision on how many components you will keep.\n\n\n\n\n\nSolution\n\n\n\nSolution 11. \n\n\n\nmethod\nrecommendation\n\n\n\n\nexplaining &gt;80% variance\nkeep 2 components\n\n\nkaiser’s rule\nkeep 2 components\n\n\nscree plot\nkeep 1 or 3 components? (subjective)\n\n\nMAP\nkeep 2 components\n\n\nparallel analysis\nkeep 1 component\n\n\n\nBecause three out of the five selection criteria above suggest to keep 2 principal components, here we will keep 2 components. This solution explains a reasonable proportion of the variance (88%), but it would be perfectly defensible to instead go for 3, explaining 98%\n\n\n\n\nQuestion 12\n\n\nperform PCA to extract the desired number of components\n\n\n\n\n\nSolution\n\n\n\nSolution 12. \n\njob_pca2 &lt;- principal(job_skills, nfactors = 2, \n                     rotate = 'none')\n\n\n\n\n\nQuestion 13\n\n\nExamine the loadings of the 2 Principal Components. Is there a link you can see?\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee Reading 8 #Examining Loadings for an explanation of the loadings.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 13. \n\njob_pca2$loadings\n\n\nLoadings:\n           PC1    PC2   \ncommun      0.984 -0.120\nprobl_solv  0.223  0.810\nlogical     0.329  0.747\nlearn       0.987 -0.110\nphysical    0.988       \nappearance  0.979 -0.125\n\n                 PC1   PC2\nSS loadings    4.035 1.261\nProportion Var 0.673 0.210\nCumulative Var 0.673 0.883\n\n\nAll loadings for the first PC seem to have a similar magnitude apart from probl_solv and logical which are closer to zero. The first component looks like a sort of average of the officers performance scores excluding problem solving and logical ability.\nThe second principal component, which explains only 21% of the total variance, has two loadings clearly distant from zero: the ones associated to problem solving and logical ability. It distinguishes police officers with strong logical and problem solving skills and low scores on other skills (note the negative magnitudes).\n\nFor interpretation purposes, it might help hiding very small loadings. This can be done by specifying the cutoff value in the print() function. However, this only works when you pass the loadings for all the PCs:\n\nprint(job_pca2$loadings, cutoff = 0.3)\n\n\nLoadings:\n           PC1    PC2   \ncommun      0.984       \nprobl_solv         0.810\nlogical     0.329  0.747\nlearn       0.987       \nphysical    0.988       \nappearance  0.979       \n\n                 PC1   PC2\nSS loadings    4.035 1.261\nProportion Var 0.673 0.210\nCumulative Var 0.673 0.883\n\n\n\n\n\n\n\nQuestion 14\n\n\nJoin the principal component scores for your retained components to the original dataset which has the arrest rates in.\nThen fit a linear model to look at how the arrest rate of police officers is predicted by the two components representing different composites of the skills ratings by HR.\nCheck for multicollinearity between your predictors. How does this compare to a model which uses all 6 of the original variables instead?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe can get out scores using mypca$scores. We can add them to an existing dataset by just adding them as a new column:\n\ndata |&gt;\n  mutate(\n    score1 = mypca$scores[,1]\n  )\n\nTo examine multicollinearity - try vif() from the car package.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 14. \n\n# add the PCA scores to the dataset\njob &lt;- \n  job |&gt; mutate(\n    pc1 = job_pca2$scores[,1],\n    pc2 = job_pca2$scores[,2]\n  )\n# use the scores in an analysis\nmod &lt;- lm(arrest_rate ~ pc1 + pc2, data = job)\n\n# multicollinearity isn't a problem, because the components are orthogonal!! \nlibrary(car)\nvif(mod)\n\npc1 pc2 \n  1   1 \n\nlm(arrest_rate ~ commun+probl_solv+logical+learn+physical+appearance, \n   data = job) |&gt;\n  vif()\n\n    commun probl_solv    logical      learn   physical appearance \n     34.67       1.17       1.23      43.56      34.98      21.78",
    "crumbs": [
      "Week 8",
      "W8 Exercises: PCA"
    ]
  },
  {
    "objectID": "11ex.html",
    "href": "11ex.html",
    "title": "W11 Exercises",
    "section": "",
    "text": "Q1 - EFA [14 marks]\n\n\nBelow are the results of a data reduction of a set of 12 items assessing environmental conscientiousness. Participants are asked to respond to each statement on a 5-point likert scale from “Strongly Disagree” to “Strongly Agree”.\nBased on the results and the item descriptions below, provide an interpretation of the factor solution. Your description should include:\n\nComment on the numerical solution [6 marks]\nDiscussion of suitability of the selected number of factors [6 marks]\nAn attempt to define the factors [2 marks]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nitem\nwording\n\n\n\n\ny1\nI recycle regularly\n\n\ny2\nI use eco-friendly transportation\n\n\ny3\nI buy sustainable products\n\n\ny4\nI know how to reduce my carbon footprint\n\n\ny5\nProtecting resources matters to me\n\n\ny6\nI care about protecting the environment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nitem\nwording\n\n\n\n\ny7\nI feel responsible for my environmental impact\n\n\ny8\nI am worried about climate change effects\n\n\ny9\nI know about the harm of single-use plastics\n\n\ny10\nI know how deforestation affects climate change\n\n\ny11\nI know about relevant environmental policies\n\n\ny12\nWildlife destruction concerns me deeply\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12 items, 4 factors extracted. explains 29% variance\nFactors ML2 & ML1 both have \\(\\geq 3\\) salient/primary loadings\n\nsalient = \\(\\geq |0.3|\\)\n\nFactors ML3 & ML4: both have only 1 item with salient loading\n3rd factor explaining only 6%, 4th factor only 2%\nSSloadings for ML3 & ML4 are &lt;1\n3 complex items (y4, y5, y6)\nsome items (y5,y6) have no salient loadings\nFactors ML1,2,3 correlated weak-moderate\nFactor ML4 not strongly correlated with others\nprobably overextracting (too many factors)\nmain pointer = low variance expl of ML3 & ML4, plus not enough items\ncomplex items y5 & y6 are spread across ML3 & ML4 - a 3 factor solution may well make more sense\nitem y4 = one to keep an eye on\nML1 = “environmental knowledge”\nML2 = “environmental behaviours”\nML3/4 combined look like “environmental concern”\n\n\n\n\n\nQ2 - SSloadings [6 marks]\n\n\nCalculate the 6 values missing from the table below: SSloadings [2 marks] and proportion variance [2 marks] & cumulative variance [2 marks].\n\n\n       PC1   PC2\nitem1 0.90  0.00\nitem2 0.90 -0.29\nitem3 0.90  0.30\nitem4 0.70  0.70\nitem5 0.81 -0.50\n\n\n               PC1  PC2 \nSS loadings     __   __ \nProportion Var  __   __ \nCumulative Var  __   __ \n\n\n\n\n\n\nTable filled in:\n\n\n                 PC1   PC2\nSS loadings    3.576 0.914\nProportion Var 0.715 0.183\nCumulative Var 0.715 0.898\n\n\nHow?\nstart by squaring all the numbers, and sum the columns to give us SSloadings:\n\n\n        PC1    PC2\nitem1 0.810 0.0000\nitem2 0.810 0.0841\nitem3 0.810 0.0900\nitem4 0.490 0.4900\nitem5 0.656 0.2500\nSum   3.576 0.9141\n\n\ndivide SSloadings by 5 (because 5 observed variables) to get proportion variance\n\n\n  PC1   PC2 \n0.715 0.183 \n\n\nthose two numbers are then cumulatively summed for cumulative variance:\n\n\n  PC1   PC2 \n0.715 0.898 \n\n\n\n\n\n\nQ3 - MLM [10 marks]\n\n\nA company that makes “6-minute journals” is undertaking some research to showcase the effectiveness of their product in helping to alleviate unwanted feelings. They recruited 166 people signing up to one of 10 “anger management classes” in different cities, and asked them if they would like to have a free journal to help with reflection. 88 participants chose to take a journal, and 78 did not. Each participant filled out weekly assessments of anger levels for 10 weeks. Scores on the anger measure can range from 0 to 15, with changes of 3 being considered ‘clinically meaningful’.\nTo investigate if having a journal helps to reduce anger levels, the company fit a multilevel model to the data, with anger levels being modelled by week number (0 to 9, with 0 representing the first week participants filled in the anger assessment), whether the journal was used (“no”/“yes”, with “no” as the reference level).\nProvide an interpretation of each of:\n\nthe fixed effects [4 marks]\nthe random effects [4 marks]\nthe relevance of the findings, considering the context of the study design and researchers’ aims [2 marks]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nanger for someone who doesn’t journal, at start (in “week 1”, or “week 0” is fine here, give benefit of doubt) is 10.22\nno significant change over the study period for those who don’t journal\npeople to take the journal have lower anger at the start by -0.3\njournal group decreases in anger over the study by an additional -0.24 compared to no-journal group\nboth participants and classes vary in starting anger levels and in change in anger over study period\nparticipants vary (both intercept and slopes of change) much more than classes\nhigh level of ppt variability relative to fixed slope\nppts who start more angry decrease less (positive correlation intercepts and slopes)\ntake journal = significant reduction in anger\neffect is small - over 10 weeks they only go down by -1.622\ndifference in two groups at outset suggests two groups are not comparable\nself-selecting journal - maybe all we’re doing is splitting up people who do/don’t want to change\n\n\n\n\n\nQ4 - hierarchical data structures [3 marks]\n\n\nProvide example levels for each of the three types of study: Cross-Sectional, Repeated Measures, Longitudinal [3 marks]\n\n\n\nlevel\ncross-sectional\nrepeated measures\nlongitudinal\n\n\n\n\n2\n…\n…\n…\n\n\n1\n…\n…\n…\n\n\n\n\n\n\n\nanything that makes sense here, obvious ones are:\n\n\n\nlevel\ncross-sec\nrpt measures\nlongitudinal\n\n\n\n\n2\ndepartment\npeople\npeople\n\n\n1\npeople\nexperimental items\ntimepoints",
    "crumbs": [
      "Week 11",
      "W11 Exercises"
    ]
  },
  {
    "objectID": "11exWALLEN.html",
    "href": "11exWALLEN.html",
    "title": "W11 Exercises",
    "section": "",
    "text": "Q1 - EFA [14 marks]\n\n\nBelow are the results of a data reduction of a set of 12 items assessing environmental conscientiousness. Participants are asked to respond to each statement on a 5-point likert scale from “Strongly Disagree” to “Strongly Agree”.\nBased on the results and the item descriptions below, provide an interpretation of the factor solution. Your description should include:\n\nComment on the numerical solution [6 marks]\nDiscussion of suitability of the selected number of factors [6 marks]\nAn attempt to define the factors [2 marks]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nitem\nwording\n\n\n\n\ny1\nI recycle regularly\n\n\ny2\nI use eco-friendly transportation\n\n\ny3\nI buy sustainable products\n\n\ny4\nI know how to reduce my carbon footprint\n\n\ny5\nProtecting resources matters to me\n\n\ny6\nI care about protecting the environment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nitem\nwording\n\n\n\n\ny7\nI feel responsible for my environmental impact\n\n\ny8\nI am worried about climate change effects\n\n\ny9\nI know about the harm of single-use plastics\n\n\ny10\nI know how deforestation affects climate change\n\n\ny11\nI know about relevant environmental policies\n\n\ny12\nWildlife destruction concerns me deeply\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\n12 items, 4 factors extracted. explains 29% variance\nFactors ML2 & ML1 both have \\(\\geq 3\\) salient/primary loadings\n\nsalient = \\(\\geq |0.3|\\)\n\nFactors ML3 & ML4: both have only 1 item with salient loading\n3rd factor explaining only 6%, 4th factor only 2%\nSSloadings for ML3 & ML4 are &lt;1\n3 complex items (y4, y5, y6)\nsome items (y5,y6) have no salient loadings\nFactors ML1,2,3 correlated weak-moderate\nFactor ML4 not strongly correlated with others\nprobably overextracting (too many factors)\nmain pointer = low variance expl of ML3 & ML4, plus not enough items\ncomplex items y5 & y6 are spread across ML3 & ML4 - a 3 factor solution may well make more sense\nitem y4 = one to keep an eye on\nML1 = “environmental knowledge”\nML2 = “environmental behaviours”\nML3/4 combined look like “environmental concern”\n\n\n\n\n\nQ2 - SSloadings [6 marks]\n\n\nCalculate the 6 values missing from the table below: SSloadings [2 marks] and proportion variance [2 marks] & cumulative variance [2 marks].\n\n\n       PC1   PC2\nitem1 0.90  0.00\nitem2 0.90 -0.29\nitem3 0.90  0.30\nitem4 0.70  0.70\nitem5 0.81 -0.50\n\n\n               PC1  PC2 \nSS loadings     __   __ \nProportion Var  __   __ \nCumulative Var  __   __ \n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. Table filled in:\n\n\n                 PC1   PC2\nSS loadings    3.576 0.914\nProportion Var 0.715 0.183\nCumulative Var 0.715 0.898\n\n\nHow?\nstart by squaring all the numbers, and sum the columns to give us SSloadings:\n\n\n        PC1    PC2\nitem1 0.810 0.0000\nitem2 0.810 0.0841\nitem3 0.810 0.0900\nitem4 0.490 0.4900\nitem5 0.656 0.2500\nSum   3.576 0.9141\n\n\ndivide SSloadings by 5 (because 5 observed variables) to get proportion variance\n\n\n  PC1   PC2 \n0.715 0.183 \n\n\nthose two numbers are then cumulatively summed for cumulative variance:\n\n\n  PC1   PC2 \n0.715 0.898 \n\n\n\n\n\n\nQ3 - MLM [10 marks]\n\n\nA company that makes “6-minute journals” is undertaking some research to showcase the effectiveness of their product in helping to alleviate unwanted feelings. They recruited 166 people signing up to one of 10 “anger management classes” in different cities, and asked them if they would like to have a free journal to help with reflection. 88 participants chose to take a journal, and 78 did not. Each participant filled out weekly assessments of anger levels for 10 weeks. Scores on the anger measure can range from 0 to 15, with changes of 3 being considered ‘clinically meaningful’.\nTo investigate if having a journal helps to reduce anger levels, the company fit a multilevel model to the data, with anger levels being modelled by week number (0 to 9, with 0 representing the first week participants filled in the anger assessment), whether the journal was used (“no”/“yes”, with “no” as the reference level).\nProvide an interpretation of each of:\n\nthe fixed effects [4 marks]\nthe random effects [4 marks]\nthe relevance of the findings, considering the context of the study design and researchers’ aims [2 marks]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\nanger for someone who doesn’t journal, at start (in “week 1”, or “week 0” is fine here, give benefit of doubt) is 10.22\nno significant change over the study period for those who don’t journal\npeople to take the journal have lower anger at the start by -0.3\njournal group decreases in anger over the study by an additional -0.24 compared to no-journal group\nboth participants and classes vary in starting anger levels and in change in anger over study period\nparticipants vary (both intercept and slopes of change) much more than classes\nhigh level of ppt variability relative to fixed slope\nppts who start more angry decrease less (positive correlation intercepts and slopes)\ntake journal = significant reduction in anger\neffect is small - over 10 weeks they only go down by -1.622\ndifference in two groups at outset suggests two groups are not comparable\nself-selecting journal - maybe all we’re doing is splitting up people who do/don’t want to change\n\n\n\n\n\nQ4 - hierarchical data structures [3 marks]\n\n\nProvide example levels for each of the three types of study: Cross-Sectional, Repeated Measures, Longitudinal [3 marks]\n\n\n\nlevel\ncross-sectional\nrepeated measures\nlongitudinal\n\n\n\n\n2\n…\n…\n…\n\n\n1\n…\n…\n…\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. anything that makes sense here, obvious ones are:\n\n\n\nlevel\ncross-sec\nrpt measures\nlongitudinal\n\n\n\n\n2\ndepartment\npeople\npeople\n\n\n1\npeople\nexperimental items\ntimepoints"
  },
  {
    "objectID": "01ex.html",
    "href": "01ex.html",
    "title": "W1 Exercises: Regression Refresher",
    "section": "",
    "text": "Workplace Pride\n\nData: lmm_jsup.csv\nA questionnaire was sent to all UK civil service departments, and the lmm_jsup.csv dataset contains all responses that were received. Some of these departments work as hybrid or ‘virtual’ departments, with a mix of remote and office-based employees. Others are fully office-based.\nThe questionnaire included items asking about how much the respondent believe in the department and how it engages with the community, what it produces, how it operates and how treats its people. A composite measure of ‘workplace-pride’ was constructed for each employee. Employees in the civil service are categorised into 3 different roles: A, B and C. The roles tend to increase in responsibility, with role C being more managerial, and role A having less responsibility. We also have data on the length of time each employee has been in the department (sometimes new employees come straight in at role C, but many of them start in role A and work up over time).\nWe’re interested in whether the different roles are associated with differences in workplace-pride.\nDataset: https://uoepsy.github.io/data/lmm_jsup.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndepartment_name\nName of government department\n\n\ndept\nDepartment Acronym\n\n\nvirtual\nWhether the department functions as hybrid department with various employees working remotely (1), or as a fully in-person office (0)\n\n\nrole\nEmployee role (A, B or C)\n\n\nseniority\nEmployees seniority point. These map to roles, such that role A is 0-4, role B is 5-9, role C is 10-14. Higher numbers indicate more seniority\n\n\nemployment_length\nLength of employment in the department (years)\n\n\nwp\nComposite Measure of 'Workplace Pride'\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and provide some descriptive statistics.\n\n\n\n\n\n\nHints\n\n\n\n\n\nDon’t remember how to do descriptives? Think back to previous courses - it’s time for some means, standard deviations, mins and maxes. For categorical variables we can do counts or proportions.\nWe’ve seen various functions such as summary(), and also describe() from the psych package.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. Here’s the dataset:\n\nlibrary(tidyverse) # for data wrangling\nlibrary(psych) \n\njsup &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\nLet’s take just the numeric variables and get some descriptives:\n\njsup |&gt; \n  select(employment_length, wp) |&gt; \n  describe()\n\n                  vars   n mean   sd median trimmed  mad  min  max range  skew\nemployment_length    1 295 12.6 4.28   13.0    12.6 4.45 0.00 30.0  30.0  0.08\nwp                   2 295 25.5 5.27   25.4    25.5 5.93 6.34 38.5  32.1 -0.05\n                  kurtosis   se\nemployment_length     0.38 0.25\nwp                   -0.14 0.31\n\n\nAnd make frequency tables for the categorical ones:\n\ntable(jsup$role)\n\n\n  A   B   C \n109  95  91 \n\n\nI’m going to use dept rather than department_name as the output will be easier to see:\n\ntable(jsup$dept)\n\n\n   ACE    CMA    CPS    FSA    GLD   HMRC    NCA   NS&I  OFGEM OFQUAL OFSTED \n    17     21     13     25     17     16     20     20     15      5     17 \n OFWAT    ORR    SFO   UKSA   UKSC \n    16     17     18     45     13 \n\ntable(jsup$virtual)\n\n\n  0   1 \n175 120 \n\n\n\n\n\n\nQuestion 2\n\n\nAre there differences in ‘workplace-pride’ between people in different roles?\n\n\n\n\n\n\nHints\n\n\n\n\n\ndoes y [continuous variable] differ by x [three groups]? lm(y ~ x)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\nmod1 &lt;- lm(wp ~ role, data = jsup)\n\nRather than doing summary(model) - I’m just going to use the broom package to pull out some of the stats in nice tidy dataframes.\nThe glance() function will give us things like the \\(R^2\\) values and \\(F\\)-statistic (basically all the stuff that is at the bottom of the summary()):\n\nlibrary(broom)\nglance(mod1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.216         0.211  4.68      40.3 3.44e-16     2  -872. 1753. 1768.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe tidy() function will give us the coefficients, standard errors, t-statistics and p-values. It’s the same information, just neater!\n\ntidy(mod1)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    28.1      0.448     62.6  3.66e-171\n2 roleB          -2.24     0.657     -3.41 7.33e-  4\n3 roleC          -5.95     0.665     -8.95 4.38e- 17\n\n\nAlternatively, we can get some quick confidence intervals for our coefficients:\n\nconfint(mod1)\n\n            2.5 % 97.5 %\n(Intercept) 27.17 28.933\nroleB       -3.54 -0.949\nroleC       -7.26 -4.638\n\n\nIt looks like roles do differ in their workplace pride. Specifically, compared to people in role A, people who are in roles B and C on average report less pride in the workplace.\n\n\n\n\n\nQuestion 3\n\n\nIs it something about the roles that make people report differences in workplace-pride, or is it possibly just that people who are newer to the company tend to feel more pride than those who have been there for a while (they’re all jaded), and the people in role A tend to be much newer to the company (making it look like the role A results in taking more pride). In other words, if we were to compare people in role A vs role B vs role C but hold constant their employment_length, we might see something different?\nFit another model to find out.\nTo help with interpreting the model, make a plot that shows all of the relevant variables that are in the model in one way or another.\n\n\n\n\n\n\nHints\n\n\n\n\n\nSo we want to adjust for how long people have been part of the company..\nRemember - if we want to estimate the effect of x on y while adjusting for z, we can do lm(y ~ z + x).\nFor the plot - put something on the x, something on the y, and colour it by the other variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\nmod2 &lt;- lm(wp ~ employment_length + role, data = jsup)\n\ntidy(mod2)\n\n# A tibble: 4 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         36.1      0.709     50.9   6.90e-147\n2 employment_length   -0.834    0.0637   -13.1   4.32e- 31\n3 roleB                0.510    0.563      0.906 3.65e-  1\n4 roleC               -0.704    0.663     -1.06  2.89e-  1\n\n\nNote that, after adjusting for employment length, there are no significant differences in wp between roles B or C compared to A.\nIf we plot the data to show all these variables together, we can kind of see why! Given the pattern of wp against employment_length, the wp for different roles are pretty much where we would expect them to be if role doesn’t make any difference (i.e., if role doesn’t shift your wp up or down).\n\nggplot(jsup, aes(x=employment_length,y=wp,col=role))+\n  geom_point(size=3,alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nDo roles differ in their workplace-pride, when adjusting for time in the company?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis may feel like a repeat of the previous question, but note that this is not a question about specific group differences. It is about whether, overall, the role groups differ. So it’s wanting to test the joint effect of the two additional parameters we’ve just added to our model. (hint hint model comparison!)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nmod2a &lt;- lm(wp ~ employment_length, data = jsup)\nmod2 &lt;- lm(wp ~ employment_length + role, data = jsup)\n\nanova(mod2a, mod2)\n\nAnalysis of Variance Table\n\nModel 1: wp ~ employment_length\nModel 2: wp ~ employment_length + role\n  Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n1    293 4091                         \n2    291 4029  2      61.9 2.23   0.11\n\n\nThis is no surprise given the previous question, we just now have a single test to report if we wanted to - after accounting for employment length, role does not explain a significant amount of variance in workplace pride.\n\n\n\n\nQuestion 5\n\n\nLet’s take a step back and remember what data we actually have. We’ve got 295 people in our dataset, from 16 departments.\nDepartments may well differ in the general amount of workplace-pride people report. People love to say that they work in the “National Crime Agency”, but other departments might not elicit such pride (*cough* HM Revenue & Customs *cough*). We need to be careful not to mistake department differences as something else (like differences due to the job role).\nMake a couple of plots to look at:\n\nhow many of each role we have from each department\nhow departments differ in their employees’ pride in their workplace\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. \n\nggplot(jsup, aes(x = role)) + \n  geom_bar()+\n  facet_wrap(~dept)\n\n\n\n\n\n\n\n\nIn this case, it looks like most of the departments have similar numbers of each role, apart from the UKSA (“UK Statistics Authority”), where we’ve got loads more of role A, and very few role C..\nNote also that in the plot below, the UKSA is, on average, full of employees who take a lot of pride in their work. Is this due to the high proportion of people in role A? or is the effect of role we’re seeing more due to differences in departments?\n\nggplot(jsup, aes(x = dept, y = wp)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = label_wrap_gen(35)) + \n  coord_flip()\n\n\n\n\n\n\n\n\nEven if we had perfectly equal numbers of roles in each department, we’re also adjusting for other things such as employment_length, and the extent to which this differs by department can have trickle-on effects on our coefficient of interest (the role coefficients).\n\n\n\n\nQuestion 6\n\n\nAdjusting for both length of employment and department, are there differences in ‘workplace-pride’ between the different roles?\nCan you make a plot of all four of the variables involved in our model?\n\n\n\n\n\n\nHints\n\n\n\n\n\nMaking the plot might take some thinking. We’ve now added dept into the mix, so a nice way might be to use facet_wrap() to make the same plot as the one we did previously, but for each department.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\nmod3 &lt;- lm(wp ~ employment_length + dept + role, data = jsup)\ntidy(mod3)\n\n# A tibble: 19 × 5\n   term              estimate std.error statistic   p.value\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)        36.4       0.631    57.6    7.17e-156\n 2 employment_length  -0.882     0.0344  -25.7    4.71e- 75\n 3 deptCMA            -3.80      0.649    -5.85   1.39e-  8\n 4 deptCPS            -0.217     0.730    -0.298  7.66e-  1\n 5 deptFSA             4.74      0.625     7.60   4.71e- 13\n 6 deptGLD             0.0582    0.682     0.0853 9.32e-  1\n 7 deptHMRC           -3.79      0.692    -5.47   1.02e-  7\n 8 deptNCA            -3.85      0.655    -5.88   1.18e-  8\n 9 deptNS&I           -0.574     0.654    -0.878  3.81e-  1\n10 deptOFGEM          -0.648     0.705    -0.919  3.59e-  1\n11 deptOFQUAL         -4.94      1.01     -4.89   1.71e-  6\n12 deptOFSTED         -5.88      0.683    -8.61   5.52e- 16\n13 deptOFWAT          -1.21      0.692    -1.75   8.17e-  2\n14 deptORR            -2.85      0.681    -4.18   3.98e-  5\n15 deptSFO            -1.36      0.672    -2.02   4.47e-  2\n16 deptUKSA            4.28      0.576     7.43   1.32e- 12\n17 deptUKSC           -2.31      0.732    -3.16   1.77e-  3\n18 roleB               1.42      0.303     4.68   4.47e-  6\n19 roleC               1.31      0.366     3.59   3.92e-  4\n\n\nIn a way, adding predictors to our model is kind of like splitting up our plots by that predictor to see the patterns. This becomes more and more difficult (/impossible) as we get more variables, but right now we can split the data into all the constituent parts.\n\nggplot(jsup, aes(x = employment_length, y = wp, col = role)) +\n  geom_point(size=3,alpha=.4)+\n  facet_wrap(~dept)\n\n\n\n\n\n\n\n\nThe association between wp and employment_length is clear in all these little sub-plots - there’s a downward trend. The department differences can be seen too: UKSA is generally a bit higher, HMRC and UKSC a bit lower, and so on. By default, the model captures these coefficients as ‘differences from the reference group’, so all these coefficients are in relation to the “ACE” department.\nSeeing the role differences is a bit harder in this plot, but think about what you would expect to see if there were no differences in roles (i.e. imagine if they were all in role A). Take for instance the FSA department, where this is easiest to see - for the people who are in role C, for people of their employment length we would expect their wp to be lower if they were in role A. Likewise for those in role B. Across all these departments, the people in role B and C (green and blue dots respectively) are a bit higher than we would expect. This is what the model coefficients tell us!\n\n\n\n\nQuestion 7\n\n\nNow we’re starting to acknowledge the grouped structure of our data - these people in our dataset are related to one another in that some belong to dept 1, some dept 2, and so on..\nLet’s try to describe our sample in a bit more detail.\n\nhow many participants do we have, and from how many departments?\nhow many participants are there, on average, from each department? what is the minimum and maximum?\nwhat is the average employment length for our participants?\nhow many departments are ‘virtual departments’ vs office-based?\n\nwhat is the overall average reported workplace-pride?\nhow much variation in workplace-pride is due to differences between departments?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe first lot of these questions can be answered using things like count(), summary(), table(), mean(), min() etc. See 1: Clustered Data #determining-sample-sizes\nFor the last one, we can use the ICC! See 1: Clustered Data #icc\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. How many respondents do we have, and from how many departments?\n\nnrow(jsup)\n\n[1] 295\n\nlength(table(jsup$dept))\n\n[1] 16\n\n\nHow many respondents are there, on average, from each dept? What is the minimum and maximum number of people in any one department?\n\njsup |&gt;\n  count(dept) |&gt; \n  summarise(min=min(n),\n            max=max(n),\n            median=median(n)\n  )\n\n# A tibble: 1 × 3\n    min   max median\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     5    45     17\n\n\nWhat is the average employment length of respondents?\n\nmean(jsup$employment_length)\n\n[1] 12.6\n\n\nHow many departments are virtual vs office based? This requires a bit more than just table(jsup$virtual), because we are describing a variable at the department level.\n\njsup |&gt; \n  group_by(virtual) |&gt;\n  summarise(\n    ndept = n_distinct(dept)\n  )\n\n# A tibble: 2 × 2\n  virtual ndept\n    &lt;dbl&gt; &lt;int&gt;\n1       0    11\n2       1     5\n\n\nWhat is the overall average ‘workplace-pride’? What is the standard deviation?\n\nmean(jsup$wp)\n\n[1] 25.5\n\nsd(jsup$wp)\n\n[1] 5.27\n\n\nFinally, how much variation in workplace-pride is attributable to department-level differences?\n\nICC::ICCbare(x = dept, y = wp, data = jsup)\n\n[1] 0.439\n\n\n\n\n\n\nQuestion 8\n\n\nWhat if we would like to know whether, when adjusting for differences due to employment length and roles, workplace-pride differs between people working in virtual-departments compared to office-based ones?\nCan you add this to the model? What happens?\n\n\n\n\n\nSolution\n\n\n\nSolution 8. Let’s add the virtual predictor to our model. Note that we don’t actually get a coefficient here - it is giving us an NA!\n\nmod4 &lt;- lm(wp ~ employment_length + dept + role + virtual, data = jsup)\n\nsummary(mod4)\n\n\nCall:\nlm(formula = wp ~ employment_length + dept + role + virtual, \n    data = jsup)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.690 -1.404 -0.027  1.178  5.054 \n\nCoefficients: (1 not defined because of singularities)\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        36.3522     0.6310   57.61  &lt; 2e-16 ***\nemployment_length  -0.8817     0.0344  -25.66  &lt; 2e-16 ***\ndeptCMA            -3.7969     0.6490   -5.85  1.4e-08 ***\ndeptCPS            -0.2173     0.7304   -0.30  0.76627    \ndeptFSA             4.7448     0.6245    7.60  4.7e-13 ***\ndeptGLD             0.0582     0.6822    0.09  0.93212    \ndeptHMRC           -3.7859     0.6924   -5.47  1.0e-07 ***\ndeptNCA            -3.8503     0.6549   -5.88  1.2e-08 ***\ndeptNS&I           -0.5737     0.6537   -0.88  0.38095    \ndeptOFGEM          -0.6479     0.7050   -0.92  0.35885    \ndeptOFQUAL         -4.9413     1.0104   -4.89  1.7e-06 ***\ndeptOFSTED         -5.8846     0.6831   -8.61  5.5e-16 ***\ndeptOFWAT          -1.2087     0.6917   -1.75  0.08169 .  \ndeptORR            -2.8452     0.6813   -4.18  4.0e-05 ***\ndeptSFO            -1.3550     0.6719   -2.02  0.04469 *  \ndeptUKSA            4.2820     0.5759    7.43  1.3e-12 ***\ndeptUKSC           -2.3131     0.7325   -3.16  0.00177 ** \nroleB               1.4179     0.3029    4.68  4.5e-06 ***\nroleC               1.3148     0.3663    3.59  0.00039 ***\nvirtual                 NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.98 on 276 degrees of freedom\nMultiple R-squared:  0.867, Adjusted R-squared:  0.859 \nF-statistic:  100 on 18 and 276 DF,  p-value: &lt;2e-16\n\n\nSo what is happening? If we think about it, if we separate out “differences due to departments” then there is nothing left to compare between departments that are virtual vs office based. Adding the between-department predictor of virtual doesn’t explain anything more - the residual sums of squares doesn’t decrease at all:\n\nanova(\n  lm(wp ~ employment_length + dept + role, data = jsup),\n  lm(wp ~ employment_length + dept + role + virtual, data = jsup)\n)\n\nAnalysis of Variance Table\n\nModel 1: wp ~ employment_length + dept + role\nModel 2: wp ~ employment_length + dept + role + virtual\n  Res.Df  RSS Df Sum of Sq F Pr(&gt;F)\n1    276 1084                      \n2    276 1084  0         0         \n\n\nAnother way of thinking about this: knowing the average workplace-pride for the department that someone is in tells me what to expect about that person’s workplace pride. But once I know their department’s average workplace-pride, knowing whether it is ‘virtual’ or ‘office-based’ doesn’t tell me anything new, for the very fact that the virtual/office-based distinction comes from comparing different departments.\nBut we’re not really interested in these departments specifically! What would be nice would be if we can look at the relevant effects of interest (things like role and virtual), but then just think of the department differences as just some sort of random variation. So we want to think of departments in a similar way to how we think of our individual employees - they vary randomly around what we expect - only they’re at a different level of observation.",
    "crumbs": [
      "W1 Exercises: Regression Refresher"
    ]
  },
  {
    "objectID": "r08_pca.html",
    "href": "r08_pca.html",
    "title": "R8: Principal Component Analysis (PCA)",
    "section": "",
    "text": "The goal of principal component analysis (PCA) is to find a smaller number of uncorrelated variables which are linear combinations of the original ( many ) variables and explain most of the variation in the data.\n\nTake a moment to think about the various constructs that you are often interested in as a researcher. This might be anything from personality traits, to language proficiency, social identity, anxiety etc. How we measure such constructs is a very important consideration for research. The things we’re interested in are very rarely the things we are directly measuring.\nConsider how we might assess levels of anxiety or depression. Can we ever directly measure anxiety?1 More often than not, we measure these things using a set of different measures which ‘look at’ the underlying construct from a different angle. In psychology, this is often questionnaire based methods, with a set of questions each of which might ask about “anxiety” from a slightly different angle. Twenty questions all measuring different aspects of anxiety are (we hope) going to correlate with one another if they are capturing some commonality (the construct of “anxiety”).\nBut they introduce a problem for us, which is how to deal with 20 variables that represent (in broad terms) the same thing. How can we assess “effects on anxiety”, rather than “effects on anxiety q1 + effects on anxiety q2 + …”, etc.\nIn addition, not all constructs might have a single dimension - we often talk about “narcissm”, but this could arguably be comprised of 3 dimensions of “grandiosity”, “vulnerability” and “antagonism”.\nThis leads us to the idea of reducing the dimensionality of our data. Can we capture a reasonable amount of the information from our 20 questions in a smaller number of variables? How many, and what would they represent?",
    "crumbs": [
      "Week 8",
      "R8: Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "r08_pca.html#footnotes",
    "href": "r08_pca.html#footnotes",
    "title": "R8: Principal Component Analysis (PCA)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEven if we cut open someone’s brain, it’s unclear what we would be looking for in order to ‘measure’ it. It is unclear whether anxiety even exists as a physical thing, or rather if it is simply the overarching concept we apply to a set of behaviours and feelings.↩︎\nThe variables are all standardised, meaning they each have a variance of 1. \\(8 \\times 1 = 8\\) so we can think of the total variance as 8!↩︎",
    "crumbs": [
      "Week 8",
      "R8: Principal Component Analysis (PCA)"
    ]
  },
  {
    "objectID": "r09_efa.html",
    "href": "r09_efa.html",
    "title": "R9: Exploratory Factor Analysis (EFA)",
    "section": "",
    "text": "Both Principal Component Analysis (PCA) and Factor Analysis (FA) are methods of getting at the dimensions that capture “variability” in the data.\nIt’s important to think about what we mean by “variability” here - it is variability across a whole set of variables. So if we start with just 3 variables, we can think of all the data visualised in 3-dimensional space (as in Figure 1). We can see in Figure 1 that the observations vary in all variables. But they vary in a specific way - observations that are high on y1 tend to be high on y2, and also high on y3. Observations that are lower, tend to be lower on all of them.\nThe methods we are looking at now are concerned with capturing this variability in fewer “dimensions”. I.e., without having to refer to all variables y1, y2, and y3, couldn’t we instead simply say that an observation is “high/low on dimension X”.\n\n\n\n\n\n\n\n\nFigure 1: 3 measured variables\n\n\n\n\nWhere Principal Component Analysis (PCA) aims to summarise a set of measured variables into a set of orthogonal (uncorrelated) dimensions, Factor Analysis (FA) is an explanatory tool, in that we are assuming that that the relationships between a set of measured variables can be explained by a number of underlying latent factors.\n\n\n\n\n\n\n\n\n\n\nPrincipal Component Analysis\nFactor Analysis\n\n\n\n\nwhat it does\ntake a set of correlated variables, reduce to orthogonal dimensions that capture variability\ntake a set of correlated variables, ask what set of dimensions (not necessarily orthogonal!) explain variability\n\n\nwhat the dimensions are\ndimensions are referred to as “components” - they are simply “composites” and do not necessarily exist\ndimensions are referred to as “factors”. They are assumed to be underlying latent variables that are the cause of why people have different scores on the observed variables\n\n\nexample\nSocioeconomic status (SES) might be a composite measure of family income, parental education, etc. If my SES increases, that doesn’t mean my parents suddenly get more education (it’s the other way around)\nAnxiety (the unmeasurable construct) is a latent factor. My underlying anxiety is what causes me to respond “strongly agree” to “I am often on edge”\n\n\n\n\nOne way to think about this is to draw the two approaches in diagrammatic form, mapping the relationships between variables. There are conventions for these sort of diagrams:\n\nsquares = variables we observe\ncircles = variables we don’t observe\nsingle-headed arrows = regression paths (pointed at outcome)\ndouble-headed arrows = correlation\n\nIn the diagrams below, note how the directions of the arrows are different between PCA and FA - in PCA, each component \\(C_i\\) is the weighted combination of the observed variables \\(y_1, ...,y_n\\), whereas in FA, each measured variable \\(y_i\\) is seen as generated by some latent factor(s) \\(F_i\\) plus some unexplained variance \\(u_i\\).\n\n\n\n\n\n\nPCA as a diagram\n\n\n\n\n\nNote that the idea of a ‘composite’ requires us to use a special shape (the hexagon), but many people would just use a square.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\noptional: PCA in full\n\n\n\n\n\nPrincipal components sequentially capture the orthogonal (i.e., perpendicular) dimensions of the dataset with the most variance. The data reduction comes when we retain fewer components than we have dimensions in our original data. So if we were being pedantic, the diagram for PCA would look something like the diagram below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEFA as a diagram\n\n\n\n\n\nExploratory Factor Analysis as a diagram has arrows going from the factors to the observed variables. Unlike PCA, we also have ‘uniqueness’ factors for each variable, representing the various stray causes that are specific to each variable. Sometimes, these uniqueness are represented by an arrow only, but they are technically themselves latent variables, and so can be drawn as circles.\n\n\n\n\n\n\n\n\n\nIt might help to read the \\(\\lambda\\)s as beta-weights (\\(b\\), or \\(\\beta\\)), because that’s all they really are. The equation \\(y_i = \\lambda_{1i} F_1 + \\lambda_{2i} F_2 + u_i\\) is just our way of saying that the variable \\(y_i\\) is the manifestation of some amount (\\(\\lambda_{1i}\\)) of an underlying factor \\(F_1\\), some amount (\\(\\lambda_{2i}\\)) of some other underlying factor \\(F_2\\), and some error (\\(u_i\\)). It’s just a set of regressions!",
    "crumbs": [
      "Week 9",
      "R9: Exploratory Factor Analysis (EFA)"
    ]
  },
  {
    "objectID": "r09_efa.html#footnotes",
    "href": "r09_efa.html#footnotes",
    "title": "R9: Exploratory Factor Analysis (EFA)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n(It’s a bit like the optimiser issue in the multi-level model block)↩︎\nadditional point here is that even if these two factors are not correlated, then an oblique rotation could estimate that too.↩︎",
    "crumbs": [
      "Week 9",
      "R9: Exploratory Factor Analysis (EFA)"
    ]
  },
  {
    "objectID": "03ex.html",
    "href": "03ex.html",
    "title": "W3 Exercises: Nested and Crossed Structures",
    "section": "",
    "text": "Data: gadeduc.csv\nThis is synthetic data from a randomised controlled trial, in which 30 therapists randomly assigned patients (each therapist saw between 2 and 28 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nThe data are available at https://uoepsy.github.io/data/lmm_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\nTable 1: Data Dictionary: lmm_gadeduc.csv\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npatient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n\n\nvisit_0\nScore on the GAD7 at baseline\n\n\nvisit_1\nGAD7 at 1 month assessment\n\n\nvisit_2\nGAD7 at 2 month assessment\n\n\nvisit_3\nGAD7 at 3 month assessment\n\n\nvisit_4\nGAD7 at 4 month assessment\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nUh-oh… these data aren’t in the same shape as the other datasets we’ve been giving you..\nCan you get it into a format that is ready for modelling?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nIt’s wide, and we want it long.\n\nOnce it’s long. “visit_0”, “visit_1”,.. needs to become the numbers 0, 1, …\nOne variable (patient) contains lots of information that we want to separate out. There’s a handy function in the tidyverse called separate(), check out the help docs!\n\n\n\n\n\n\n\n\n\n1 - reshaping\n\n\n\nSolution 1. Here’s the data. We have one row per patient, but we have multiple observations for each patient across the columns..\n\ngeduc = read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\nhead(geduc)\n\n# A tibble: 6 × 6\n  patient      visit_0 visit_1 visit_2 visit_3 visit_4\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 VC_Control_1      24      24      26      29      28\n2 VC_Control_2      24      26      28      29      30\n3 VC_Control_3      25      29      27      29      30\n4 VC_Control_4      24      25      25      26      26\n5 VC_Control_5      28      28      27      29      28\n6 VC_Control_6      26      28      25      27      28\n\n\nWe can make it long by taking the all the columns from visit_0 to visit_4 and shoving their values into one variable, and keeping the name of the column they come from as another variable:\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\")\n\n# A tibble: 2,410 × 3\n   patient      visit     GAD\n   &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt;\n 1 VC_Control_1 visit_0    24\n 2 VC_Control_1 visit_1    24\n 3 VC_Control_1 visit_2    26\n 4 VC_Control_1 visit_3    29\n 5 VC_Control_1 visit_4    28\n 6 VC_Control_2 visit_0    24\n 7 VC_Control_2 visit_1    26\n 8 VC_Control_2 visit_2    28\n 9 VC_Control_2 visit_3    29\n10 VC_Control_2 visit_4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\n\n2 - time is numeric\n\n\n\nSolution 2. Now we know how to get our data long, we need to sort out our time variable (visit) and make it into numbers.\nWe can replace all occurrences of the string \"visit_\" with nothingness \"\", and then convert them to numeric.\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) \n\n# A tibble: 2,410 × 3\n   patient      visit   GAD\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 VC_Control_1     0    24\n 2 VC_Control_1     1    24\n 3 VC_Control_1     2    26\n 4 VC_Control_1     3    29\n 5 VC_Control_1     4    28\n 6 VC_Control_2     0    24\n 7 VC_Control_2     1    26\n 8 VC_Control_2     2    28\n 9 VC_Control_2     3    29\n10 VC_Control_2     4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\n\n3 - splitting up the patient variable\n\n\n\nSolution 3. Finally, we need to sort out the patient variable. It contains 3 bits of information that we will want to have separated out. It has the therapist (their initials), then the group (treatment or control), and then the patient number. These are all separated by an underscore “_“.\nThe separate() function takes a column and separates it into several things (as many things as we give it), splitting them by some user defined separator such as an underscore:\n\ngeduc_long &lt;- geduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |&gt;\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n\nAnd we’re ready to go!\n\ngeduc_long\n\n# A tibble: 2,410 × 5\n   therapist group   patient visit   GAD\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 VC        Control 1           0    24\n 2 VC        Control 1           1    24\n 3 VC        Control 1           2    26\n 4 VC        Control 1           3    29\n 5 VC        Control 1           4    28\n 6 VC        Control 2           0    24\n 7 VC        Control 2           1    26\n 8 VC        Control 2           2    28\n 9 VC        Control 2           3    29\n10 VC        Control 2           4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\nQuestion 2\n\n\nVisualise the data. Does it look like the treatment had an effect?\nDoes it look like it worked for every therapist?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nremember, stat_summary() is very useful for aggregating data inside a plot.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. Here’s the overall picture. The average score on the GAD7 at each visit gets more and more different between the two groups. The treatment looks effective..\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nLet’s split this up by therapist, so we can see the averages across each therapist’s set of patients.\nThere’s clear variability between therapists in how well the treatment worked. For instance, the therapists EU and OD don’t seem to have much difference between their groups of patients.\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\") +\n  facet_wrap(~therapist)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFit a model to test if the psychoeducational treatment is associated with greater improvement in anxiety over time.\n\n\n\n\n\n1 - fixed effects\n\n\n\nSolution 5. We want to know if how anxiety (GAD) changes over time (visit) is different between treatment and control (group).\nHopefully this should hopefully come as no surprise1 - it’s an interaction!\n\nlmer(GAD ~ visit * group + ...\n       ...\n     data = geduc_long)\n\n\n\n\n\n\n2 - grouping structure\n\n\n\nSolution 6. We have multiple observations for each of the 482 patients, and those patients are nested within 30 therapists.\nNote that in our data, the patient variable does not uniquely specify the individual patients. i.e. patient “1” from therapist “AO” is a different person from patient “1” from therapist “BJ”. To correctly group the observations into different patients (and not ‘patient numbers’), we need to have therapist:patient.\nSo we capture therapist-level differences in ( ... | therapist) and the patients-within-therapist-level differences in ( ... | therapist:patient):\n\nlmer(GAD ~ visit * group + ...\n       ( ... | therapist) + \n       ( ... | therapist:patient),\n     data = geduc_long)\n\n\n\n\n\n\n3 - random effects\n\n\n\nSolution 7. Note that each patient can change differently in their anxiety levels over time - i.e. the slope of visit could vary by participant.\nLikewise, some therapists could have patients who change differently from patients from another therapist, so visit|therapist can be included.\nEach patient is in one of the two groups - they’re either treatment or control. So we can’t say that “differences in anxiety due to treatment varies between patients”, because for any one patient the “difference in anxiety due to treatment” is not defined in our study design.\nHowever, therapists see multiple different patients, some of which are in the treatment group, and some of which are in the control group. So the treatment effect could be different for different therapists!\n\nmod1 &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|therapist:patient),\n             geduc_long)\n\n\n\n\n\nQuestion 4\n\n\nFor each of the models below, what is wrong with the random effect structure?\n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\nThe patient variable doesn’t capture the different patients within therapists, so this actually fits crossed random effects and treats all data where patient==1 as from the same group (even if this includes several different patients’ worth of data from different therapists!)\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\nUsing the / here means we have the same random slopes fitted for therapists and for patients-within-therapists. but the effect of group can’t vary by patient, so this doesn’t work. hence why we need to split them up into (...|therapist)+(...|therapist:patient).\n\n\n\n\nQuestion 5\n\n\nLet’s suppose that I don’t want the psychoeducation treatment, I just want the standard therapy sessions that the ‘Control’ group received. Which therapist should I go to?\n\n\n\n\n\n\nHints\n\n\n\n\n\ndotplot.ranef.mer() might help here!\nYou can read about ranef in Chapter 2 #making-model-predictions.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. It would be best to go to one of the therapists SZ, YS, or IT…\nWhy? These therapists all have the most negative slope of visit:\n\ndotplot.ranef.mer(ranef(mod1))$therapist\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nRecreate this plot.\nThe faint lines represent the model estimated lines for each patient. The points and ranges represent our fixed effect estimates and their uncertainty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nyou can get the patient-specific lines using augment() from the broom.mixed package, and the fixed effects estimates using the effects package.\nremember that the “patient” column doesn’t group observations into unique patients.\nremember you can pull multiple datasets into ggplot:\n\n\nggplot(data = dataset1, aes(x=x,y=y)) + \n  geom_point() + # points from dataset1\n  geom_line(data = dataset2) # lines from dataset2\n\n\nsee more in Chapter 2 #visualising-models\n\n\n\n\n\n\n\n\n\n1 - the relevant parts\n\n\n\nSolution 10. The effects package will give us the fixed effect estimates:\n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\nWe want to get the fitted values for each patient. We can get fitted values using augment(). But the patient variable doesn’t capture the unique patients, it just captures their numbers (which aren’t unique to each therapist).\nSo we can create a new column called upatient which pastes together the therapists initials and the patient numbers\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  )\n\n# A tibble: 2,410 × 17\n     GAD visit group   therapist patient upatient .fitted .resid  .hat .cooksd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1    24     0 Control VC        1       VC1         24.2 -0.198 0.454 0.0210 \n 2    24     1 Control VC        1       VC1         25.3 -1.28  0.239 0.239  \n 3    26     2 Control VC        1       VC1         26.4 -0.360 0.186 0.0128 \n 4    29     3 Control VC        1       VC1         27.4  1.56  0.294 0.508  \n 5    28     4 Control VC        1       VC1         28.5 -0.522 0.563 0.284  \n 6    24     0 Control VC        2       VC2         24.8 -0.843 0.454 0.383  \n 7    26     1 Control VC        2       VC2         26.2 -0.171 0.239 0.00426\n 8    28     2 Control VC        2       VC2         27.5  0.502 0.186 0.0250 \n 9    29     3 Control VC        2       VC2         28.8  0.174 0.294 0.00633\n10    30     4 Control VC        2       VC2         30.2 -0.153 0.563 0.0246 \n# ℹ 2,400 more rows\n# ℹ 7 more variables: .fixed &lt;dbl&gt;, .mu &lt;dbl&gt;, .offset &lt;dbl&gt;, .sqrtXwt &lt;dbl&gt;,\n#   .sqrtrwt &lt;dbl&gt;, .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;\n\n\n\n\n\n\n\n2 - constructing the plot\n\n\n\nSolution 11. \n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  ) |&gt;\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")",
    "crumbs": [
      "W3 Exercises: Nested and Crossed Structures"
    ]
  },
  {
    "objectID": "03ex.html#footnotes",
    "href": "03ex.html#footnotes",
    "title": "W3 Exercises: Nested and Crossed Structures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif it does, head back to where we learned about interactions in the single level regressions lm(). It’s just the same here.↩︎",
    "crumbs": [
      "W3 Exercises: Nested and Crossed Structures"
    ]
  }
]