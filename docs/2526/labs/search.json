[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "",
    "text": "This site contains weekly exercises for the Data Analysis for Psychology in R 3 (DAPR3) course.\nAt the end of each week, solutions (where these are not already available) will be made visible directly beneath each question."
  },
  {
    "objectID": "index.html#about-dapr3",
    "href": "index.html#about-dapr3",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "About DAPR3",
    "text": "About DAPR3\nData Analysis for Psychology in R 3 (DAPR3) is a course undertaken by 3rd year students in Psychology. DAPR3 builds on the content of DAPR2 and covers more advanced methods that are invaluable for analysing many types of psychological study, preparing students for their dissertations. The course offers students a solid foundation in multilevel modeling, expanding the linear model to analyze “hierarchical data”. Such data often involves observations clustered within higher-level groups, such as trials within participants, timepoints within individuals, or children within schools. In the second half of the course, we delve into data reduction techniques. These methods allow us to effectively summarize multiple correlated variables, either through weighted composites or by positing underlying latent factors. Additionally, students will gain insights into crucial concepts, including measurement error, validity, reliability, and replicability. These concepts are especially essential for researchers in psychology, where surveys or questionnaires are used to conduct studies of underlying constructs that cannot be directly measured."
  },
  {
    "objectID": "index.html#installupdate-r-rstudio",
    "href": "index.html#installupdate-r-rstudio",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "Install/Update R & RStudio",
    "text": "Install/Update R & RStudio\nMake sure you have installed both R and RStudio on your computer. You may have done this previously for DAPR2, in which case it is probably worth doing some updates.\nPlease make sure to read and follow the instructions below slowly and carefully!!\n\nFor instructions on how to install R and RStudio, click here\nFor instructions on how to update R and RStudio, click here"
  },
  {
    "objectID": "index.html#update-packages",
    "href": "index.html#update-packages",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "Update Packages",
    "text": "Update Packages\nIt’s worth keeping packages up to date, so it might be worth updating all your packages.\nRunning this code will update all your packages. Just put it into the console (bottom left bit of RStudio):\n\noptions(pkgType = \"binary\")\nupdate.packages(ask = FALSE)"
  },
  {
    "objectID": "index.html#new-packages",
    "href": "index.html#new-packages",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "New packages!",
    "text": "New packages!\nNow it is probably worth installing a few of the packages that we will be using in DAPR3. There are a few that we will need. For each one, check whether you have it already installed, because there’s not much point wasting time re-installing something you already have!\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models\npsych: for factor analysis\nlavaan: for latent variable models"
  },
  {
    "objectID": "07ex.html",
    "href": "07ex.html",
    "title": "W7 Exercises: Scale Scores & PCA",
    "section": "",
    "text": "Gambler’s fallacy\n\nDataset: gamblers.csv\nA researcher is interested in assessing if people who gamble will tend to lose more if they are more ‘impulsive’, and whether this might depend on whether they are gambling online or in a casino.\nThey recruited 482 participants (248 in a casino, and 234 on an online gambling site). Each participant filled out a 6 question measure of “impulsivity”, and then their total net gains (or losses) for the day were recorded (in £). All people were only playing the game BlackJack.\nOur research question: does greater impulsivity lead to bigger losses when comparing online gamblers to casino gamblers?\nDataset: The data can be found at https://uoepsy.github.io/data/gamblers.csv\n\n\n\n\nTable 1: gamblers.csv Data Dictionary\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nonline\nwhether the person was gambling in a casino or online\n\n\nimp_1\nI often act on the spur of the moment without thinking.\n\n\nimp_2\nI find it hard to resist temptations.\n\n\nimp_3\nI make decisions quickly, even when they have serious consequences.\n\n\nimp_4\nI find it hard to stay focused on tasks that take a long time to finish.\n\n\nimp_5\nI prefer safe activities rather than risky things just for fun.\n\n\nimp_6\nI am usually patient and can wait for what I want.\n\n\ngain\nnet losses or gains upon leaving/logging out\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and have a look at it.\n\nWhat does each row represent?\n\nWhat measurement(s) show us a person’s impulsivity?\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. Here’s the data:\n\ngdat &lt;- read_csv(\"https://uoepsy.github.io/data/gamblers.csv\")\nhead(gdat)\n\n# A tibble: 6 × 8\n  online imp_1                      imp_2          imp_3 imp_4 imp_5 imp_6  gain\n  &lt;chr&gt;  &lt;chr&gt;                      &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 casino Disagree                   Neither Disag… Disa… Neit… Neit… Neit…     0\n2 casino Disagree                   Disagree       Disa… Neit… Agree Agree   -12\n3 casino Neither Disagree nor Agree Neither Disag… Neit… Neit… Neit… Disa…   -35\n4 casino Neither Disagree nor Agree Agree          Neit… Agree Agree Neit…   -20\n5 online Disagree                   Neither Disag… Disa… Neit… Neit… Neit…   -14\n6 casino Neither Disagree nor Agree Disagree       Disa… Stro… Neit… Neit…   -28\n\n\nEach row is a participant, for each person there are 6 columns all measuring the construct of “impulsivity”.\nAnd for each of those columns, there’s a whole load of words in there!\n\n\n\n\nQuestion 2\n\n\nFirst things first, our questionnaire software has given us the responses all in the descriptors used for each point of the likert scale, which is a bit annoying.\nConvert them all to numbers, which we can then work with.\n\n\n\nWhat we have\nWhat we want\n\n\n\n\nStrongly Agree\n5\n\n\nAgree\n4\n\n\nAgree\n4\n\n\nStrongly Disagree\n1\n\n\nNeither Disagree nor Agree\n3\n\n\nAgree\n4\n\n\nDisagree\n2\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee 1: Data Wrangling for Questionnaires#variable-recoding.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. We want to turn all of the variables from imp_1 to imp_6 into numbers.\nTo do it with one variable:\n\ngdat |&gt; mutate(\n  imp_1 = case_match(imp_1,\n                     \"Strongly Disagree\" ~ 1,\n                     \"Disagree\" ~ 2,\n                     \"Neither Disagree nor Agree\" ~ 3,\n                     \"Agree\" ~ 4,\n                     \"Strongly Agree\" ~ 5\n  )\n)\n\nAnd we can do it to all at once with across().\n\ngdat &lt;- gdat |&gt; mutate(\n  across(c(imp_1:imp_6),\n         ~case_match(.,\n                     \"Strongly Disagree\" ~ 1,\n                     \"Disagree\" ~ 2,\n                     \"Neither Disagree nor Agree\" ~ 3,\n                     \"Agree\" ~ 4,\n                     \"Strongly Agree\" ~ 5\n         ))\n  )\n\nhead(gdat)\n\n# A tibble: 6 × 8\n  online imp_1 imp_2 imp_3 imp_4 imp_5 imp_6  gain\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 casino     2     3     2     3     3     3     0\n2 casino     2     2     2     3     4     4   -12\n3 casino     3     3     3     3     3     2   -35\n4 casino     3     4     3     4     4     3   -20\n5 online     2     3     2     3     3     3   -14\n6 casino     3     2     2     1     3     3   -28\n\n\n\n\n\n\nQuestion 3\n\n\nJust looking at the impulsivity questions, create a correlation matrix of 6 variables.\nWhat do you notice? Does it make sense given the wording of the questions?\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\ncor(gdat[,2:7])\n\n       imp_1  imp_2  imp_3  imp_4  imp_5  imp_6\nimp_1  1.000  0.470  0.555  0.044 -0.464 -0.453\nimp_2  0.470  1.000  0.414  0.321 -0.204 -0.305\nimp_3  0.555  0.414  1.000  0.102 -0.406 -0.461\nimp_4  0.044  0.321  0.102  1.000 -0.244 -0.334\nimp_5 -0.464 -0.204 -0.406 -0.244  1.000  0.390\nimp_6 -0.453 -0.305 -0.461 -0.334  0.390  1.000\n\n\nCorrelations are all positive except for those with Q5 and Q6. Q5 and Q6 are positively related, but they are negatively related to the other questions.\nIf you’re like Elizabeth and you have a hard time making sense of a matrix full of numbers, then the function heatmap() might be your friend. It represents the correlation matrix as colours. Cells representing strong positive correlations appear darker, and cells representing strong negative correlations appear lighter.\n\ncor(gdat[,2:7]) |&gt;\n  heatmap()\n\n\n\n\n\n\n\n\nThere’s a dark diagonal line down the middle, which represents the perfect correlation between an item and itself. The cells at the intersections of Q5 and Q6 are fairly dark, and the cells at the intersections of Q1–Q4 are also fairly dark (the positive correlations). Where Q5 and Q6 meet Q1–Q4, the cells are fairly light (the negative correlations). In addition to the bracketing structure at the plot margins (in technical terms a “dendrogram”), this visual approach to a correlation matrix shows that Q5 and Q6 tend to pattern together in one way, and that the other four Qs tend to pattern together in another way.\nThis makes sense given the way the questions are worded - if people are impulsive, they will be more likely to disagree to Q5 and Q6, but agree with the others:\n\nqitems\n\n[1] \"I often act on the spur of the moment without thinking.\"                 \n[2] \"I find it hard to resist temptations.\"                                   \n[3] \"I make decisions quickly, even when they have serious consequences.\"     \n[4] \"I find it hard to stay focused on tasks that take a long time to finish.\"\n[5] \"I prefer safe activities rather than risky things just for fun.\"         \n[6] \"I am usually patient and can wait for what I want.\"                      \n\n\n\n\n\n\nQuestion 4\n\n\nReverse score questions 5 and 6.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nSee 1: Data Wrangling for Questionnaires#reverse-coding\nBe careful!! if you have some code that reverse scores a question, and you run it twice, you will essentially reverse-reverse score the question, and it goes back to the original ordering!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. There’s only 2, so let’s do this individually for each question:\n\ngdat &lt;- gdat |&gt; \n  mutate(\n    imp_5 = 6 - imp_5,\n    imp_6 = 6 - imp_6,\n)\nhead(gdat)\n\n# A tibble: 6 × 8\n  online imp_1 imp_2 imp_3 imp_4 imp_5 imp_6  gain\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 casino     2     3     2     3     3     3     0\n2 casino     2     2     2     3     2     2   -12\n3 casino     3     3     3     3     3     4   -35\n4 casino     3     4     3     4     2     3   -20\n5 online     2     3     2     3     3     3   -14\n6 casino     3     2     2     1     3     3   -28\n\n\n\n\n\n\nQuestion 5\n\n\nTake a look at the correlation of the impulsivity questions again.\nWhat has changed?\n\n\n\n\n\nSolution\n\n\n\nSolution 5. The negative correlations are now positive!\n\ncor(gdat[,2:7])\n\n      imp_1 imp_2 imp_3 imp_4 imp_5 imp_6\nimp_1 1.000 0.470 0.555 0.044 0.464 0.453\nimp_2 0.470 1.000 0.414 0.321 0.204 0.305\nimp_3 0.555 0.414 1.000 0.102 0.406 0.461\nimp_4 0.044 0.321 0.102 1.000 0.244 0.334\nimp_5 0.464 0.204 0.406 0.244 1.000 0.390\nimp_6 0.453 0.305 0.461 0.334 0.390 1.000\n\n\n\n\n\n\nQuestion 6\n\n\nWe’re finally getting somewhere! Let’s create a score for “impulsivity” and add it as a new column onto the existing data frame.\nThe description of the questionnaire says that we should take the sum of the scores on each question, to get an overall measure of impulsivity.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe function rowSums() should help us here! See an example in 1: Data Wrangling for Questionnaires#row-scoring\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\ngdat$impulsivity &lt;- rowSums(gdat[,2:7])\n\n\n\n\n\nQuestion 7\n\n\nProvide some descriptive statistics for the impulsivity scale scores of people at the two locations (online vs casino).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe describe() and describeBy() functions from the psych package can often pretty useful for this kind of thing. Alternatively, data |&gt; group_by(...) |&gt; summarise(....)!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. \n\nlibrary(psych)\ndescribeBy(gdat$impulsivity, group=gdat$online)\n\n\n Descriptive statistics by group \ngroup: casino\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis  se\nX1    1 248 16.8 3.22     17    16.8 2.97   7  25    18 -0.11    -0.13 0.2\n------------------------------------------------------------ \ngroup: online\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis  se\nX1    1 234 19.8 3.12     20    19.8 2.97  11  30    19 0.09    -0.06 0.2\n\n\nThe tidyverse way:\n\ngdat |&gt;\n  group_by(online) |&gt;\n  summarise(\n    mean = mean(impulsivity),\n    median = median(impulsivity),\n    min = min(impulsivity),\n    max = max(impulsivity),\n    sd = sd(impulsivity)\n  )\n\n# A tibble: 2 × 6\n  online  mean median   min   max    sd\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 casino  16.8     17     7    25  3.22\n2 online  19.8     20    11    30  3.12\n\n\n\n\n\n\nQuestion 8\n\n\nDoes greater impulsivity lead to bigger losses when comparing online gamblers to casino gamblers?\nUsing the scale scores that you just computed, create a plot to show how impulsivity is associated with gains/losses of gamblers in the two places (casino vs online).\n\n\n\n\n\nSolution\n\n\n\nSolution 8. Something like this should do the trick:\n\nggplot(gdat,aes(x=impulsivity,y=gain,col=online))+\n  geom_point(size = 3, alpha = .3)+\n  geom_smooth(method=lm)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nBased on the plot in the previous question, if you fit the model lm(gain ~ impulsivity * online) to this data (where impulsivity is the scale score), what coefficients would the model estimate? would the sign of each coefficient be positive or negative?\nOnce you’ve made a good effort to predict the answers to these questions, fit the model and see if your predictions are borne out. (If your predictions are different from the outcomes, reflect on why the outcomes are the way they are.)\n\n\n\n\n\nSolution\n\n\n\nSolution 9. Here’s the model:\n\nmod1 &lt;- lm(gain ~ online * impulsivity, data = gdat)\n\nIt’s going to estimate 4 things:\n\ntibble(\n  estimate = c(\"intercept\",\"onlineonline\",\"impulsivity\",\"onlineonline:impulsivity\"),\n  prediction = c(\"around zero/slightly negative\",\"positive\",\"negative\",\"negative\"),\n  explanation = c(\"the 'online' variable is coded with casino as the reference level, so the intercept is going to be the height of the casino line where impulsivity is 0. so it looks like it will be around 0, or a bit below.\",\n                 \"this coefficient will tell us the difference between casino and online when impulsivity is zero. the blue line in the plot is going to be higher than the red line when impulsivity is zero, so this coefficient will be positive\",\n                 \"this is going to be how gains/losses change when impulsivity increases, specifically for the casino group. so in my plot it is the slope of the red line. it's going to be decreasing\",\n                 \"this is going to be how the association between impulsivity and gains/losses changes when we move from casino to online. We know this association is negative in the casino group, and the online group looks like it is even more steeply downwards, so this is going to be a negative coefficient\"\n                 )\n) |&gt; gt::gt()\n\n\n\n\n\n\n\nestimate\nprediction\nexplanation\n\n\n\n\nintercept\naround zero/slightly negative\nthe 'online' variable is coded with casino as the reference level, so the intercept is going to be the height of the casino line where impulsivity is 0. so it looks like it will be around 0, or a bit below.\n\n\nonlineonline\npositive\nthis coefficient will tell us the difference between casino and online when impulsivity is zero. the blue line in the plot is going to be higher than the red line when impulsivity is zero, so this coefficient will be positive\n\n\nimpulsivity\nnegative\nthis is going to be how gains/losses change when impulsivity increases, specifically for the casino group. so in my plot it is the slope of the red line. it's going to be decreasing\n\n\nonlineonline:impulsivity\nnegative\nthis is going to be how the association between impulsivity and gains/losses changes when we move from casino to online. We know this association is negative in the casino group, and the online group looks like it is even more steeply downwards, so this is going to be a negative coefficient\n\n\n\n\n\n\n\n\nsummary(mod1)$coefficients\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                -2.415      3.488  -0.692 0.489038\nonlineonline               12.192      5.563   2.192 0.028884\nimpulsivity                -0.737      0.204  -3.610 0.000338\nonlineonline:impulsivity   -0.533      0.297  -1.793 0.073646\n\n\n\n\n\n\nQuestion 10\n\n\nTake a look again at the wordings of the questions on impulsivity. Do you think they equally represent the construct of ‘impulsivity’?\nIf you’re stuck, think about whether each question might be measuring something else, in addition to (or instead of?) impulsivity.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nimp_1\nI often act on the spur of the moment without thinking.\n\n\nimp_2\nI find it hard to resist temptations.\n\n\nimp_3\nI make decisions quickly, even when they have serious consequences.\n\n\nimp_4\nI find it hard to stay focused on tasks that take a long time to finish.\n\n\nimp_5\nI prefer safe activities rather than risky things just for fun.\n\n\nimp_6\nI am usually patient and can wait for what I want.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is a very subjective question. “Impulsivity” will mean subtly different things to each one of us. The idea is that we want to get at whatever idea it is that is shared across us when we use this word. To me, one of these questions feels a little less closely linked to being an ‘impulsive’ behaviour than the others.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. I’m going to rank them in order of how much I think each question captures “impulsivity” to me.\nI hope that some of you disagree with me about this ranking - that’s what makes measurement frustrating fun!\n\n\n\n\n\n\n\n\nvariable\ndescription\nmy_thoughts\n\n\n\n\nimp_1\nI often act on the spur of the moment without thinking.\nclearly impulsivity\n\n\nimp_3\nI make decisions quickly, even when they have serious consequences.\ncould be impulsivity, could be that you're really good at making decisions\n\n\nimp_6\nI am usually patient and can wait for what I want.\nsimilar to imp_2, impatience and impulsivity kind of go hand in hand, but this is not quite so clearly the definition of impulsivity as the first two\n\n\nimp_2\nI find it hard to resist temptations.\n'temptations' here makes me immediately think of edible temptations! which is one manifestation of impulsivity i guess!\n\n\nimp_5\nI prefer safe activities rather than risky things just for fun.\nis risk taking the same as impulsivity? you can take calculated risks? people do 'risky' sports like climbing for fun, but not out of impulsivity?\n\n\nimp_4\nI find it hard to stay focused on tasks that take a long time to finish.\nthis doesn't really feel like it is as clearly impulsivity. lots of things can distract us from tasks. boredom?\n\n\n\n\n\n\n\n\n\n\n\nQuestion 11\n\n\nOkay, so if we’re not very happy that our 6 questions are equally representative of “impulsivity” (or maybe groups of questions capture distinct aspects of the construct?), we might not want to work with the plain old sum of impulsivity scores that we used above.\nWhat are we going to do?\nLet’s start by doing a Principal Component Analysis (PCA) on the 6 original items, and extracting 6 components.\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee 2: Dimension Reduction#the-key-methods for the demonstration!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 11. \n\nimppca &lt;- principal(gdat[,2:7], nfactors = 6, rotate = \"none\")\nimppca\n\nPrincipal Components Analysis\nCall: principal(r = gdat[, 2:7], nfactors = 6, rotate = \"none\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n       PC1   PC2   PC3   PC4   PC5   PC6 h2       u2 com\nimp_1 0.78 -0.38 -0.09  0.10 -0.32  0.35  1 -4.4e-16 2.4\nimp_2 0.66  0.16 -0.66  0.19 -0.08 -0.26  1 -1.1e-15 2.6\nimp_3 0.76 -0.31 -0.07 -0.16  0.54  0.06  1 -8.9e-16 2.3\nimp_4 0.41  0.87  0.04  0.07  0.10  0.24  1  5.6e-16 1.7\nimp_5 0.67 -0.05  0.56  0.45  0.01 -0.19  1 -1.1e-15 2.9\nimp_6 0.73  0.14  0.23 -0.56 -0.22 -0.17  1 -4.4e-16 2.5\n\n                       PC1  PC2  PC3  PC4  PC5  PC6\nSS loadings           2.78 1.04 0.81 0.59 0.46 0.32\nProportion Var        0.46 0.17 0.14 0.10 0.08 0.05\nCumulative Var        0.46 0.64 0.77 0.87 0.95 1.00\nProportion Explained  0.46 0.17 0.14 0.10 0.08 0.05\nCumulative Proportion 0.46 0.64 0.77 0.87 0.95 1.00\n\nMean item complexity =  2.4\nTest of the hypothesis that 6 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0 \n with the empirical chi square  0  with prob &lt;  NA \n\nFit based upon off diagonal values = 1\n\n\n\n\n\n\nQuestion 12\n\n\nTake a look at the ‘variance accounted for’ by each component (you could use a scree plot to show this too!), and think back to our research question, which has absolutely nothing to do with whether “impulsivity” is one thing, or two things, or 6 things…\nHow many components do you want to keep?\n\n\n\n\n\nSolution\n\n\n\nSolution 12. Our research question, remember is asking “does greater impulsivity lead to bigger losses when comparing online gamblers to casino gamblers?”\nWe’re getting bogged down in the weeds of what do we even mean by ‘impulsivity’?? I would make a case that our research question kind of pre-supposes that “impulsivity” is just one thing. If we reduce these 6 questions down to two or more things, then our research question becomes a little bit more complex to answer, with lots of nuance about what sort of impulsivity we’re talking about. So from a purely pragmatic standpoint, I am really hoping we can just keep one thing, and call that thing “impulsivity”!\nAs it stands, the scree plot is in our favour. It shows a kink (or ‘elbow’, if you’d prefer) at 2 components, suggesting that we would be fine to keep just one.\n\nscree(gdat[,2:7])\n\n\n\n\n\n\n\n\nThe numbers going into the scree plot are shown in the ‘variance accounted for’ bit of the PCA. These tell us that if we keep just one component, then we are capturing 47% of the variability in the questionnaire. If we kept two, then we would capture 64%, 3 would capture 77%, and so on, until we just keep 6 and we’re capturing 100%.\n\nimppca$Vaccounted\n\n                        PC1   PC2   PC3   PC4    PC5    PC6\nSS loadings           2.782 1.043 0.810 0.594 0.4552 0.3158\nProportion Var        0.464 0.174 0.135 0.099 0.0759 0.0526\nCumulative Var        0.464 0.637 0.772 0.872 0.9474 1.0000\nProportion Explained  0.464 0.174 0.135 0.099 0.0759 0.0526\nCumulative Proportion 0.464 0.637 0.772 0.872 0.9474 1.0000\n\n\nThere’s no “right” answer here as to how much we should keep. 47% makes it feel like we’re losing more than we’re capturing (which we are), but that might just be what we have to do!\n\n\n\n\nQuestion 13\n\n\nExtract the scores for the first principal component, and attach them to your dataset as a new set of scores for “impulsivity”.\nAttend also to the loadings for that first component - is it related more to the questions you felt were more clearly asked about ‘impulsivity’?\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo extract the scores, see 2: Dimension Reduction#the-key-methods.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 13. Here are the scores added to our data:\n\ngdat$pc1 &lt;- imppca$scores[,1]\n\nAnd here are the loadings for that first component. It’s less related to imp_4 and imp_2, and more related to imp_1 and imp_3 (this kind of fits in with my view that these two questions are more obviously asking about “impulsivity” to me).\n\nsort(imppca$loadings[,1])\n\nimp_4 imp_2 imp_5 imp_6 imp_3 imp_1 \n0.411 0.660 0.670 0.733 0.762 0.782 \n\n\nThe nice result of this is that, if we use these scores and loadings in subsequent analyses, the questions that are better at targeting the construct we care about are going to be weighted more heavily than the questions that are worse at targeting that construct. The PCA scores are therefore a more accurate way of representing impulsivity than the un-weighted summed score we computed above.\n\n\n\n\nQuestion 14\n\n\nUsing your PCA scores, not the old summed scale scores, create a plot that shows the relationship between impulsivity and financial loss or gain in the two different locations (casino and online). What changes, compared to the old plot?\nNext, fit a new linear model that uses the PCA scores, not the old summed scale scores, to address the question of how impulsivity might affect gains in different locations. What changes, compared to the old model?\n\n\n\n\n\nSolution\n\n\n\nSolution 14. \n\nlibrary(patchwork)\n\np1 &lt;- ggplot(gdat,aes(x=impulsivity,y=gain,col=online))+\n  geom_point(size = 3, alpha = .3)+\n  geom_smooth(method=lm) +\n  labs(title=\"scale scores\")\n\np2 &lt;- ggplot(gdat,aes(x=pc1,y=gain,col=online))+\n  geom_point(size = 3, alpha=.3)+\n  geom_smooth(method=lm) +\n  labs(title=\"PC scores\")\n\np1 + p2 + plot_layout(guides=\"collect\")\n\n\n\n\n\n\n\n\nThere are more different pc1 values than there are different scale score values. The scale score values can only be integers (that is, whole numbers), while the pc1 values can be decimal numbers too. This makes sense, because if the principal component scores are some weighting of each variable, then we’re going to end up with far more possible scores.\n\nmod2 &lt;- lm(gain ~ online * pc1, data = gdat)\n\nsjPlot::tab_model(mod1, mod2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \ngain\ngain\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n-2.41\n-9.27 – 4.44\n0.489\n-15.91\n-17.32 – -14.51\n&lt;0.001\n\n\nonline [online]\n12.19\n1.26 – 23.12\n0.029\n2.72\n0.68 – 4.75\n0.009\n\n\nimpulsivity\n-0.74\n-1.14 – -0.34\n&lt;0.001\n\n\n\n\n\nonline [online] ×\nimpulsivity\n-0.53\n-1.12 – 0.05\n0.074\n\n\n\n\n\npc1\n\n\n\n-2.70\n-4.09 – -1.30\n&lt;0.001\n\n\nonline [online] × pc1\n\n\n\n-2.16\n-4.19 – -0.12\n0.038\n\n\nObservations\n482\n482\n\n\nR2 / R2 adjusted\n0.091 / 0.085\n0.105 / 0.099\n\n\n\n\n\n\n\nNote that the better measurement of “impulsivity” by the PCA (weighting our scores more towards imp_1 and imp_3) results in a significant interaction here.\nIt’s also worth noting that the online [online] coefficient in the new model is also significant, but that is because the PCA scores are standardised, whereas the scale scores are not. So “0” means something very different on those two measures."
  },
  {
    "objectID": "04ex.html",
    "href": "04ex.html",
    "title": "W4 Exercises: Centering",
    "section": "",
    "text": "Hangry\n\nData: hangry1.csv\nThe study is interested in evaluating whether levels of hunger are associated with levels of irritability (i.e., “the hangry hypothesis”). 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week.\nThe data are available at: https://uoepsy.github.io/data/hangry1.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRemember that what we’re interested in is “whether levels of hunger are associated with levels of irritability (i.e., the hangry hypothesis).”\nRead in the data, call the data frame hangry, and fit the model below. How well does it address the research question?\n\nmod1 &lt;- lmer(q_irritability ~ q_hunger + \n                (1 + q_hunger | ppt), \n                data = hangry)\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nAlways plot your data! It’s tempting to just go straight to interpreting coefficients of this model, but in order to understand what a model says we must have a theory about how the data are generated.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\nhangry &lt;- read_csv(\"https://uoepsy.github.io/data/hangry1.csv\")\n\nmod1 &lt;- lmer(q_irritability ~ q_hunger + \n                (1 + q_hunger | ppt), \n                data = hangry)\n\nThe model above will give us that same old formulaic expression of “for people on average, a 1 unit increase in q_hunger is associated with a 0.17 increase in q_irritability”.\nThe problem is that in trying to estimate what does q_irritability do as q_hunger increases, we’re ignoring the fact that people tend to have different average levels of q_hunger:\n\nggplot(hangry, aes(x = q_hunger, y = q_irritability, group = ppt)) +\n  geom_point() +\n  geom_line(alpha=.4) + \n  facet_wrap(~ppt)\n\n\n\n\n\n\n\n\nSo if we try to interpret the fixed effect of our model above as “what happens to a persons’ irritability when they are 1 more hungry?”, we’re not accurately estimating this because our model doesn’t account for the fact that the numbers in q_hunger mean very different things for different people - for person 1 a hunger score of 60 might be “I’m really hungry”, but for person 2 (who usually has a hunger score in the 80s or 90s), 60 could mean “I’m not very hungry at all”.\n\n\n\n\nQuestion 2\n\n\n\n\n\n\n\n\nwithin effects, between effects, and smushed effects\n\n\n\n\n\nResearch Question: are levels of hunger associated with levels of irritability (i.e., the hangry hypothesis)?\nThink about the relationship between irritability and hunger. How should we interpret this research aim?\nIs it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between q_irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a combination of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that this isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it a good estimation of how person \\(i\\) changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are).\n\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant. Call this column hunger_btwn_ppts, since we’ll use it to look at the between-person effect of hunger on irritability.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score. (In other words, each hunger score minus that person’s average hunger score.) Call this column hunger_wi_ppts, since we’ll use it to look at the within-person effect of hunger on irritability.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll find group_by() |&gt; mutate() very useful here, as seen in Chapter 10 #group-mean-centering.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\nhangry &lt;- \n    hangry |&gt; group_by(ppt) |&gt;\n        mutate(\n            hunger_btwn_ppts = mean(q_hunger),\n            hunger_wi_ppts = q_hunger - hunger_btwn_ppts\n        )\nhead(hangry)\n\n# A tibble: 6 × 5\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   hunger_btwn_ppts hunger_wi_ppts\n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1             42       52 N2p1              39.2          12.8 \n2             24       47 N2p1              39.2           7.8 \n3             17        8 N2p1              39.2         -31.2 \n4             26       47 N2p1              39.2           7.8 \n5             27       42 N2p1              39.2           2.80\n6             17       48 N2p2              39.6           8.4 \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people? (This is the between effect, because we’re looking at differences between people’s average hungriness.)\nDoes it look like when people are more hungry than normal, they are more irritable? (This is the within effect, because we’re looking at deviations within each individual person’s hungriness.)\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou might find stat_summary() useful here for plotting the between effect (see Chapter 10 #group-mean-centering)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. Let’s start just by making a scatterplot of each person’s average hunger, plotted against all of their irritability scores.\n\nggplot(hangry,aes(x=hunger_btwn_ppts,y=q_irritability))+\n  geom_point()\n\n\n\n\n\n\n\n\nThe reason we kind of see the points forming vertical columns is because each person’s mean hunger score is located at a single point on the x axis, while their irritability scores take on different values.\nThis is a lot of data, though. We might find it easier to look at a plot where each participant is represented as their mean irritability, along with an indication of the standard error of their irritability scores:\n\nggplot(hangry,aes(x=hunger_btwn_ppts,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nIt’s hard to see any clear relationship between a persons’ average hunger and their irritability scores here.\nIt is also a bit difficult to get at the relationship between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data.\n\nggplot(hangry,aes(x=hunger_wi_ppts,y=q_irritability, group=ppt)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2)\n\n\n\n\n\n\n\n\nIt looks like most of these lines are sloping upwards, but there’s a fair bit of variability in people’s slopes: some people’s lines are going up, some are going down.\nSo we can actually make a guess at what we’re going to see when we model. We’ll probably have a positive fixed effect of hunger_wi_ppts (i.e. A below will be positive), and the by-participant variation in these slopes will be quite large relative to the fixed effect (i.e B below will be quite large in comparison to A)\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr \n ppt      (Intercept)           ...      ... \n          hunger_wi_ppts        ...      *B*\n Residual                       ...      ...    \n\n...\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)      ...        ...     ...\nhunger_wi_ppts   *A*        ...     ...\n...              ...        ...     ...\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation-level deviations from those averages). Those two parts represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables (hunger_btwn_ppts and hunger_wi_ppts) as predictors, instead of the raw hunger scores.\nInclude the appropriate random effects.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWe can only put one of these variables in the random effects (1 + hunger | participant). Think about the fact that each participant has only one value for their average hungriness.\n\nIf the model fails to converge, and if it’s a fairly simple model (i.e one or two random slopes), then often you can switch optimizer (see Chapter 2 #convergence-warnings-singular-fits). For instance, try adding control = lmerControl(optimizer = \"bobyqa\") to the model.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. With the defaults, this model doesn’t converge\n\nhangrywb &lt;- lmer(q_irritability ~ hunger_btwn_ppts + hunger_wi_ppts + \n                (1 + hunger_wi_ppts | ppt), \n                data = hangry)\n\nWarning message:\nIn checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,  :\n  Model failed to converge with max|grad| = 0.0027028 (tol = 0.002, component 1)\nChanging the optimizer helps:\n\nhangrywb &lt;- lmer(q_irritability ~ hunger_btwn_ppts + hunger_wi_ppts + \n                (1 + hunger_wi_ppts | ppt), \n                data = hangry,\n                control = lmerControl(optimizer = \"bobyqa\"))\n\n\n\n\n\n\n\noptional - why change the optimizer?\n\n\n\n\n\nNote that the max|grad| convergence error of the initial model was very close to the tolerance (see Chapter 8 #non-convergence for an explanation of what this tolerance is).\nThe fact that it is close indicates that we may be quite close to a solution, so it’s worth investigating if this is simply an optimizer problem.\nOne other thing to do would be to consider all available optimizers, see which ones converge, and compare estimates across them. If the estimates are the same (or pretty close), and some of these converge, then it gives us more trust in our model. We can do this with the code below. We can see that 5 optimizers don’t give error messages, and that they all give pretty much the same estimated fixed effects. We can go further and compare random effects variances too, but we won’t do that here.\n\n# fit with all optimizers\nallopts = allFit(hangrywb)\n\nbobyqa : [OK]\nNelder_Mead : [OK]\nnlminbwrap : [OK]\nnloptwrap.NLOPT_LN_NELDERMEAD : [OK]\nnloptwrap.NLOPT_LN_BOBYQA : \n\n\n[OK]\n\n\n\n# error messages from each optimizer \n# (NULL here means no message, which is good)\nsummary(allopts)$msgs\n\n$bobyqa\nNULL\n\n$Nelder_Mead\nNULL\n\n$nlminbwrap\nNULL\n\n$nloptwrap.NLOPT_LN_NELDERMEAD\nNULL\n\n$nloptwrap.NLOPT_LN_BOBYQA\n[1] \"Model failed to converge with max|grad| = 0.0027028 (tol = 0.002, component 1)\"\n\n# fixed effect estimates for all optimizers\nsummary(allopts)$fixef\n\n                              (Intercept) hunger_btwn_ppts hunger_wi_ppts\nbobyqa                               17.6         -0.00644          0.187\nNelder_Mead                          17.6         -0.00644          0.187\nnlminbwrap                           17.6         -0.00644          0.187\nnloptwrap.NLOPT_LN_NELDERMEAD        17.6         -0.00647          0.187\nnloptwrap.NLOPT_LN_BOBYQA            17.6         -0.00648          0.187\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nWrite down what each of the fixed effects means.\n\n\n\n\n\nSolution\n\n\n\nSolution 5. Here are the fixed effects:\n\nfixef(hangrywb)\n\n     (Intercept) hunger_btwn_ppts   hunger_wi_ppts \n        17.62005         -0.00644          0.18663 \n\n\n\n\n\n\n\n\n\n\nterm\nest\ninterpretation\n\n\n\n\n(Intercept)\n17.620\nestimated irritability score for someone with an average hunger of 0, and not deviating from that average (i.e. hunger_wi_ppts = 0)\n\n\nhunger_btwn_ppts\n-0.006\nestimated difference in irritability between two people who differ in average hunger level by 1 (e.g., a person with average hunger of 11 vs someone with average hunger level of 10), when they are at their average (hunger_wi_ppts = 0)\n\n\nhunger_wi_ppts\n0.187\nestimated change in irritability score for every 1 more hungry a person is than they normally are\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nHave a go at also writing an explanation for yourself of the random effects part of the output (i.e., the stuff that comes out when you run the code VarCorr(MODELNAMEHERE)).\nThere’s no formulaic way to interpret these, but have a go at describing in words what they represent, and how that adds to the picture your model describes.\nDon’t worry about making it read like a report - just write yourself an explanation!\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\nVarCorr(hangrywb)\n\n Groups   Name           Std.Dev. Corr \n ppt      (Intercept)    6.992         \n          hunger_wi_ppts 0.366    -0.08\n Residual                4.772         \n\n\n\n\n\n\n\n\n\n\nterm\nest\ninterpretation\n\n\n\n\nsd__(Intercept)\n6.992\nParticipant level variability in irritability when they are at their average hunger level - i.e. when everybody is at their own average level of hunger, they vary in their irritability scores with a standard deviation of 7.\n\n\nsd__hunger_wi_ppts\n0.366\nParticipants vary quite a bit in how deviations from hunger are associated with irritability. They vary around the fixed effect of 0.19 with a standard deviation of 0.37. To think about what this means, imagine a normal distribution that is centered on 0.19 and has a standard deviation of 0.36. A fairly large portion of that distribution would fall below zero (i.e. have a negative slope). And we would also expect some slopes that are e.g., .5, .6 etc.\n\n\ncor__(Intercept).hunger_wi_ppts\n-0.080\nThis estimate is basically zero, but it represents the relationship between participants' relative standing at the intercept and their relative standing on the slopes. So participants who are more irritable than others when at their average hunger, tend to have very very slightly more negative slopes. In other words, people who are more irritable when at average hunger become irritable *slightly* slower, as they get hungrier, compared to people who are less irritable\n\n\nsd__Observation\n4.772\nthe residual variance doesn't really have much of an interpretation - it really just represents all the leftover stuff that the model doesn't explain. If we imagine all of the individual participant lines, 4.77 represents how spread out (on the scale of irritability scores) the individual observations are around those lines\n\n\n\n\n\n\n\n\n\n\n\n\nHangry 2\n\nQuestion 7\n\n\nA second dataset on the same variables is available at: https://uoepsy.github.io/data/hangry2.csv.\nThese data are from people who were following a five-two diet, while the original dataset were from people who were not following any diet. (On the five-two diet, people eat normally for five days a week, but then restrict their intake for two days a week.)\nCombine the datasets together so we can fit a model to see if the hangry effect differs between people on diets vs those who aren’t.\nCall the new dataframe hangryfull.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nSomething like bind_rows() might help here. If you’ve not seen it before, remember that you can look up the help documentation in the bottom-right panel of RStudio.\nBe sure to keep an indicator of which group the data are in in a column called diet.\n\nFor example, use mutate() to add an identifier column to each one before binding.\nOr use the .id argument of bind_rows() to identify the original data frame. (Check the documentation to see how to use .id!)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Here are our two datasets:\n\nhangry1 &lt;- read_csv(\"https://uoepsy.github.io/data/hangry1.csv\")\nhangry2 &lt;- read_csv(\"https://uoepsy.github.io/data/hangry2.csv\")\n\nIf we simply bind them together using bind_rows() like this…\n\nhangryfull &lt;- \n  bind_rows(\n    hangry1, \n    hangry2\n  )\n\n… then we wouldn’t know which data was from which group!\nThere are a couple ways of adding identifiers to keep the non-diet data and the diet data apart.\nOne way: Add a new identifier variable to each data frame first.\n\nhangryfull &lt;- \n  bind_rows(\n    hangry1 |&gt; mutate(diet = \"N\"), \n    hangry2 |&gt; mutate(diet = \"Y\")\n  )\nhead(hangryfull)\n\n# A tibble: 6 × 4\n  q_irritability q_hunger ppt   diet \n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1             42       52 N2p1  N    \n2             24       47 N2p1  N    \n3             17        8 N2p1  N    \n4             26       47 N2p1  N    \n5             27       42 N2p1  N    \n6             17       48 N2p2  N    \n\n\nAnother way: Within bind_rows(), you can assign each data frame a name that represents its value in the identifier column. And you can give the identifier column a name using the argument .id, as follows:\n\nhangryfull &lt;-\n  bind_rows(\n  'N' = hangry1, \n  'Y' = hangry2,\n  .id = 'diet'\n  )\n\nBoth methods have exactly the same result! You can pick your favourite way.\n\n\n\n\nQuestion 8\n\n\nDoes the relationship between hunger and irritability depend on whether or not people are following the five-two diet?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWhich relationship between hunger and irritability are we talking about? The between effect or the within effect? It could be both!\nTo fit the model we want, we’ll need to create those two variables (hunger_btwn_ppts and hunger_wi_ppts) for this combined dataset again.\nThis model will also require a variable that tells us whether people were on the diet or not (the identifier variable from before). We’ll also need to figure out whether we can get random slopes over the new diet variable. Can we include random slopes for each participant over diet? Why or why not?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nhangryfull &lt;- \n    hangryfull |&gt; group_by(ppt) |&gt;\n        mutate(\n            hunger_btwn_ppts = mean(q_hunger),\n            hunger_wi_ppts = q_hunger - hunger_btwn_ppts\n        )\n\nhangrywbdiet &lt;- lmer(q_irritability ~ (hunger_btwn_ppts + hunger_wi_ppts) * diet + \n                (1 + hunger_wi_ppts | ppt), \n                data = hangryfull,\n                control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(hangrywbdiet, corr = FALSE)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: q_irritability ~ (hunger_btwn_ppts + hunger_wi_ppts) * diet +  \n    (1 + hunger_wi_ppts | ppt)\n   Data: hangryfull\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 2735\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4138 -0.5906 -0.0454  0.5426  2.3954 \n\nRandom effects:\n Groups   Name           Variance Std.Dev. Corr \n ppt      (Intercept)    48.083   6.934         \n          hunger_wi_ppts  0.145   0.381    -0.01\n Residual                23.305   4.828         \nNumber of obs: 405, groups:  ppt, 81\n\nFixed effects:\n                        Estimate Std. Error t value\n(Intercept)             17.13095    5.14648    3.33\nhunger_btwn_ppts         0.00386    0.10533    0.04\nhunger_wi_ppts           0.18577    0.07560    2.46\ndietY                  -10.85470    6.53568   -1.66\nhunger_btwn_ppts:dietY   0.46590    0.13354    3.49\nhunger_wi_ppts:dietY     0.38141    0.10139    3.76\n\n\n\n\n\n\nQuestion 9\n\n\nConstruct two plots, one for each of the interactions that the model estimates. This model is a bit of a confusing one, so plotting may help a bit with understanding what those interactions represent.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\neffects(terms, mod) |&gt; as.data.frame() |&gt; ggplot(.....)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. The xlevels bit here just gives us the little dataframe to plot with more levels at it, so that it gives us smoother lines. Try it with and without to see what I mean!\n\nlibrary(effects)\neffect(\"hunger_btwn_ppts*diet\", hangrywbdiet, xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=hunger_btwn_ppts, y=fit,col=diet))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=diet),alpha=.4)+\n  labs(x=\"participants' average hunger level\")\n\n\n\n\n\n\n\n\nWe saw in our original model that for the reference level of diet, the “N” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. In our full model this is the hunger_btwn_ppts coefficient.\nWe also saw the interaction hunger_btwn_ppts:dietY indicates that irritability is estimated to increase by 0.47 more for those in the diet than it does for those not on the diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\neffect(\"hunger_wi_ppts*diet\", hangrywbdiet, xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=hunger_wi_ppts, y=fit,col=diet))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=diet),alpha=.4)+\n  labs(x=\"increase from participants' average hunger level\")\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_wi_ppts we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re in the diet “N” group). This is the slope of the red line - the hunger_wi_ppts coefficient from our full model.\nThe interaction hunger_wi_ppts:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive which matches with the fact that the blue line is steeper in this plot. In other words, if people are on a diet, then as their hunger increases relative to their average, they become irritable faster than people who are not on a diet.\n\n\n\n\nQuestion 10\n\n\nRun significance tests for the fixed effects, and write up the results.\n\n\n\n\n\nSolution\n\n\n\nSolution 10. \n\nhangrywbdiet.p &lt;- lmerTest::lmer(q_irritability ~ (hunger_btwn_ppts + hunger_wi_ppts) * diet + \n                (1 + hunger_wi_ppts | ppt), \n                data = hangryfull,\n                control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(hangrywbdiet.p)$coefficients\n\n                        Estimate Std. Error   df t value Pr(&gt;|t|)\n(Intercept)             17.13095     5.1465 77.0  3.3287 0.001341\nhunger_btwn_ppts         0.00386     0.1053 77.0  0.0367 0.970834\nhunger_wi_ppts           0.18577     0.0756 65.4  2.4573 0.016659\ndietY                  -10.85470     6.5357 77.0 -1.6608 0.100813\nhunger_btwn_ppts:dietY   0.46590     0.1335 77.0  3.4888 0.000806\nhunger_wi_ppts:dietY     0.38141     0.1014 68.5  3.7617 0.000352\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on the five-two diet, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores and their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes over relative-hunger level were included for participants. The model was fitted with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser. \\(P\\)-values were obtained using the Satterthwaite approximation for degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(b = 0.19,\\ SE = 0.08,\\ t(65.41) = 2.46,\\ p=0.017\\)), and that this association was further increased for those following the five-two diet (\\(b = 0.38,\\ SE = 0.1,\\ t(68.49) = 3.76,\\ p&lt;0.001\\)).\nFor those not on a specific diet, there was no evidence for an association between irritability and being generally a more hungry person (\\(p=0.971\\)). However, there was a significant interaction between average hunger and being on the five-two diet (\\(b = 0.47,\\ SE = 0.13,\\ t(77) = 3.49,\\ p&lt;0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association with irritability of both between-person hunger and within-person hunger."
  },
  {
    "objectID": "02ex.html",
    "href": "02ex.html",
    "title": "W2 Exercises: Introducing MLM",
    "section": "",
    "text": "These first set of exercises are not “how to do analyses with multilevel models” - they are designed to get you thinking, and help with an understanding of how these models work.\n\n\nQuestion 1\n\n\nRecall the data from last week’s exercises. Instead of looking at the roles A, B and C, we’ll look in more fine grained detail at the seniority. This is mainly so that we have a continuous variable to work with as it makes this illustration easier.\nThe chunk of code below shows a function for plotting that you might not be familiar with - stat_summary(). This takes the data in the plot and “summarises” the Y-axis variable into the mean at every unique value on the x-axis. So below, rather than having a lot of individual data points that represent every employee’s wp (workplace pride), we let stat_summary() compute the mean (the points) plus and minus the standard error (the vertical lines) of all the wp observations at each value of seniority:\n\nlibrary(tidyverse)\njsup &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\nggplot(jsup, aes(x = seniority, y = wp, col = role)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nBelow is some code that fits a model of the workplace-pride predicted by seniority level. Line 2 then gets the ‘fitted’ values from the model and adds them as a new column to the dataset, called pred_lm. The fitted values are what the model predicts the workplace pride to be for every value of seniority.\nLines 4-7 then plot the data, split up by each department, and adds lines showing the model fitted values.\nRun the code and check that you get a plot. What do you notice about the lines?\n\nlm_mod &lt;- lm(wp ~ seniority, data = jsup)\njsup$pred_lm &lt;- predict(lm_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_lm), col = \"red\")\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. We should get something like this:\n\nlm_mod &lt;- lm(wp ~ seniority, data = jsup)\njsup$pred_lm &lt;- predict(lm_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_lm), col = \"red\")\n\n\n\n\n\n\n\n\nNote that the lines are exactly the same for each department. This makes total sense, because the model (which is where we’ve got the lines from) completely ignores the department variable!\n\n\n\n\nQuestion 2\n\n\nBelow are 3 more code chunks that all 1) fit a model, then 2) add the fitted values of that model to the plot.\nThe first model is a ‘no-pooling’ approach, similar to what we did in last week’s exercises - adding in dept as a predictor.\nThe second and third are multilevel models. The second fits random intercepts by-department, and the third fits random intercepts and slopes of seniority.\nCopy each chunk and run through the code. Pay attention to how the lines differ.\n\n\nCode\nfe_mod &lt;- lm(wp ~ dept + seniority, data = jsup)\njsup$pred_fe &lt;- predict(fe_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_fe), col = \"blue\")\n\n\n\n\nCode\nlibrary(lme4)\nri_mod &lt;- lmer(wp ~ seniority + (1|dept), data = jsup)\njsup$pred_ri &lt;- predict(ri_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_ri), col = \"green\")\n\n\n\n\nCode\nrs_mod &lt;- lmer(wp ~ seniority + (1 + seniority|dept), data = jsup)\njsup$pred_rs &lt;- predict(rs_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_rs), col = \"orange\")\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. With the first model, wp ~ seniority + dept, we are saying the following things:\n\nWe allow the model to estimate an association between workplace pride and seniority. This line is estimated without respect to department, or in other words, for the “average” department.\nWe also allow the model to take that line and shift it up and down for each different department. The slope of the line stays the same. All that’s changing is the vertical position of the line.\n\nLook at UKSA and FSA: the lines are shifted up compared to the others.\nLook at UKSC and OFSTED: the lines are shifted down compared to the others.\n\n\n\nfe_mod &lt;- lm(wp ~ dept + seniority, data = jsup)\njsup$pred_fe &lt;- predict(fe_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_fe), col = \"blue\")\n\n\n\n\n\n\n\n\nWith the second model, wp ~ seniority + (1|dept), we are saying the following things:\n\nWe allow the model to estimate an association between workplace pride and seniority.\nWith (1|dept), we allow the model to adjust the intercept of that line, depending on the scores of each department.\n\nIn other words, the wp ~ seniority bit is modelling the average line for all departments, and the (1|dept) bit is saying “now nudge that line up or down so that it fits the data of each individual department better”.\nThis is very similar to the fixed-effects-based model above. The difference is that the nudges up and down are now drawn from a single distribution, so they are all slightly closer to the mean of departments than the changes that the model above made.\nPeople often call these “random intercepts by department”. You may also hear “intercept adjustments by department”.\nIf we look at this model’s summary, we can see how it combines all these individual adjustments into a distribution and give us some summary statistics. This way, we can see how big the adjustments tend to be.\n\n\nSide note: Why use random effects instead of fixed effects, if they do the same thing?\n\nThey don’t really do the same thing: random effects can also include random slopes, which the fixed-effect model cannot do. We’ll see random slopes next.\nFixed effects are generally used for predictors that we have particular hypotheses/predictions about. Random effects are used when we have other sources of non-independence we need to tell the model about.\n\n\nlibrary(lme4)\nri_mod &lt;- lmer(wp ~ seniority + (1|dept), data = jsup)\njsup$pred_ri &lt;- predict(ri_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_ri), col = \"green\")\n\n\n\n\n\n\n\n\nFinally, with the third model, wp ~ seniority + (1 + seniority|dept), we are saying the following things:\n\nWe allow the model to estimate an association between workplace pride and seniority, for departments on average. That’s the wp ~ seniority bit, like before.\nWith (1 + seniority|dept), we allow the model to adjust the intercept of that line AND to adjust the slope of that line for each individual department.\n\nIn other words, the wp ~ seniority bit is modelling the average line for all departments, and the (1 + seniority|dept) bit is saying “now nudge that line up or down AND change how steep it is, so that it fits the data of each individual department better”.\nPeople often call these “random intercepts by department and random slopes over seniority by department”. You may also hear “intercept adjustments by department and adjustments to the slope of seniority by department”.\nIf we look at this model’s summary, we can see how it combines all these individual adjustments into a distribution of intercept adjustments and a distribution of slope adjustments, and give us some summary statistics about both. This way, we can see how big the adjustments tend to be.\n\n\nSo in this next plot, the height of the lines is changing, but additionally, each department’s association between seniority and workplace pride can be different. Some departments (OFQUAL, OFSTED, ORR) have a negative association, some have a flatter association (e.g, FSA, UKSA etc).\n\nrs_mod &lt;- lmer(wp ~ seniority + (1 + seniority|dept), data = jsup)\njsup$pred_rs &lt;- predict(rs_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_rs), col = \"orange\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFrom the previous questions you should have a model called ri_mod.\nBelow is a plot of the fitted values from that model. Rather than having a separate facet for each department as we did above, I have put them all on one plot. The thick black line is the average intercept and slope of the departments lines.\nIdentify the parts of the plot that correspond to A1-4 in the summary output of the model below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nChoose from these options:\n\nwhere the black line cuts the y axis (at x=0)\n\nthe slope of the black line\n\nthe standard deviation of the distances from all the individual datapoints (employees) to the line for the department in which it works.\n\nthe standard deviation of the distances from all the individual department lines to the black line\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1 = the standard deviation of the distances from all the individual department lines to the black line\n\nA2 = the standard deviation of the distances from all the individual datapoints (employees) to the line for the department in which it works.\nA3 = where the black line cuts the y axis\n\nA4 = the slope of the black line\n\n\n\n\n\nOptional Extra\n\n\nBelow is the model equation for the ri_mod model.\nIdentify the part of the equation that represents each of A1-4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n\\text{For Employee }j\\text{ from Dept }i & \\\\\n\\text{Level 1 (Employee):}& \\\\\n\\text{wp}_{ij} &= b_{0i} + b_1 \\cdot \\text{seniority}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (Dept):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{Where:}& \\\\\n\\zeta_{0i} &\\sim N(0,\\sigma_{0}) \\\\\n\\varepsilon &\\sim N(0,\\sigma_{e}) \\\\\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nChoose from:\n\n\\(\\sigma_{\\varepsilon}\\)\n\n\\(b_{1}\\)\n\n\\(\\sigma_{0}\\)\n\n\\(\\gamma_{00}\\)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nA1 = \\(\\sigma_{0}\\)\n\nA2 = \\(\\sigma_{\\varepsilon}\\)\n\nA3 = \\(\\gamma_{00}\\)\n\nA4 = \\(b_{1}\\)"
  },
  {
    "objectID": "02ex.html#footnotes",
    "href": "02ex.html#footnotes",
    "title": "W2 Exercises: Introducing MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is like taking predict() from the model, and then then grouping by age, and calculating the mean of those predictions. However, we can do this more easily using augment() and then some fancy stat_summary() in ggplot↩︎\nprovided that the confidence intervals and p-values are constructed using the same methods↩︎"
  },
  {
    "objectID": "00tutors.html",
    "href": "00tutors.html",
    "title": "Tutors",
    "section": "",
    "text": "In Week 1, start all labs with a quick:\n“hello. almost all of you are well practiced at DAPR labs now. take a seat, say hello to your colleagues, open Rstudio, and get started on the exercises on Learn.\nwork together with people on your table - explain stuff to one another, troubleshoot together etc.\nfor any questions, we have lovely tutors wandering around, just grab one, or put your hand up etc, and they will be able to help.\nThe groups for the report will not be set until week 3, so for now just sit with whoever you want”"
  },
  {
    "objectID": "00tutors.html#tutor-guidelines",
    "href": "00tutors.html#tutor-guidelines",
    "title": "Tutors",
    "section": "Tutor guidelines",
    "text": "Tutor guidelines\n\ntry to do less:\n\nstanding around chatting\nusing your phone\nsitting with one student for too long\n\ntry to do more:\n\nmake sure every table gets an introduction to a tutor (“hello, i’m X, how are you getting on? etc”)\nmake sure tables don’t get ignored (even if no hands are going up)\nif all is quiet, try actively asking a table 1) where they’re up to in the exercises, 2) if they need any help etc."
  },
  {
    "objectID": "00prereq.html",
    "href": "00prereq.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Install/Update R & RStudio\nMake sure you have installed both R and RStudio on your computer. You may have done this previously for DAPR2, in which case it is probably worth doing some updates.\nPlease make sure to read and follow the instructions below slowly and carefully!!\n\nFor instructions on how to install R and RStudio, click here\nFor instructions on how to update R and RStudio, click here\n\n\n\nUpdate Packages\nIt’s worth keeping packages up to date, so it might be worth updating all your packages.\nRunning this code will update all your packages. Just put it into the console (bottom left bit of RStudio):\n\noptions(pkgType = \"binary\")\nupdate.packages(ask = FALSE)\n\n\n\nNew packages!\nNow it is probably worth installing a few of the packages that we will be using in DAPR3. There are a few that we will need. For each one, check whether you have it already installed, because there’s not much point wasting time re-installing something you already have!\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models\npsych: for factor analysis\nlavaan: for latent variable models"
  },
  {
    "objectID": "01ex.html",
    "href": "01ex.html",
    "title": "W1: Regression Refresher",
    "section": "",
    "text": "Workplace Pride\n\nData: lmm_jsup.csv\nA questionnaire was sent to all UK civil service departments, and the lmm_jsup.csv dataset contains all responses that were received. Some of these departments work as hybrid or ‘virtual’ departments, with a mix of remote and office-based employees. Others are fully office-based.\nThe questionnaire included items asking about how much the respondent believe in the department and how it engages with the community, what it produces, how it operates and how treats its people. A composite measure of ‘workplace-pride’ was constructed for each employee. Employees in the civil service are categorised into 3 different roles: A, B and C. The roles tend to increase in responsibility, with role C being more managerial, and role A having less responsibility. We also have data on the length of time each employee has been in the department (sometimes new employees come straight in at role C, but many of them start in role A and work up over time).\nWe’re interested in whether the different roles are associated with differences in workplace-pride.\nDataset: https://uoepsy.github.io/data/lmm_jsup.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndepartment_name\nName of government department\n\n\ndept\nDepartment Acronym\n\n\nvirtual\nWhether the department functions as hybrid department with various employees working remotely (1), or as a fully in-person office (0)\n\n\nrole\nEmployee role (A, B or C)\n\n\nseniority\nEmployees seniority point. These map to roles, such that role A is 0-4, role B is 5-9, role C is 10-14. Higher numbers indicate more seniority\n\n\nemployment_length\nLength of employment in the department (years)\n\n\nwp\nComposite Measure of 'Workplace Pride'\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and provide some descriptive plots and statistics of each individual variable.\n\n\n\n\n\n\nHints\n\n\n\n\n\nDon’t remember how to do descriptives? Think back to previous courses – it’s time for some means, standard deviations, mins and maxes. For categorical variables we can do counts or proportions.\nWe’ve seen various functions such as summary(), and also describe() from the psych package.\nFor continuous variables, histograms are a good first port of call: try hist(). For categorical variables, you could try plotting the outcome of table().\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. Here’s the dataset:\n\nlibrary(tidyverse) # for data wrangling\nlibrary(psych) \n\njsup &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\nLet’s take just the numeric variables and get some descriptives:\n\njsup |&gt; \n  select(employment_length, wp) |&gt; \n  describe()\n\n                  vars   n mean   sd median trimmed  mad  min  max range  skew\nemployment_length    1 295 12.6 4.28   13.0    12.6 4.45 0.00 30.0  30.0  0.08\nwp                   2 295 25.5 5.27   25.4    25.5 5.93 6.34 38.5  32.1 -0.05\n                  kurtosis   se\nemployment_length     0.38 0.25\nwp                   -0.14 0.31\n\n\nAnd make frequency tables for the categorical ones:\n\ntable(jsup$role)\n\n\n  A   B   C \n109  95  91 \n\n\nI’m going to use dept rather than department_name as the output will be easier to see:\n\ntable(jsup$dept)\n\n\n   ACE    CMA    CPS    FSA    GLD   HMRC    NCA   NS&I  OFGEM OFQUAL OFSTED \n    17     21     13     25     17     16     20     20     15      5     17 \n OFWAT    ORR    SFO   UKSA   UKSC \n    16     17     18     45     13 \n\ntable(jsup$virtual)\n\n\n  0   1 \n175 120 \n\n\n\n\n\n\nQuestion 2\n\n\nAre there differences in ‘workplace-pride’ between people in different roles?\nFirst, plot these two variables together. Based on this plot and your training from DAPR2, try to answer these questions:\n\nIf you fit a model to this data, how would the predictor be coded?\nWhat coefficients would the model estimate?\nWould the sign of the coefficients be positive or negative?\n\nOnce you’ve made a good effort to predict the answers to these questions, fit a model and see if your predictions are borne out. (If your predictions are different from the outcomes, reflect on why the outcomes are the way they are.)\n\n\n\n\n\n\nHints\n\n\n\n\n\ndoes y [continuous variable] differ by x [three groups]?\nlm(y ~ x)?\nBy default, R uses treatment coding (aka dummy coding), and the reference level is the one that comes first in the alphabet.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\nmod1 &lt;- lm(wp ~ role, data = jsup)\n\nRather than doing summary(model) - I’m just going to use the broom package to pull out some of the stats in nice tidy dataframes.\nThe glance() function will give us things like the \\(R^2\\) values and \\(F\\)-statistic (basically all the stuff that is at the bottom of the summary()):\n\nlibrary(broom)\nglance(mod1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.216         0.211  4.68      40.3 3.44e-16     2  -872. 1753. 1768.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe tidy() function will give us the coefficients, standard errors, t-statistics and p-values. It’s the same information, just neater!\n\ntidy(mod1)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    28.1      0.448     62.6  3.66e-171\n2 roleB          -2.24     0.657     -3.41 7.33e-  4\n3 roleC          -5.95     0.665     -8.95 4.38e- 17\n\n\nAlternatively, we can get some quick confidence intervals for our coefficients:\n\nconfint(mod1)\n\n            2.5 % 97.5 %\n(Intercept) 27.17 28.933\nroleB       -3.54 -0.949\nroleC       -7.26 -4.638\n\n\nIt looks like roles do differ in their workplace pride. Specifically, compared to people in role A, people who are in roles B and C on average report less pride in the workplace.\n\n\n\n\n\nQuestion 3\n\n\nOne possibility: Something about the roles themselves makes people report differences in workplace pride.\nAnother possibility: People who are newer to the company feel more pride (they’re less jaded), and people in Role A tend to be newer. So something else is at play, but the model above makes it look like it’s about role.\nIn other words, if we were to compare people in each role but hold constant their employment_length, might we see something different?\nMake a plot that shows all these relevant variables. Based on that plot, have a guess at the following questions:\n\nWhat coefficients will be estimated by a model fit to this data?\nWould the sign of the coefficients be positive or negative?\n\nOnce you’ve made a good effort to predict the answers to these questions, fit a model and see. (If your predictions are different from the outcomes, reflect on why the outcomes are the way they are.)\n\n\n\n\n\n\nHints\n\n\n\n\n\nSo we want to adjust for how long people have been part of the company..\nRemember - if we want to estimate the effect of x on y while adjusting for z, we can do lm(y ~ z + x).\nFor the plot - put something on the x, something on the y, and colour it by the other variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\nmod2 &lt;- lm(wp ~ employment_length + role, data = jsup)\n\ntidy(mod2)\n\n# A tibble: 4 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         36.1      0.709     50.9   6.90e-147\n2 employment_length   -0.834    0.0637   -13.1   4.32e- 31\n3 roleB                0.510    0.563      0.906 3.65e-  1\n4 roleC               -0.704    0.663     -1.06  2.89e-  1\n\n\nNote that, after adjusting for employment length, there are no significant differences in wp between roles B or C compared to A.\nIf we plot the data to show all these variables together, we can kind of see why! Given the pattern of wp against employment_length, the wp for different roles are pretty much where we would expect them to be if role doesn’t make any difference (i.e., if role doesn’t shift your wp up or down).\n\nggplot(jsup, aes(x=employment_length,y=wp,col=role))+\n  geom_point(size=3,alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nLet’s take a step back and remember what data we actually have. We’ve got 295 people in our dataset, from 16 departments.\nDepartments may well differ in the general amount of workplace-pride people report. People love to say that they work in the “National Crime Agency”, but other departments might not elicit such pride (*cough* HM Revenue & Customs *cough*). We need to be careful not to mistake department differences as something else (like differences due to the job role).\nMake a couple of plots to look at:\n\nhow many of each role we have from each department\nhow departments differ in their employees’ pride in their workplace\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nggplot(jsup, aes(x = role)) + \n  geom_bar()+\n  facet_wrap(~dept)\n\n\n\n\n\n\n\n\nIn this case, it looks like most of the departments have similar numbers of each role, apart from the UKSA (“UK Statistics Authority”), where we’ve got loads more of role A, and very few role C..\nNote also that in the plot below, the UKSA is, on average, full of employees who take a lot of pride in their work. Is this due to the high proportion of people in role A? or is the effect of role we’re seeing more due to differences in departments?\n\nggplot(jsup, aes(x = dept, y = wp)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = label_wrap_gen(35)) + \n  coord_flip()\n\n\n\n\n\n\n\n\nEven if we had perfectly equal numbers of roles in each department, we’re also adjusting for other things such as employment_length, and the extent to which this differs by department can have trickle-on effects on our coefficient of interest (the role coefficients).\n\n\n\n\nQuestion 5\n\n\nAdjusting for both length of employment and department, are there differences in ‘workplace-pride’ between the different roles? Fit a model to find out.\nCan you make a plot of all four of the variables involved in our model?\n\n\n\n\n\n\nHints\n\n\n\n\n\nMaking the plot might take some thinking. We’ve now added dept into the mix, so a nice way might be to use facet_wrap() to make the same plot as the one we did previously, but for each department.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. \n\nmod3 &lt;- lm(wp ~ employment_length + dept + role, data = jsup)\ntidy(mod3)\n\n# A tibble: 19 × 5\n   term              estimate std.error statistic   p.value\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)        36.4       0.631    57.6    7.17e-156\n 2 employment_length  -0.882     0.0344  -25.7    4.71e- 75\n 3 deptCMA            -3.80      0.649    -5.85   1.39e-  8\n 4 deptCPS            -0.217     0.730    -0.298  7.66e-  1\n 5 deptFSA             4.74      0.625     7.60   4.71e- 13\n 6 deptGLD             0.0582    0.682     0.0853 9.32e-  1\n 7 deptHMRC           -3.79      0.692    -5.47   1.02e-  7\n 8 deptNCA            -3.85      0.655    -5.88   1.18e-  8\n 9 deptNS&I           -0.574     0.654    -0.878  3.81e-  1\n10 deptOFGEM          -0.648     0.705    -0.919  3.59e-  1\n11 deptOFQUAL         -4.94      1.01     -4.89   1.71e-  6\n12 deptOFSTED         -5.88      0.683    -8.61   5.52e- 16\n13 deptOFWAT          -1.21      0.692    -1.75   8.17e-  2\n14 deptORR            -2.85      0.681    -4.18   3.98e-  5\n15 deptSFO            -1.36      0.672    -2.02   4.47e-  2\n16 deptUKSA            4.28      0.576     7.43   1.32e- 12\n17 deptUKSC           -2.31      0.732    -3.16   1.77e-  3\n18 roleB               1.42      0.303     4.68   4.47e-  6\n19 roleC               1.31      0.366     3.59   3.92e-  4\n\n\nIn a way, adding predictors to our model is kind of like splitting up our plots by that predictor to see the patterns. This becomes more and more difficult (/impossible) as we get more variables, but right now we can split the data into all the constituent parts.\n\nggplot(jsup, aes(x = employment_length, y = wp, col = role)) +\n  geom_point(size=3,alpha=.4)+\n  facet_wrap(~dept)\n\n\n\n\n\n\n\n\nThe association between wp and employment_length is clear in all these little sub-plots - there’s a downward trend. The department differences can be seen too: UKSA is generally a bit higher, HMRC and UKSC a bit lower, and so on. By default, the model captures these coefficients as ‘differences from the reference group’, so all these coefficients are in relation to the “ACE” department.\nSeeing the role differences is a bit harder in this plot, but think about what you would expect to see if there were no differences in roles (i.e. imagine if they were all in role A). Take for instance the FSA department, where this is easiest to see - for the people who are in role C, for people of their employment length we would expect their wp to be lower if they were in role A. Likewise for those in role B. Across all these departments, the people in role B and C (green and blue dots respectively) are a bit higher than we would expect. This is what the model coefficients tell us!\n\n\n\n\nQuestion 6\n\n\nNow we’re starting to acknowledge the grouped structure of our data - these people in our dataset are related to one another in that some belong to dept 1, some dept 2, and so on..\nLet’s try to describe our sample in a bit more detail.\n\nhow many participants do we have, and from how many departments?\nhow many participants are there, on average, from each department? what is the minimum and maximum?\nwhat is the average employment length for our participants?\nhow many departments are ‘virtual departments’ vs office-based?\n\nwhat is the overall average reported workplace-pride?\nhow much variation in workplace-pride is due to differences between departments?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe first lot of these questions can be answered using things like count(), summary(), table(), mean(), min() etc. See 1: Clustered Data #determining-sample-sizes\nFor the last one, we can use the ICC! See 1: Clustered Data #icc\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. How many respondents do we have, and from how many departments?\n\nnrow(jsup)\n\n[1] 295\n\nlength(table(jsup$dept))\n\n[1] 16\n\n\nHow many respondents are there, on average, from each dept? What is the minimum and maximum number of people in any one department?\n\njsup |&gt;\n  count(dept) |&gt; \n  summarise(min=min(n),\n            max=max(n),\n            median=median(n)\n  )\n\n# A tibble: 1 × 3\n    min   max median\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     5    45     17\n\n\nWhat is the average employment length of respondents?\n\nmean(jsup$employment_length)\n\n[1] 12.6\n\n\nHow many departments are virtual vs office based? This requires a bit more than just table(jsup$virtual), because we are describing a variable at the department level.\n\njsup |&gt; \n  group_by(virtual) |&gt;\n  summarise(\n    ndept = n_distinct(dept)\n  )\n\n# A tibble: 2 × 2\n  virtual ndept\n    &lt;dbl&gt; &lt;int&gt;\n1       0    11\n2       1     5\n\n\nWhat is the overall average ‘workplace-pride’? What is the standard deviation?\n\nmean(jsup$wp)\n\n[1] 25.5\n\nsd(jsup$wp)\n\n[1] 5.27\n\n\nFinally, how much variation in workplace-pride is attributable to department-level differences?\n\nICC::ICCbare(x = dept, y = wp, data = jsup)\n\n[1] 0.439\n\n\n\n\n\n\nQuestion 7\n\n\nWhat if we would like to know whether, when adjusting for differences due to employment length and department and roles, workplace-pride differs between people working in virtual-departments compared to office-based ones?\nCan you add this to the model? What happens?\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Let’s add the virtual predictor to our model. Note that we don’t actually get a coefficient here - it is giving us an NA!\n\nmod4 &lt;- lm(wp ~ employment_length + dept + role + virtual, data = jsup)\n\nsummary(mod4)\n\n\nCall:\nlm(formula = wp ~ employment_length + dept + role + virtual, \n    data = jsup)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.690 -1.404 -0.027  1.178  5.054 \n\nCoefficients: (1 not defined because of singularities)\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        36.3522     0.6310   57.61  &lt; 2e-16 ***\nemployment_length  -0.8817     0.0344  -25.66  &lt; 2e-16 ***\ndeptCMA            -3.7969     0.6490   -5.85  1.4e-08 ***\ndeptCPS            -0.2173     0.7304   -0.30  0.76627    \ndeptFSA             4.7448     0.6245    7.60  4.7e-13 ***\ndeptGLD             0.0582     0.6822    0.09  0.93212    \ndeptHMRC           -3.7859     0.6924   -5.47  1.0e-07 ***\ndeptNCA            -3.8503     0.6549   -5.88  1.2e-08 ***\ndeptNS&I           -0.5737     0.6537   -0.88  0.38095    \ndeptOFGEM          -0.6479     0.7050   -0.92  0.35885    \ndeptOFQUAL         -4.9413     1.0104   -4.89  1.7e-06 ***\ndeptOFSTED         -5.8846     0.6831   -8.61  5.5e-16 ***\ndeptOFWAT          -1.2087     0.6917   -1.75  0.08169 .  \ndeptORR            -2.8452     0.6813   -4.18  4.0e-05 ***\ndeptSFO            -1.3550     0.6719   -2.02  0.04469 *  \ndeptUKSA            4.2820     0.5759    7.43  1.3e-12 ***\ndeptUKSC           -2.3131     0.7325   -3.16  0.00177 ** \nroleB               1.4179     0.3029    4.68  4.5e-06 ***\nroleC               1.3148     0.3663    3.59  0.00039 ***\nvirtual                 NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.98 on 276 degrees of freedom\nMultiple R-squared:  0.867, Adjusted R-squared:  0.859 \nF-statistic:  100 on 18 and 276 DF,  p-value: &lt;2e-16\n\n\nSo what is happening? If we think about it, if we separate out “differences due to departments” then there is nothing left to compare between departments that are virtual vs office based. Adding the between-department predictor of virtual doesn’t explain anything more - the residual sums of squares doesn’t decrease at all:\n\nanova(\n  lm(wp ~ employment_length + dept + role, data = jsup),\n  lm(wp ~ employment_length + dept + role + virtual, data = jsup)\n)\n\nAnalysis of Variance Table\n\nModel 1: wp ~ employment_length + dept + role\nModel 2: wp ~ employment_length + dept + role + virtual\n  Res.Df  RSS Df Sum of Sq F Pr(&gt;F)\n1    276 1084                      \n2    276 1084  0         0         \n\n\nAnother way of thinking about this: knowing the average workplace-pride for the department that someone is in tells me what to expect about that person’s workplace pride. But once I know their department’s average workplace-pride, knowing whether it is ‘virtual’ or ‘office-based’ doesn’t tell me anything new, for the very fact that the virtual/office-based distinction comes from comparing different departments.\nBut we’re not really interested in these departments specifically! What would be nice would be if we can look at the relevant effects of interest (things like role and virtual), but then just think of the department differences as just some sort of random variation. So we want to think of departments in a similar way to how we think of our individual employees - they vary randomly around what we expect - only they’re at a different level of observation."
  },
  {
    "objectID": "03ex.html",
    "href": "03ex.html",
    "title": "W3 Exercises: Nested and Crossed Structures",
    "section": "",
    "text": "Data: gadeduc.csv\nThis is synthetic data from a randomised controlled trial, in which 30 therapists randomly assigned patients (each therapist saw between 2 and 28 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nThe data are available at https://uoepsy.github.io/data/lmm_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\nTable 1: Data Dictionary: lmm_gadeduc.csv\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npatient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n\n\nvisit_0\nScore on the GAD7 at baseline\n\n\nvisit_1\nGAD7 at 1 month assessment\n\n\nvisit_2\nGAD7 at 2 month assessment\n\n\nvisit_3\nGAD7 at 3 month assessment\n\n\nvisit_4\nGAD7 at 4 month assessment\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nUh-oh… these data aren’t in the same shape as the other datasets we’ve been giving you..\nCan you get it into a format that is ready for modelling?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nIt’s wide, and we want it long.\n\nOnce it’s long. “visit_0”, “visit_1”,.. needs to become the numbers 0, 1, …\nOne variable (patient) contains lots of information that we want to separate out. There’s a handy function in the tidyverse called separate(), check out the help docs!\n\n\n\n\n\n\n\n\n\n1 - reshaping\n\n\n\nSolution 1. Here’s the data. We have one row per patient, but we have multiple observations for each patient across the columns..\n\ngeduc = read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\nhead(geduc)\n\n# A tibble: 6 × 6\n  patient      visit_0 visit_1 visit_2 visit_3 visit_4\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 VC_Control_1      24      24      26      29      28\n2 VC_Control_2      24      26      28      29      30\n3 VC_Control_3      25      29      27      29      30\n4 VC_Control_4      24      25      25      26      26\n5 VC_Control_5      28      28      27      29      28\n6 VC_Control_6      26      28      25      27      28\n\n\nWe can make it long by taking the all the columns from visit_0 to visit_4 (that is, from the second column 2 to the last column last_col()) and shoving their values into one variable called GAD, and keeping the name of the column they come from as another variable called visit:\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\")\n\n# A tibble: 2,410 × 3\n   patient      visit     GAD\n   &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt;\n 1 VC_Control_1 visit_0    24\n 2 VC_Control_1 visit_1    24\n 3 VC_Control_1 visit_2    26\n 4 VC_Control_1 visit_3    29\n 5 VC_Control_1 visit_4    28\n 6 VC_Control_2 visit_0    24\n 7 VC_Control_2 visit_1    26\n 8 VC_Control_2 visit_2    28\n 9 VC_Control_2 visit_3    29\n10 VC_Control_2 visit_4    30\n# ℹ 2,400 more rows\n\n\nThis is step 1 of our data wrangling. In the next step, we’ll pipe the result of pivot_longer() into mutate().\nIn general, building up and running your data wrangling pipeline step by step, the way we’re illustrating here, is a good way to make sure each step of your code really is doing what you think it’s doing.\n\n\n\n\n\n2 - time is numeric\n\n\n\nSolution 2. Now we know how to get our data long, we need to sort out our time variable (visit) and make it into numbers.\nWe can replace all occurrences of the string \"visit_\" in our data with nothingness \"\", and then convert what remains—the visit number—to numeric.\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) \n\n# A tibble: 2,410 × 3\n   patient      visit   GAD\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 VC_Control_1     0    24\n 2 VC_Control_1     1    24\n 3 VC_Control_1     2    26\n 4 VC_Control_1     3    29\n 5 VC_Control_1     4    28\n 6 VC_Control_2     0    24\n 7 VC_Control_2     1    26\n 8 VC_Control_2     2    28\n 9 VC_Control_2     3    29\n10 VC_Control_2     4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\n\n3 - splitting up the patient variable\n\n\n\nSolution 3. Finally, we need to sort out the patient variable. It contains 3 bits of information that we will want to have separated out. It has the therapist (their initials), then the group (treatment or control), and then the patient number. These are all separated by an underscore “_“.\nThe separate() function takes a column and separates it into several things (as many things as we give it), splitting them by some user defined separator such as an underscore:\n\ngeduc_long &lt;- geduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |&gt;\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n\nAnd we’re ready to go!\n\ngeduc_long\n\n# A tibble: 2,410 × 5\n   therapist group   patient visit   GAD\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 VC        Control 1           0    24\n 2 VC        Control 1           1    24\n 3 VC        Control 1           2    26\n 4 VC        Control 1           3    29\n 5 VC        Control 1           4    28\n 6 VC        Control 2           0    24\n 7 VC        Control 2           1    26\n 8 VC        Control 2           2    28\n 9 VC        Control 2           3    29\n10 VC        Control 2           4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\nQuestion 2\n\n\nVisualise the data. Does it look like the treatment had an effect over time? Does it look like the treatment worked when used by every therapist?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nremember, stat_summary() is very useful for aggregating data inside a plot.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. Here’s the overall picture. The average score on the GAD7 at each visit gets more and more different between the two groups. The treatment looks effective..\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nLet’s split this up by therapist, so we can see the averages across each therapist’s set of patients.\nThere’s clear variability between therapists in how well the treatment worked. For instance, the therapists EU and OD don’t seem to have much difference between their groups of patients.\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\") +\n  facet_wrap(~therapist)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFit a model to test if the psychoeducational treatment is associated with greater improvement in anxiety over time.\nStep 1: Choose the appropriate fixed effects.\nStep 2: Think about the grouping structure in the data.\nStep 3: Choose the appropriate random effects.\nNote that the patient variable does not uniquely specify the individual patients. That is, patient “1” from therapist “AO” is a different person from patient “1” from therapist “BJ”.\n\n\n\n\n\n1 - fixed effects\n\n\n\nSolution 5. We want to know if how anxiety (GAD) changes over time (visit) is different between treatment and control (group).\nHopefully this should hopefully come as no surprise1 - it’s an interaction!\n\nlmer(GAD ~ visit * group + ...\n       ...\n     data = geduc_long)\n\n\n\n\n\n\n2 - grouping structure\n\n\n\nSolution 6. We have multiple observations for each of the 482 patients, and those patients are nested within 30 therapists.\nNote that in our data, the patient variable does not uniquely specify the individual patients. i.e. patient “1” from therapist “AO” is a different person from patient “1” from therapist “BJ”. To correctly group the observations into different patients (and not ‘patient numbers’), we need to have therapist:patient.\nSo we capture therapist-level differences in ( ... | therapist) and the patients-within-therapist-level differences in ( ... | therapist:patient):\n\nlmer(GAD ~ visit * group + ...\n       ( ... | therapist) + \n       ( ... | therapist:patient),\n     data = geduc_long)\n\n\n\n\n\n\n3 - random effects\n\n\n\nSolution 7. Note that each patient can change differently in their anxiety levels over time - i.e. the slope of visit could vary by participant.\nLikewise, some therapists could have patients who change differently from patients from another therapist, so visit|therapist can be included.\nEach patient is in one of the two groups - they’re either treatment or control. So we can’t say that “differences in anxiety due to treatment varies between patients”, because for any one patient the “difference in anxiety due to treatment” is not defined in our study design.\nHowever, therapists see multiple different patients, some of which are in the treatment group, and some of which are in the control group. So the treatment effect could be different for different therapists!\n\nmod1 &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|therapist:patient),\n             geduc_long)\n\n\n\n\n\n\n4 - interpreting the model\n\n\n\nSolution 8. The question asked whether the psychoeducational treatment (in the variable group) is associated with greater improvement in anxiety (GAD) over time (visit).\nLet’s take a look at the model summary to find out.\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: GAD ~ visit * group + (1 + visit * group | therapist) + (1 +  \n    visit | therapist:patient)\n   Data: geduc_long\n\nREML criterion at convergence: 8696\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7051 -0.5341  0.0147  0.5412  2.9954 \n\nRandom effects:\n Groups            Name                 Variance Std.Dev. Corr             \n therapist:patient (Intercept)          1.448    1.203                     \n                   visit                1.014    1.007    0.07             \n therapist         (Intercept)          1.602    1.266                     \n                   visit                0.211    0.459    -0.09            \n                   groupTreatment       0.082    0.286    -0.13  0.02      \n                   visit:groupTreatment 0.154    0.392    -0.14 -0.20 -0.49\n Residual                               0.706    0.840                     \nNumber of obs: 2410, groups:  therapist:patient, 482; therapist, 30\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)           25.1910     0.2528   99.63\nvisit                 -0.5780     0.1120   -5.16\ngroupTreatment         0.0313     0.1377    0.23\nvisit:groupTreatment  -0.8375     0.1233   -6.79\n\nCorrelation of Fixed Effects:\n            (Intr) visit  grpTrt\nvisit       -0.073              \ngroupTrtmnt -0.278  0.029       \nvst:grpTrtm -0.067 -0.442 -0.160\n\n\nThe question focuses on the fixed effects, so let’s look at those coefficients and their t-values.\n\nvisit: We see a negative association between visit and GAD: the more therapy visits you have - when you are in the control condition - the less your anxiety. The t-value here is more extreme than 2 (which is, as a rule of thumb, approximately the t-value that represents the boundary of the 95% CI). So we likely have a significant negative association between time and anxiety. If this association is significant, that means we can reject the null hypothesis that there’s no change in anxiety over time.\ngroupTreatment: We see a very small positive association between group and GAD, but the error is much bigger than the estimate itself, and consequently, the t-value is also pretty close to 0. I doubt we can reject the null hypothesis that, on average, there’s no difference between the treatment and control groups at baseline (when visit = 0).\nvisit:groupTreatment: We see a negative coefficient for the interaction between visit and groupTreatment, accompanied by a fairly extreme t-value. We likely have a significant negative interaction. This means that we can reject the null hypothesis that there’s no difference between groups as time goes on. And the negative coefficient means that, as visits increase, the GAD of the treatment group decreases more than the GAD of the control group does. (Note: Interactions are REALLY hard to interpret just based on model coefficients. The best way to interpret them is to look at plots of the data.)\n\nTo see whether our guesses about significance based on the t-values were on the right track, we can re-fit the model using lmerTest.\n\nmod1_test &lt;- lmerTest::lmer(\n  GAD ~ visit*group + \n    (1+visit*group|therapist) +\n    (1+visit|therapist:patient),\n  geduc_long\n)\nsummary(mod1_test)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: GAD ~ visit * group + (1 + visit * group | therapist) + (1 +  \n    visit | therapist:patient)\n   Data: geduc_long\n\nREML criterion at convergence: 8696\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7051 -0.5341  0.0147  0.5412  2.9954 \n\nRandom effects:\n Groups            Name                 Variance Std.Dev. Corr             \n therapist:patient (Intercept)          1.448    1.203                     \n                   visit                1.014    1.007    0.07             \n therapist         (Intercept)          1.602    1.266                     \n                   visit                0.211    0.459    -0.09            \n                   groupTreatment       0.082    0.286    -0.13  0.02      \n                   visit:groupTreatment 0.154    0.392    -0.14 -0.20 -0.49\n Residual                               0.706    0.840                     \nNumber of obs: 2410, groups:  therapist:patient, 482; therapist, 30\n\nFixed effects:\n                     Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)           25.1910     0.2528 28.7236   99.63  &lt; 2e-16 ***\nvisit                 -0.5780     0.1120 25.7026   -5.16  2.3e-05 ***\ngroupTreatment         0.0313     0.1377 21.4424    0.23     0.82    \nvisit:groupTreatment  -0.8375     0.1233 18.8876   -6.79  1.8e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) visit  grpTrt\nvisit       -0.073              \ngroupTrtmnt -0.278  0.029       \nvst:grpTrtm -0.067 -0.442 -0.160\n\n\nYes, the effects deemed “significant” based on Satterthwaite’s method are the ones we expected.\nSo yes, it looks like the treatment group does improve more over time, compared to the control group, taking into account all the variability introduced by individual patients and therapists.\n\n\n\n\nQuestion 4\n\n\nFor each of the models below, what is wrong with the random effect structure?\n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. \n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\nThe patient variable doesn’t capture the different patients within therapists, so this actually fits crossed random effects and treats all data where patient==1 as from the same group (even if this includes several different patients’ worth of data from different therapists!)\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\nUsing the / here means we have the same random slopes fitted for therapists and for patients-within-therapists.\nConcretely, (1+visit*group|therapist/patient) is shorthand for (1 + visit*group | therapist) (which is fine) + (1 + visit*group | patient:therapist) (which is not fine, because we don’t have data for every participant for both groups.\nIn other words, the effect of group can’t vary by patient, so this doesn’t work, hence why we need to split them up into (...|therapist)+(...|therapist:patient).\n\n\n\n\nQuestion 5\n\n\nLet’s suppose that I don’t want the psychoeducation treatment, I just want the standard therapy sessions that the ‘Control’ group received. Which therapist should I go to?\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou don’t need to fit a new model here, you can use the one you fitted above.\nranef() and dotplot.ranef.mer() will help! You can read about ranef in Chapter 2 #making-model-predictions.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. The three best therapists to go to are SZ, AO, or IT.\nWhy?\n\nWe said we don’t care about getting the treatment, so we can ignore the parameters groupTreatment and visit:groupTreatment.\n(Intercept) doesn’t tell us about the effect that each therapist has over time, just how good they’re estimated to be at visit 0.\nvisit is the parameter that tells us about how anxiety changes over time. And because lower GAD is better, we want scores to decrease over time.\n\nSo we can look at which therapists have the most negative slope estimated for visit:\n\nranef(mod1)$therapist |&gt;\n  arrange(visit)\n\n   (Intercept)   visit groupTreatment visit:groupTreatment\nSZ      0.9726 -0.6888        0.24166             -0.22320\nAO      1.1433 -0.6187        0.12344             -0.36512\nIT     -1.1505 -0.5899       -0.07236              0.31511\nYS     -0.4870 -0.5669       -0.06644             -0.03350\nGW      1.2185 -0.4012        0.08380             -0.22423\nYF      1.6864 -0.3062       -0.21447              0.27606\nBT      0.3180 -0.2006        0.02727             -0.27354\nCX     -2.0326 -0.1830        0.07968              0.04233\nYE     -0.0753 -0.1195       -0.11639              0.37761\nBJ      0.4484 -0.1189        0.13246             -0.17041\nOD      0.3721 -0.1095       -0.00822              0.01029\nLI      1.2295 -0.1060        0.05089             -0.30193\nWB     -1.3593 -0.0762        0.12760             -0.22965\nXA     -0.5062 -0.0623       -0.03204              0.28860\nOI     -0.2767 -0.0181       -0.21767              0.33782\nTV     -1.9303  0.0405       -0.04086              0.11940\nDF     -0.4995  0.0754        0.12759              0.03544\nDJ     -1.4707  0.0854        0.06194              0.02057\nPM      0.2754  0.1224       -0.02785              0.04959\nMV      0.9641  0.1595       -0.01262              0.15925\nOE     -0.2371  0.1873        0.13168              0.01462\nRW      0.3232  0.2004        0.10068             -0.31089\nLO      1.8762  0.2146       -0.05419              0.00335\nMY     -0.5791  0.2166       -0.02693              0.08901\nKD      0.4017  0.2669        0.01437             -0.36765\nEU     -1.3717  0.3149       -0.03285              0.26114\nKI      2.7613  0.3277       -0.29719              0.17487\nCS     -1.2525  0.4870       -0.00583             -0.00779\nXQ     -1.3365  0.5703       -0.27314              0.33068\nVC      0.5744  0.8969        0.19601             -0.39784\n\n\nDouble check using the plot:\n\ndotplot.ranef.mer(ranef(mod1))$therapist\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nRecreate this plot.\nThe faint lines represent the model estimated lines for each patient. The points and ranges represent our fixed effect estimates and their uncertainty.\nMake sure you’re plotting model estimates, not the raw data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nyou can get the patient-specific lines using augment() from the broom.mixed package, and the fixed effects estimates using effect() from the effects package.\nremember that the “patient” column doesn’t group observations into unique patients.\nremember you can pull multiple datasets into ggplot:\n\n\nggplot(data = dataset1, aes(x=x,y=y)) + \n  geom_point() + # points from dataset1\n  geom_line(data = dataset2) # lines from dataset2\n\n\nsee more in Chapter 2 #visualising-models\n\n\n\n\n\n\n\n\n\n1 - the relevant parts\n\n\n\nSolution 11. The effects package will give us the fixed effect estimates:\n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\nWe want to get the fitted values for each patient. We can get fitted values using augment(). But the patient variable doesn’t capture the unique patients, it just captures their numbers (which aren’t unique to each therapist).\nSo we can create a new column called upatient which pastes together the therapists initials and the patient numbers\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  )\n\n# A tibble: 2,410 × 17\n     GAD visit group   therapist patient upatient .fitted .resid  .hat .cooksd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1    24     0 Control VC        1       VC1         24.2 -0.198 0.454 0.0210 \n 2    24     1 Control VC        1       VC1         25.3 -1.28  0.239 0.239  \n 3    26     2 Control VC        1       VC1         26.4 -0.360 0.186 0.0128 \n 4    29     3 Control VC        1       VC1         27.4  1.56  0.294 0.508  \n 5    28     4 Control VC        1       VC1         28.5 -0.522 0.563 0.284  \n 6    24     0 Control VC        2       VC2         24.8 -0.843 0.454 0.383  \n 7    26     1 Control VC        2       VC2         26.2 -0.171 0.239 0.00426\n 8    28     2 Control VC        2       VC2         27.5  0.502 0.186 0.0250 \n 9    29     3 Control VC        2       VC2         28.8  0.174 0.294 0.00633\n10    30     4 Control VC        2       VC2         30.2 -0.153 0.563 0.0246 \n# ℹ 2,400 more rows\n# ℹ 7 more variables: .fixed &lt;dbl&gt;, .mu &lt;dbl&gt;, .offset &lt;dbl&gt;, .sqrtXwt &lt;dbl&gt;,\n#   .sqrtrwt &lt;dbl&gt;, .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;\n\n\n\n\n\n\n\n2 - constructing the plot\n\n\n\nSolution 12. \n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  ) |&gt;\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")"
  },
  {
    "objectID": "03ex.html#footnotes",
    "href": "03ex.html#footnotes",
    "title": "W3 Exercises: Nested and Crossed Structures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif it does, head back to where we learned about interactions in the single level regressions lm(). It’s just the same here.↩︎"
  },
  {
    "objectID": "05ex.html",
    "href": "05ex.html",
    "title": "W5 Exercises: Bringing it all together",
    "section": "",
    "text": "Take your pick!\n\nQuestion 1\n\n\nYou can find all the datasets that we have seen (and more!) as an additional doc in the readings page.\nFor each one, there is a quick explanation of the study design which also details the research aims of the project.\nPick one of the datasets and, in your groups:\n\nexplore the data, and do any required cleaning (most of them are clean already)\nconduct an analysis to address the research aims\nwrite a short description of the sample data (see Chapter 11 #the-sample-data)\nwrite a short explanation of your methods (see Chapter 11 #the-methods)\nwrite a short summary of your results, along with suitable visualisations and tables (see Chapter 11 #the-results)\nPost some of your writing on Piazza and we can collectively discuss it!\n\n\nEach of the datasets contains some tags that give an indication of the type of study. Anything with either “#binomial-outcome” or “#non-linear” you can ignore as we have not covered this in DAPR3.\n\n\n\n\n\n\n\n\n\n\n\nFlashcards: lm to lmer\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\nStudy tip: Choose a topic in one of the boxes below, but don’t expand the box yet. Instead, grab a pen and paper, and first spend two minutes writing down everything you can think of about that topic. After those two minutes, you can expand the box and see if you covered the key information. Write down anything you missed. Another day, try writing about the same topic again, and see if you can include anything you missed the first time.\nWhy does this work? Research on learning has shown that if you test yourself at recalling ideas (and especially if you repeatedly test yourself across several days or weeks), you’ll get better at remembering and understanding these ideas in the long term.\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\n\nFormula:\n\n\\(y_i = b_0 + b_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + (which represents the intercept, \\(b_0\\)) is always there, even if we don’t explicitly write it. (It’s possible to tell R to remove it, e.g., by explicitly writing 0 + predictor, but realistically you probably won’t ever want to fit a model with no intercept.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-level data\n\n\n\n\n\nIf our dataset contains multiple observations from the same subject, for example, or multiple observations of the same stimulus, then those data points are not independent. For example:\n\nSubject A may behave a particular consistent way, and Subject B may behave a different particular consistent way. So, one data point from Subject A will be closer to the other data points from Subject A than it will be to the data points from Subject B.\nSimilarly, Stimulus A may provoke a particular reaction, while Stimulus B may provoke a different particular reaction. So a data point from Stimulus A will look more like the other data from Stimulus A than it will to the data from Stimulus B.\n\nThis is one way for data points to be non-independent. But a simple regression assumes independence. So, we need some extra way to take this non-independence into account. And we do that using random effects.\nIn other words: when our data is ‘clustered’ or ‘grouped’ such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nTo illustrate how a simple regression falls short: If we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nBy including a random-intercept term, we are letting our model estimate some variability around the average intercept parameter. The variability reflects how the intercept should be adjusted (nudged up or down) to better suit the data from different members of each group.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = b_{0i} + b_{1} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(b_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\n(“Random intercepts” are called “random” because the adjustments to the intercept are modelled as random samples from an underlying normal distribution with mean 0 and standard deviation \\(\\sigma_0\\).)\n\nRemember, variance is just standard deviation squared!\n\nWe can now see that the intercept estimate \\(b_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect (that is, an adjustment, a nudge) for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has been adjusted for each subject.\nEach subject’s line has been moved up or down. The intercepts can be different, but the slopes are all the same.\n\n\n\n\n\n\n\n\n\nAnother way of comparing the estimates of the random intercept model is to plot all the by-subject lines (the colourful lines) and overlay the fixed effect line (the thick green line):\n\n\n\n\n\n\n\n\n\nHere, we see that the green fixed effect line is the average of all the by-subject lines, and that all the by-subject lines have the exact same slope as the fixed effect line—they’ve just been nudged up or down.\n\n\n\n\n\n\n\n\n\nPooling & Shrinkage\n\n\n\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different. The green lines are the predictions from the model with a random intercept by subject. The purple lines (sometimes hidden behind the green one) are the predictions from the model with a fixed effect of subject.\n\n\n\n\n\n\n\n\n\nWhy is the purple line for sub_373 below the green line? Why is the purple line for sub_374 above the green line? The short answer: “shrinkage”. The green lines (which come from the random intercept model) are “shrunk” closer to the average line than the purple lines (which come from the fixed effect model).\n\nIn general, when we use multilevel models, the estimates for each member of a group are shrunk toward the average. (How much they are shrunk by depends on how variable the members of a group are, as well as how many data points each member contributes.)\nShrinkage is a benefit of multilevel models for a couple reasons:\n\nShrinkage makes our estimates a bit more conservative (it’s harder to get extreme estimates when they are shrunk toward the average).\nShrinkage means that we can make a more educated guess about the estimates for members of a group, based on the estimates from everyone else in the group.\n\nYou could think of it like this: In the example above, the multilevel model “borrows information” or “borrows strength” from subjects sub_352, sub_369, and all the others, to inform its estimates for sub_373 and about sub_374. “Borrowing information” or “borrowing strength” also gets termed “partial pooling”, because we are partially combining (that is, partially pooling) all the information across members of a group to get an average, but still allowing those group members to vary a little.\n\n\n\n\n\n\n\n\nmodel\npooling\nexplanation\n\n\n\n\nlm(y~x)\ncomplete pooling\nall information from all members of a group is combined (pooled) together, and a line is fit that doesn’t take grouping into account at all\n\n\nlm(y~group + x)\nno pooling\ninformation is divided between the members of the group, and differnences between group members are estimated. Observations from group \\(i\\) contribute only to estimates about group \\(i\\)\n\n\nlmer(y~x+(1\ngroup))\npartial pooling\n\n\n\n\n\n\n\n\n\n\n\n\nRandom slopes\n\n\n\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = b_{0i} + b_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(b_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(b_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\n(Like random intercepts, “random slopes” are called “random” because the adjustments to the slope are modelled as random samples from an underlying normal distribution with mean 0 and standard deviation \\(\\sigma_1\\).)\n\nRemember, variance is just standard deviation squared!\n\nAs with the intercept \\(b_{0i}\\), the slope of the predictor \\(b_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel parameters: Fixed effects\n\n\n\n\n\nFixed effects are the parameters estimated for the average group, around which the random effects can vary.\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the members of the group all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n    405.790      -0.672 \n\n\n\n\n\n\n\n\n\n\n\nModel parameters: Variance components\n\n\n\n\n\nVariance components are the variances and covariances of the random effects. A multilevel model estimates variance components as well as fixed effects.\nThe reason we call random effects “random” is because the adjustments to the fixed effects are modelled as random samples from an underlying normal distribution. The width of these distributions represents how much the group tends to deviate from each fixed effect. We can think of the width of these distributions as the variance components.\n\n\n\n\n\n\n\n\n\n\nWe can extract these using the VarCorr() function, and we can also see them in the “random effects” part of the summary() output from a model.\nLooking at these variance components, we can ask:\n\nHow much do members of a group vary around the fixed intercept? And around the fixed slope? (This is answered by the standard deviation.)\nDo members of a group with higher intercepts also have higher slopes? (This is the correlation between random intercept and random slope.)\n\n\nVarCorr(random_slopes_model)\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 72.72         \n          x1           1.36    -0.35\n Residual             25.74         \n\n\n\nRemember, variance is just standard deviation squared!\n\n\n\n\n\n\n\n\n\n\nExtracting group-member-specific random effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can extract each group member’s deviations from the group average (i.e., from the fixed effect) using the ranef() function.\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)      x1\nsub_308       31.33 -1.4400\nsub_309      -28.83  0.4184\nsub_310        2.71  0.0599\nsub_330       59.40  0.3853\nsub_331       74.96  0.1739\nsub_332       91.09 -0.2346\nsub_333       97.85 -0.1906\nsub_334      -54.19 -0.5585\nsub_335      -16.90  0.9207\nsub_337       52.22 -1.1660\nsub_349      -67.76 -0.6844\nsub_350       -5.82 -1.2379\nsub_351       61.20  0.0550\nsub_352       -7.91 -0.6650\nsub_369      -47.64 -0.4681\nsub_370      -33.12 -1.1100\nsub_371       77.58 -0.2040\nsub_372      -36.39 -0.4583\nsub_373     -197.58  1.7990\nsub_374      -52.20  4.6051\n\nwith conditional variances for \"subject\" \n\n\nWhat do these numbers mean?\nThese are the differences between the line that fits each subject’s data and the line defined by the fixed intercept and the fixed slope. So the first entry, the line that fits the data from sub_308 has an intercept that’s 31.33 higher than the fixed intercept and a slope that is 1.44 below the fixed slope.\nIf we wanted to find the parameters of each subject’s line, we could compute them with this information and the model’s fixed effects. But the next drop-down box shows us how to find those parameters more simply.\n\n\n\n\n\n\n\n\n\n\n\nExtracting group-member-specific coefficients\n\n\n\n\n\nWe can see the estimated intercept and slope for each subject specifically, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)     x1\nsub_308         437 -2.112\nsub_309         377 -0.254\nsub_310         409 -0.612\nsub_330         465 -0.287\nsub_331         481 -0.498\nsub_332         497 -0.907\nsub_333         504 -0.863\nsub_334         352 -1.231\nsub_335         389  0.248\nsub_337         458 -1.838\nsub_349         338 -1.357\nsub_350         400 -1.910\nsub_351         467 -0.617\nsub_352         398 -1.337\nsub_369         358 -1.140\nsub_370         373 -1.782\nsub_371         483 -0.876\nsub_372         369 -1.131\nsub_373         208  1.127\nsub_374         354  3.933\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nLet’s confirm that the outcome of coef() is the same as taking the fixed effects and then adding each subject’s random effects (as seen in the previous drop-down box).\n\ncbind(\n  int = fixef(random_slopes_model)[1] + \n    ranef(random_slopes_model)$subject[,1],\n  slope = fixef(random_slopes_model)[2] + \n    ranef(random_slopes_model)$subject[,2]\n)\n\n      int  slope\n [1,] 437 -2.112\n [2,] 377 -0.254\n [3,] 409 -0.612\n [4,] 465 -0.287\n [5,] 481 -0.498\n [6,] 497 -0.907\n [7,] 504 -0.863\n [8,] 352 -1.231\n [9,] 389  0.248\n[10,] 458 -1.838\n[11,] 338 -1.357\n[12,] 400 -1.910\n[13,] 467 -0.617\n[14,] 398 -1.337\n[15,] 358 -1.140\n[16,] 373 -1.782\n[17,] 483 -0.876\n[18,] 369 -1.131\n[19,] 208  1.127\n[20,] 354  3.933\n\n\nYes, both of these outcomes are the same.\n\n\n\n\n\n\n\n\n\nPlotting random effects\n\n\n\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms &lt;- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssumptions, Influence\n\n\n\n\n\nFor a simple linear model, \\(\\color{red}{y} = \\color{blue}{b_0 + b_1(x)} \\color{black}{+ \\varepsilon}\\), we distinguish between the systematic model part (\\(b_0 + b_1(x)\\)) and the errors, which randomly vary around the systematic model (\\(\\varepsilon\\)). In words, we can summarise the linear model expression as \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}}\\) \\(+ \\text{error}\\).\nIn the multi-level model, another source of error (i.e., of variation) come from our random effects: \\(\\color{red}{\\text{outcome}}\\) = \\(\\color{blue}{\\text{model}}\\) \\(+ \\text{group-error} + \\text{individual-error}\\). As such, random effects are another form of residual. Our assumptions of “zero mean, constant variance” apply at both levels of residuals (see Figure 1).\n\n\n\n\n\n\n\n\nFigure 1: The black dashed lines show our model assumptions. We assume that the intercept adjustments are normally distributed (the large normal distribution at the y axis), and we further assume that the residuals between the individual data points and the subject-level line are also normally distributed with equal variance across the range of the predictor (the smaller normal distributions around one of the subject-level lines).\n\n\n\n\n\n\nWe can assess these normality of both resid(model) and ranef(model) by constructing plots using functions such as hist(), qqnorm() and qqline().\n\nWe can also use plot(model, type=c(\"p\",\"smooth\")) to give us our residuals vs fitted plot (smooth line should be horizontal at approx zero, showing zero mean).\n\nplot(model, form = sqrt(abs(resid(.))) ~ fitted(.), type = c(\"p\",\"smooth\")) will give us our scale-location plot (smooth line should be horizontal, showing constant variance).\n\nWe can also use the check_model() function from the performance package to get lots of info at once:\n\nlibrary(performance)\ncheck_model(random_slopes_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\nTo get p-values for our coefficients, there are lots of different ways (see Optional Chapter 3 if you’re interested).\nFor DAPR3, we are recommending using the “Satterthwaite” method, which can be done by re-fitting the model using the lmerTest package:\n\nrandom_slopes_model2 &lt;- lmerTest::lmer( outcome ~ 1 + x1 + (1+x1|subject), data=dat)\nsummary(random_slopes_model2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: outcome ~ 1 + x1 + (1 + x1 | subject)\n   Data: dat\n\nREML criterion at convergence: 1862\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.069 -0.417 -0.014  0.431  5.226 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n subject  (Intercept) 5287.68  72.72         \n          x1             1.86   1.36    -0.35\n Residual              662.33  25.74         \nNumber of obs: 185, groups:  subject, 20\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)  405.790     16.666  18.045   24.35    3e-15 ***\nx1            -0.672      0.313  16.757   -2.15    0.047 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n   (Intr)\nx1 -0.370\n\n\nIf we want to test multiple coefficients at once, then we can conduct model comparisons by doing a likelihood ratio test. We start with a full model, and then create a more restricted comparison model by removing the bits that we want to test.\nIn the example below, the full model is outcome ~ 1 + x1 + x2 + (1 + x1 | subject). If we wanted to test whether the predictors x1 and x2 contribute significantly to the model, then we would create a restricted model for comparison by removing those terms. That gives us outcome ~ 1 + (1 + x1 | subject).\nYou’ll notice that x1 still appears in the random effect: we’re still getting a random slope over x1 by subject, even though x1 is no longer in the fixed effects. This looks a little weird, but when doing model comparison, it’s OK to have random slopes of predictors that aren’t in the fixed effects.\n\nmodel2 &lt;- lmer( outcome ~ 1 + x1 + x2+ (1+x1|subject), data=dat)\nmodel2.0 &lt;- lmer( outcome ~ 1 + (1+x1|subject), data=dat)\n\nanova(model2.0, model2)\n\n\n\n\n\n\n\n\n\n\nVisualising group-member-specific fitted values\n\n\n\n\n\nThe model fitted (or “model predicted”) values can be obtained a couple ways.\n\npredict() returns just the predicted values, which is a little inconvenient because you’d have to map them back to the original data that you gave to the model.\nbroom.mixed::augment() returns the predicted values as well as the data that you gave to the model.\n\nWe would typically like to plot the fitted values for each member of the grouping variable (e.g., for each subject):\n\nlibrary(broom.mixed)\naugment(random_slopes_model) |&gt;\n  ggplot(aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising fixed effects along with the group-member-specific fitted values\n\n\n\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. A good option is to use the effects package to construct a dataframe of the linear prediction across the values of a predictor, plus confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\nWe might then want to combine this with our plot of fitted values to make a plot that shows both the estimates for the average group (this is the fixed effects part) and the amount to which groups vary around that average (this we can see with the fitted values plot)\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\n\naugment(random_slopes_model) |&gt;\n  ggplot(aes(x=x1))+\n  geom_line(aes(y=.fitted,group=subject), alpha=.1) + \n  geom_line(data = ef, aes(y=fit))+\n  geom_ribbon(data = ef, aes(y=fit,ymin=lower,ymax=upper), \n              col=\"red\", fill=\"red\",alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested variables\n\n\n\n\n\nIn the following schematic, we have observations from several children, who are each from a particular school.\n\n\n\n\n\n\n\n\n\nEvery school contains multiple children, and each child appears only in a single school. So, the child variable is “nested” within the school variable.\nIn R, we specify random effects for nested variables using the syntax\n(1 | school) + (1 | child:school)\nor\n(1 | school) + (1 | school:child)\nThe specific order of the variables around the colon symbol doesn’t matter. (But if you think about nesting like “child within school”, then maybe the child:school order is more intuitive.)\n\n\n\n\n\n\n\n\n\nCrossed variables\n\n\n\n\n\nIn the following schematic, we have observations from several participants who have each taken part in several tasks.\n\n\n\n\n\n\n\n\n\nBecause we have data from every participant for every task, we describe the participant and task variables as “fully crossed”.\n\nIn contrast: the following schematic shows observations from several participants, who have each only done a subset of the tasks.\n\n\n\n\n\n\n\n\n\nWe have some data from every participant, and we have some data from every task, but it’s not the case that every participant took part in every task. In this case, we describe the participant and task variables as “partially crossed” or “not fully crossed”.\n\nIn R, we specify random effects for both fully crossed and partially crossed variables as follows:\n(1 | ppt) + (1 | task)\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\n\n\n\n\nMLM allows us to model effects in the linear model as varying for different groups.\nImportant terminology note: in this box, “group” is used to mean “a single member of a grouping variable”, e.g., an individual participant in a study, whereas in the flash cards above, “group” was used to mean the grouping variable itself, e.g., participant.\nThe coefficients we remember from simple linear models (the \\(b\\)’s) are now modelled as a distribution: their mean is the effect for the average group, and the variance defines how much the groups vary around the mean. We can see this in Figure 2, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 2 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\n\n\n\nFigure 2: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(b\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{b_{0i} \\cdot 1 + b_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{b_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{b_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is $= ({0i}, {1i}):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho_{01}1 \\\\\n        \\rho_{01} & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\nWe fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\nlibrary(lme4)\nm1 &lt;- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 638\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4945 -0.5722 -0.0135  0.6254  2.3912 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.262    1.504        \n          x           0.796    0.892    0.55\n Residual             4.367    2.090        \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    1.726      0.967    1.78\nx              1.151      0.297    3.88\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)"
  },
  {
    "objectID": "08ex.html",
    "href": "08ex.html",
    "title": "W8 Exercises: EFA",
    "section": "",
    "text": "Data: Conduct Problems\nA researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours:\n\nBreaking curfew\nVandalism\nSkipping school\nBullying\nSpreading malicious rumours\nFighting\nLying\nUsing a weapon\nStealing\nThreatening others\n\nOur task is to use the dimension reduction techniques we learned about in the lecture to help inform how to organise the items she has developed into subscales.\nThe data can be found at https://uoepsy.github.io/data/conduct_probs_scale.csv\n\n\nQuestion 1\n\n\nRead in the dataset.\nCreate a correlation matrix for the items, and inspect the items to check their suitability for exploratory factor analysis\n\n\n\n\n\n\nHints\n\n\n\n\n\nTake a look at Reading 9# Initial Checks.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\ncpdata &lt;- read.csv(\"https://uoepsy.github.io/data/conduct_probs_scale.csv\")\n# discard the first column\ncpdata &lt;- cpdata[,-1]\n\nHere’s a correlation matrix. There’s no obvious blocks of items here, but we can see that there are some fairly high correlations, as well as some weaker ones. All are positive.\n\nheatmap(cor(cpdata))\n\n\n\n\n\n\n\n\nThe Bartlett’s test comes out with a p-value of 0 (which isn’t possible, but it’s been rounded for some reason). This suggests that we reject the null of this test (that our correlation matrix is proportional to the identity matrix). This is good. It basically means “we have some non-zero correlations”!\n\nlibrary(psych)\ncortest.bartlett(cor(cpdata), n=450)\n\n$chisq\n[1] 2238\n\n$p.value\n[1] 0\n\n$df\n[1] 45\n\n\nThe overall sampling adequacy is 0.87, which is pretty good! (or rather, which is ‘meritorious’!). MSA for all items is &gt;.8\n\nKMO(cpdata)  \n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cpdata)\nOverall MSA =  0.87\nMSA for each item = \n   breaking_curfew          vandalism    skipping_school           bullying \n              0.84               0.88               0.92               0.82 \n spreading_rumours           fighting              lying       using_weapon \n              0.81               0.94               0.88               0.95 \n          stealing threatening_others \n              0.90               0.94 \n\n\nFinally, all the relationships here look fairly linear:\n\npairs.panels(cpdata)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nHow many dimensions should be retained?\nThis question can be answered in the same way as we did for PCA - use a scree plot, parallel analysis, and MAP test to guide you.\n\n\n\n\n\nScree\n\n\n\nSolution 2. The scree plot shows a kink at 3, which suggests retaining 2 components.\n\nscree(cpdata)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAP\n\n\n\nSolution 3. The MAP suggests retaining 2 factors. I’m just extracting the actual map values here to save having to show all the other output. We can see that the 2nd entry is the smallest:\n\nVSS(cpdata, plot = FALSE, n = ncol(cpdata))$map\n\n [1] 0.1058 0.0338 0.0576 0.1035 0.1494 0.2520 0.3974 0.4552 1.0000     NA\n\n\n\n\n\n\n\nParallel Analysis\n\n\n\nSolution 4. Parallel analysis suggests 2 factors as well:\n\nfa.parallel(cpdata, fa = \"both\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  2 \n\n\n\n\n\n\n\nMaking a decision\n\n\n\nSolution 5. Again, a quite clear picture that 2 factors is preferred:\n\n\n\n\n\n\n\n\nguides\nsuggestion\n\n\n\n\nScree\n2\n\n\nMAP\n2\n\n\nParallel Analysis\n2\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nUse the function fa() from the psych package to conduct and EFA to extract 2 factors (this is what we suggest based on the various tests above, but you might feel differently - the ideal number of factors is subjective!). Use a suitable rotation (rotate = ?) and extraction method (fm = ?).\n\n\n\n\n\n\nHints\n\n\n\n\n\nWould you expect factors to be correlated? If so, you’ll want an oblique rotation.\nSee R9#doing-an-efa.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. For example, you could choose an oblimin rotation to allow factors to correlate. Let’s use MLE as the estimator.\n\nconduct_efa &lt;- fa(cpdata, nfactors=2, rotate='oblimin', fm=\"ml\")\n\n\n\n\n\nQuestion 4\n\n\nInspect your solution. Make sure to look at and think about the loadings, the variance accounted for, and the factor correlations (if estimated).\n\n\n\n\n\n\nHints\n\n\n\n\n\nJust printing an fa object:\n\nmyfa &lt;- fa(data, ..... )\nmyfa\n\nWill give you lots and lots of information.\nYou can extract individual parts using:\n\nmyfa$loadings for the loadings\nmyfa$Vaccounted for the variance accounted for by each factor\nmyfa$Phi for the factor correlation matrix\n\nYou can find a quick guide to reading the fa output here: efa_output.pdf.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Things look pretty good here. Each item has a clear primary loading on to one of the factors, and the complexity for all items is 1 (meaning they’re clearly link to just one of the factors). The h2 column is showing that the 2 factor solution is explaining 39%+ of the variance in each item. Both factors are well determined, having a at least 3 salient loadings.\nThe 2 factors together explain 57% of the variance in the data - both factors explain a similar amount (29% for factor 1, 28% for factor 2).\nWe can also see that there is a moderate correlation between the two factors. Use of an oblique rotation was appropriate - if the correlation had been very weak, then it might not have differed much from if we used an orthogonal rotation.\n\nconduct_efa\n\n\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                     ML1   ML2   h2   u2 com\nbreaking_curfew    -0.05  0.88 0.74 0.26   1\nvandalism           0.05  0.69 0.51 0.49   1\nskipping_school    -0.01  0.67 0.44 0.56   1\nbullying            0.90  0.00 0.81 0.19   1\nspreading_rumours   0.93 -0.02 0.85 0.15   1\nfighting            0.65 -0.02 0.42 0.58   1\nlying               0.02  0.77 0.60 0.40   1\nusing_weapon        0.63  0.09 0.45 0.55   1\nstealing            0.04  0.70 0.51 0.49   1\nthreatening_others  0.62 -0.01 0.38 0.62   1\n\n                       ML1  ML2\nSS loadings           2.91 2.80\nProportion Var        0.29 0.28\nCumulative Var        0.29 0.57\nProportion Explained  0.51 0.49\nCumulative Proportion 0.51 1.00\n\n With factor correlations of \n     ML1  ML2\nML1 1.00 0.43\nML2 0.43 1.00\n\nMean item complexity =  1\n\n\n\n\n\n\nQuestion 5\n\n\nLook back to the description of the items, and suggest a name for your factors based on the patterns of loadings.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo sort the loadings, you can use\n\nprint(myfa$loadings, sort = TRUE)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. You can inspect the loadings using:\n\nprint(conduct_efa$loadings, sort=TRUE)\n\n\nLoadings:\n                   ML1    ML2   \nbullying            0.899       \nspreading_rumours   0.931       \nfighting            0.653       \nusing_weapon        0.629       \nthreatening_others  0.621       \nbreaking_curfew            0.878\nvandalism                  0.694\nskipping_school            0.671\nlying                      0.767\nstealing                   0.696\n\n                 ML1   ML2\nSS loadings    2.890 2.782\nProportion Var 0.289 0.278\nCumulative Var 0.289 0.567\n\n\nWe can see that, ordered like this, we have five items that have high loadings for one factor and another five items that have high loadings for the other.\nThe five items for factor 2 all have in common that they are non-aggressive forms of conduct problems. The five items for factor 1 are all more aggressive behaviours. We could, therefore, label our factors: ‘aggressive’ and ‘non-aggressive’ conduct problems.\n\n\n\n\nQuestion 6\n\n\nCompare three different solutions:\n\nyour current solution from the previous questions\none where you fit 1 more factor\none where you fit 1 fewer factors\n\nWhich one looks best?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’re looking here to assess:\n\nhow much variance is accounted for by each solution\ndo all factors load on 3+ items at a salient level?\n\ndo all items have at least one loading at a salient level?\nare there any “Heywood cases” (communalities or standardised loadings that are &gt;1)?\nshould we perhaps remove some of the more complex items?\nis the factor structure (items that load on to each factor) coherent, and does it make theoretical sense?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. The 1-factor model explains 37% of the variance (as opposed to the 57% explained by the 2 factor solution), and all items load fairly high on the factor. The downside here is that we’re not discerning between different types of conduct problems that we did in the 2 factor solution.\n\nconduct_1 &lt;- fa(cpdata, nfactors=1, fm=\"ml\")\nconduct_1\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 1, fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML1   h2   u2 com\nbreaking_curfew    0.42 0.18 0.82   1\nvandalism          0.42 0.18 0.82   1\nskipping_school    0.35 0.13 0.87   1\nbullying           0.89 0.79 0.21   1\nspreading_rumours  0.90 0.81 0.19   1\nfighting           0.64 0.41 0.59   1\nlying              0.43 0.18 0.82   1\nusing_weapon       0.68 0.46 0.54   1\nstealing           0.42 0.17 0.83   1\nthreatening_others 0.61 0.38 0.62   1\n\n                ML1\nSS loadings    3.69\nProportion Var 0.37\n\nMean item complexity =  1\nTest of the hypothesis that 1 factor is sufficient.\n\ndf null model =  45  with the objective function =  5.03 with Chi Square =  2238\ndf of  the model are 35  and the objective function was  1.78 \n\nThe root mean square of the residuals (RMSR) is  0.19 \nThe df corrected root mean square of the residuals is  0.22 \n\nThe harmonic n.obs is  450 with the empirical chi square  1465  with prob &lt;  1.9e-285 \nThe total n.obs was  450  with Likelihood Chi Square =  789  with prob &lt;  3.4e-143 \n\nTucker Lewis Index of factoring reliability =  0.557\nRMSEA index =  0.219  and the 90 % confidence intervals are  0.206 0.232\nBIC =  575\nFit based upon off diagonal values = 0.8\nMeasures of factor score adequacy             \n                                                   ML1\nCorrelation of (regression) scores with factors   0.96\nMultiple R square of scores with factors          0.92\nMinimum correlation of possible factor scores     0.84\n\n\nThe 3-factor model explains 60% of the variance (only 3% more than the 2-factor model). Notably, the third factor is not very clearly defined - it only has 1 salient loading (possibly 2 if we consider the 0.3 to be salient, but that item is primarily loaded on the 2nd factor).\n\nconduct_3 &lt;- fa(cpdata, nfactors=3, rotate='oblimin', fm=\"ml\")\nconduct_3\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 3, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                     ML1   ML2   ML3   h2   u2 com\nbreaking_curfew    -0.02  0.61  0.31 0.71 0.29 1.5\nvandalism           0.06  0.12  0.74 0.72 0.28 1.1\nskipping_school     0.00  0.56  0.14 0.44 0.56 1.1\nbullying            0.90  0.09 -0.10 0.82 0.18 1.0\nspreading_rumours   0.92 -0.02  0.03 0.85 0.15 1.0\nfighting            0.65 -0.13  0.14 0.43 0.57 1.2\nlying               0.02  0.85 -0.06 0.67 0.33 1.0\nusing_weapon        0.63  0.08  0.02 0.45 0.55 1.0\nstealing            0.06  0.69  0.02 0.53 0.47 1.0\nthreatening_others  0.62 -0.08  0.09 0.39 0.61 1.1\n\n                       ML1  ML2  ML3\nSS loadings           2.93 2.14 0.94\nProportion Var        0.29 0.21 0.09\nCumulative Var        0.29 0.51 0.60\nProportion Explained  0.49 0.36 0.16\nCumulative Proportion 0.49 0.84 1.00\n\n With factor correlations of \n     ML1  ML2  ML3\nML1 1.00 0.39 0.32\nML2 0.39 1.00 0.68\nML3 0.32 0.68 1.00\n\nMean item complexity =  1.1\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  45  with the objective function =  5.03 with Chi Square =  2238\ndf of  the model are 18  and the objective function was  0.02 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  450 with the empirical chi square  3.98  with prob &lt;  1 \nThe total n.obs was  450  with Likelihood Chi Square =  10.5  with prob &lt;  0.91 \n\nTucker Lewis Index of factoring reliability =  1.01\nRMSEA index =  0  and the 90 % confidence intervals are  0 0.016\nBIC =  -99.5\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML1  ML2  ML3\nCorrelation of (regression) scores with factors   0.96 0.93 0.88\nMultiple R square of scores with factors          0.93 0.86 0.77\nMinimum correlation of possible factor scores     0.85 0.72 0.53\n\n\n\n\n\n\nQuestion 7\n\n\nWrite a brief paragraph or two that summarises your method and the results from your chosen optimal factor structure for the 10 conduct problems.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWrite about the process that led you to the number of factors. Discuss the patterns of loadings and provide definitions of the factors.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. The main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description).\nAn example summary would be:\n\nFirst, the data were checked for their suitability for factor analysis. Normality was checked using visual inspection of histograms, linearity was checked through the inspection of the linear and lowess lines for the pairwise relations of the variables, and factorability was confirmed using a KMO test, which yielded an overall KMO of \\(.87\\) with no variable KMOs \\(&lt;.50\\). An exploratory factor analysis was conducted to inform the structure of a new conduct problems test. Inspection of a scree plot alongside parallel analysis (using principal components analysis; PA-PCA) and the MAP test were used to guide the number of factors to retain. All three methods suggested retaining two factors; however, a one-factor and three-factor solution were inspected to confirm that the two-factor solution was optimal from a substantive and practical perspective, e.g., that it neither blurred important factor distinctions nor included a minor factor that would be better combined with the other in a one-factor solution. These factor analyses were conducted using maximum likelihood estimation and (for the two- and three-factor solutions) an oblimin rotation, because it was expected that the factors would correlate. Inspection of the factor loadings and correlations reinforced that the two-factor solution was optimal: both factors were well-determined, including 5 loadings \\(&gt;|0.3|\\) and the one-factor model blurred the distinction between different forms of conduct problems. The factor loadings are provided in Table 11. Based on the pattern of factor loadings, the two factors were labelled ‘aggressive conduct problems’ and ‘non-aggressive conduct problems’. These factors had a correlation of \\(r=.43\\). Overall, they accounted for 57% of the variance in the items, suggesting that a two-factor solution effectively summarised the variation in the items.\n\n\n\n\nTable 1: Factor Loadings\n\n\n\n\n\n\n\nML1\nML2\n\n\n\n\nspreading_rumours\n0.93\n\n\n\nbullying\n0.90\n\n\n\nfighting\n0.65\n\n\n\nusing_weapon\n0.63\n\n\n\nthreatening_others\n0.62\n\n\n\nvandalism\n\n0.88\n\n\nstealing\n\n0.77\n\n\nlying\n\n0.70\n\n\nskipping_school\n\n0.69\n\n\nbreaking_curfew\n\n0.67"
  },
  {
    "objectID": "08ex.html#footnotes",
    "href": "08ex.html#footnotes",
    "title": "W8 Exercises: EFA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou should provide the table of factor loadings. It is conventional to omit factor loadings \\(&lt;|0.3|\\); however, be sure to ensure that you mention this in a table note.↩︎\nmissing this step leads to the jingle-jangle fallacies, because we would now go out and tell everyeon that this scale measures ‘creativity’!↩︎"
  }
]