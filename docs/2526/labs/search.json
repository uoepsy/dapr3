[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "",
    "text": "This site contains weekly exercises for the Data Analysis for Psychology in R 3 (DAPR3) course.\nAt the end of each week, solutions (where these are not already available) will be made visible directly beneath each question."
  },
  {
    "objectID": "index.html#about-dapr3",
    "href": "index.html#about-dapr3",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "About DAPR3",
    "text": "About DAPR3\nData Analysis for Psychology in R 3 (DAPR3) is a course undertaken by 3rd year students in Psychology. DAPR3 builds on the content of DAPR2 and covers more advanced methods that are invaluable for analysing many types of psychological study, preparing students for their dissertations. The course offers students a solid foundation in multilevel modeling, expanding the linear model to analyze “hierarchical data”. Such data often involves observations clustered within higher-level groups, such as trials within participants, timepoints within individuals, or children within schools. In the second half of the course, we delve into data reduction techniques. These methods allow us to effectively summarize multiple correlated variables, either through weighted composites or by positing underlying latent factors. Additionally, students will gain insights into crucial concepts, including measurement error, validity, reliability, and replicability. These concepts are especially essential for researchers in psychology, where surveys or questionnaires are used to conduct studies of underlying constructs that cannot be directly measured."
  },
  {
    "objectID": "index.html#installupdate-r-rstudio",
    "href": "index.html#installupdate-r-rstudio",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "Install/Update R & RStudio",
    "text": "Install/Update R & RStudio\nMake sure you have installed both R and RStudio on your computer. You may have done this previously for DAPR2, in which case it is probably worth doing some updates.\nPlease make sure to read and follow the instructions below slowly and carefully!!\n\nFor instructions on how to install R and RStudio, click here\nFor instructions on how to update R and RStudio, click here"
  },
  {
    "objectID": "index.html#update-packages",
    "href": "index.html#update-packages",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "Update Packages",
    "text": "Update Packages\nIt’s worth keeping packages up to date, so it might be worth updating all your packages.\nRunning this code will update all your packages. Just put it into the console (bottom left bit of RStudio):\n\noptions(pkgType = \"binary\")\nupdate.packages(ask = FALSE)"
  },
  {
    "objectID": "index.html#new-packages",
    "href": "index.html#new-packages",
    "title": "Data Analysis for Psychology in R 3 Workbook",
    "section": "New packages!",
    "text": "New packages!\nNow it is probably worth installing a few of the packages that we will be using in DAPR3. There are a few that we will need. For each one, check whether you have it already installed, because there’s not much point wasting time re-installing something you already have!\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models\npsych: for factor analysis\nlavaan: for latent variable models"
  },
  {
    "objectID": "08ex.html",
    "href": "08ex.html",
    "title": "W8 Exercises: EFA",
    "section": "",
    "text": "Data: Conduct Problems\nA researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours:\n\nBreaking curfew\nVandalism\nSkipping school\nBullying\nSpreading malicious rumours\nFighting\nLying\nUsing a weapon\nStealing\nThreatening others\n\nOur task is to use the dimension reduction techniques we learned about in the lecture to help inform how to organise the items she has developed into subscales.\nThe data can be found at https://uoepsy.github.io/data/conduct_ninepoint.csv\n\n\nQuestion 1\n\n\nRead in the dataset.\nCreate a correlation matrix for the items, and inspect the items to check their suitability for exploratory factor analysis.\n\n\n\n\n\n\nHints\n\n\n\n\n\nTake a look at the reading on initial checks for EFA.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\ncpdata &lt;- read.csv(\"https://uoepsy.github.io/data/conduct_ninepoint.csv\")\n# discard the first column, which only contains IDs\ncpdata &lt;- cpdata[,-1]\n\nHere’s a correlation matrix visualised with heatmap(). Dark colours mean strong correlations, lighter colours mean weaker ones. Looks like we’re dealing with two obvious blocks of items here.\n\nheatmap(cor(cpdata))\n\n\n\n\n\n\n\n\nNext: Bartlett’s test checks whether our correlation matrix is different from the identity matrix (a matrix of all 0s except for 1s on the diagonal). It comes out with a p-value of 0 (which isn’t a possible p-value, so this must have been rounded down for some reason). But it’s definitely below 0.05, so we can reject the null that the correlation matrix is proportional to the identity matrix. This is good. It basically means “we have some non-zero correlations”!\n\nlibrary(psych)\ncortest.bartlett(cor(cpdata), n=450)\n\n$chisq\n[1] 1876\n\n$p.value\n[1] 0\n\n$df\n[1] 45\n\n\nNext: The KMO sampling adequacy tells us how “factorable” the correlation matrix is—how well can we take it apart into potential subfactors? The overall MSA (representing sampling adequacy) is 0.87, which is pretty good! (Or in the official terms of the KMO measure: ‘meritorious’!). And for each individual item, the MSA is above 0.8. Also good.\n\nKMO(cpdata)  \n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = cpdata)\nOverall MSA =  0.87\nMSA for each item = \n   breaking_curfew          vandalism    skipping_school           bullying \n              0.83               0.88               0.92               0.82 \n spreading_rumours           fighting              lying       using_weapon \n              0.82               0.93               0.87               0.93 \n          stealing threatening_others \n              0.89               0.92 \n\n\nFinally: All the relationships here look fairly linear, which is good, because correlations are based on linear relationships.\n\npairs.panels(cpdata)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nHow many dimensions should be retained?\nThis question can be answered in the same way as we did for PCA - use a scree plot, parallel analysis, and MAP test to guide you. See the brief PCA walkthrough here.\n\n\n\n\n\nScree\n\n\n\nSolution 2. A scree plot shows how much variance is explained by each additional factor. The scree plot shows a kink at 3, which suggests retaining 2 components: the components before the kink.\n\nscree(cpdata)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAP\n\n\n\nSolution 3. The Minimum Average Partial (MAP) test goes through all components in turn, removing each of them from the correlation matrix and seeing what impact that has on the remaining correlations. The values that it computes for each component are the “average squared partial correlation” between the remaining components.\nOne of the values in this list will be the smallest. That small value represents the point at which removing a component removes important shared variance, rather than removing variable-specific variance.\nIn short, we want to find the smallest MAP value and choose the corresponding number of factors.\nI’m just extracting the actual map values here. We can see that the second entry is the smallest, so the MAP test also suggests retaining two factors.\n\nVSS(cpdata, plot = FALSE, n = ncol(cpdata))$map\n\n [1] 0.0856 0.0318 0.0552 0.1009 0.1503 0.2192 0.3048 0.5425 1.0000     NA\n\n\n\n\n\n\n\nParallel Analysis\n\n\n\nSolution 4. Parallel analysis involves comparing the observed dataset to simulated datasets in which the variables are totally uncorrelated. We want to keep the components that look different from the simulated data.\nParallel analysis suggests two factors as well.\n\nfa.parallel(cpdata, fa = \"both\")\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  2 \n\n\n\n\n\n\n\nMaking a decision\n\n\n\nSolution 5. Again, a quite clear picture that two factors is preferred:\n\n\n\n\n\n\n\n\nguides\nsuggestion\n\n\n\n\nScree\n2\n\n\nMAP\n2\n\n\nParallel Analysis\n2\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nUse the function fa() from the psych package to conduct and EFA to extract two factors. (We made the choice of two factors based on the various tests above, but you might feel differently—the ideal number of factors is totally subjective!) Use a suitable rotation (rotate = ?) and extraction method (fm = ?).\n\n\n\n\n\n\nHints\n\n\n\n\n\nWould you expect factors to be correlated? If so, you’ll want an oblique rotation.\nSee the readings here.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. For example, you could choose an oblimin rotation to allow factors to correlate. Let’s use MLE as the estimator.\n\nconduct_efa &lt;- fa(cpdata, nfactors=2, rotate='oblimin', fm=\"ml\")\n\n\n\n\n\nQuestion 4\n\n\nInspect your solution. Make sure to look at and think about the loadings, the variance accounted for, and the factor correlations (if estimated).\n\n\n\n\n\n\nHints\n\n\n\n\n\nJust printing an fa object:\n\nmyfa &lt;- fa(data, ..... )\nmyfa\n\nWill give you lots and lots of information.\nYou can extract individual parts using:\n\nmyfa$loadings for the loadings\nmyfa$complexity for the complexity (essentially the number of factors that a given item loads onto)\nmyfa$Vaccounted for the variance accounted for by each factor\nmyfa$Phi for the factor correlation matrix\n\nYou can find a quick guide to reading the fa output here: efa_output.pdf.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. \n\nconduct_efa\n\n\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 2, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                     ML1   ML2   h2   u2 com\nbreaking_curfew    -0.06  0.87 0.71 0.29   1\nvandalism           0.07  0.65 0.46 0.54   1\nskipping_school     0.03  0.62 0.40 0.60   1\nbullying            0.87 -0.01 0.75 0.25   1\nspreading_rumours   0.89 -0.01 0.77 0.23   1\nfighting            0.59  0.00 0.35 0.65   1\nlying               0.01  0.76 0.58 0.42   1\nusing_weapon        0.61  0.09 0.42 0.58   1\nstealing            0.03  0.70 0.50 0.50   1\nthreatening_others  0.60 -0.01 0.36 0.64   1\n\n                       ML1  ML2\nSS loadings           2.66 2.64\nProportion Var        0.27 0.26\nCumulative Var        0.27 0.53\nProportion Explained  0.50 0.50\nCumulative Proportion 0.50 1.00\n\n With factor correlations of \n     ML1  ML2\nML1 1.00 0.42\nML2 0.42 1.00\n\nMean item complexity =  1\n\n\nThings look pretty good here:\n\nEach item has a clear primary loading on to one of the factors. This is even easier to see if we just pull out the loadings, because loadings below 0.3 are hidden:\n\n\nconduct_efa$loadings\n\n\nLoadings:\n                   ML1    ML2   \nbreaking_curfew            0.865\nvandalism                  0.647\nskipping_school            0.618\nbullying            0.870       \nspreading_rumours   0.885       \nfighting            0.593       \nlying                      0.758\nusing_weapon        0.608       \nstealing                   0.696\nthreatening_others  0.602       \n\n                 ML1   ML2\nSS loadings    2.633 2.616\nProportion Var 0.263 0.262\nCumulative Var 0.263 0.525\n\n\n\nThe h2 column is showing that the 2 factor solution is explaining at minimu 35% of the variance in each item, often more.\nThe complexity for all items is 1 (meaning each item is clearly linked to just one of the factors).\nBoth factors are well determined, having at least 3 salient loadings.\n\nThe proportion of variance explained shows us that the two factors together explain 53% of the variance in the data. And both factors explain a similar amount: 27% for factor 1, 26% for factor 2.\nWe can also see that there is a moderate correlation between the two factors. Use of an oblique rotation was appropriate - if the correlation had been very weak, then it might not have differed much from if we used an orthogonal rotation.\n\n\n\n\n\nQuestion 5\n\n\nThis is the fun part! Look back at what each item is asking about, and suggest a name for your factors based on the patterns of loadings. What underlying construct do you think each of the two factors might be picking up on?\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo sort and group the loadings together in a more readable way, you can use\n\nprint(myfa$loadings, sort = TRUE)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nprint(conduct_efa$loadings, sort=TRUE)\n\n\nLoadings:\n                   ML1    ML2   \nbullying            0.870       \nspreading_rumours   0.885       \nfighting            0.593       \nusing_weapon        0.608       \nthreatening_others  0.602       \nbreaking_curfew            0.865\nvandalism                  0.647\nskipping_school            0.618\nlying                      0.758\nstealing                   0.696\n\n                 ML1   ML2\nSS loadings    2.633 2.616\nProportion Var 0.263 0.262\nCumulative Var 0.263 0.525\n\n\nWe can see that, ordered like this, we have five items that have high loadings for one factor and another five items that have high loadings for the other.\nThe five items for factor 1 all have in common that they are more aggressive kinds of misbehaviour. On the other hand, the five items for factor 2 are non-aggressive kinds of misbehaviour. We could, therefore, label our factors as ‘aggressive’ and ‘non-aggressive’ conduct problems.\n\n\n\n\nQuestion 6\n\n\nCompare three different solutions:\n\nyour current solution from the previous questions\none where you fit 1 more factor\none where you fit 1 fewer factors\n\nWhich one looks best?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWe’re looking here to assess:\n\nhow much variance is accounted for by each solution\ndo all factors load on 3+ items at a salient level?\n\ndo all items have at least one loading at a salient level?\nare there any “Heywood cases” (communalities or standardised loadings that are &gt;1)?\nshould we perhaps remove some of the more complex items?\nis the factor structure (items that load on to each factor) coherent, and does it make theoretical sense?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. The 1-factor model explains 35% of the variance (as opposed to the 53% explained by the 2 factor solution), and all items load fairly high on the factor. The downside here is that we’re not discerning between different types of conduct problems that we did in the 2 factor solution.\n\nconduct_1 &lt;- fa(cpdata, nfactors=1, fm=\"ml\")\nconduct_1\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 1, fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                    ML1   h2   u2 com\nbreaking_curfew    0.46 0.21 0.79   1\nvandalism          0.46 0.21 0.79   1\nskipping_school    0.41 0.17 0.83   1\nbullying           0.82 0.68 0.32   1\nspreading_rumours  0.83 0.69 0.31   1\nfighting           0.58 0.34 0.66   1\nlying              0.47 0.22 0.78   1\nusing_weapon       0.65 0.43 0.57   1\nstealing           0.45 0.20 0.80   1\nthreatening_others 0.59 0.34 0.66   1\n\n                ML1\nSS loadings    3.48\nProportion Var 0.35\n\nMean item complexity =  1\nTest of the hypothesis that 1 factor is sufficient.\n\ndf null model =  45  with the objective function =  4.22 with Chi Square =  1876\ndf of  the model are 35  and the objective function was  1.52 \n\nThe root mean square of the residuals (RMSR) is  0.17 \nThe df corrected root mean square of the residuals is  0.19 \n\nThe harmonic n.obs is  450 with the empirical chi square  1143  with prob &lt;  2.1e-217 \nThe total n.obs was  450  with Likelihood Chi Square =  675  with prob &lt;  1.8e-119 \n\nTucker Lewis Index of factoring reliability =  0.55\nRMSEA index =  0.202  and the 90 % confidence intervals are  0.189 0.215\nBIC =  461\nFit based upon off diagonal values = 0.82\nMeasures of factor score adequacy             \n                                                   ML1\nCorrelation of (regression) scores with factors   0.94\nMultiple R square of scores with factors          0.88\nMinimum correlation of possible factor scores     0.76\n\n\nThe 3-factor model explains 58% of the variance (only 5% more than the 2-factor model). Notably, the third factor is not very clearly defined - it only has 1 salient loading (possibly 2 if we consider the 0.3 to be salient, but that item is primarily loaded on the 2nd factor).\n\nconduct_3 &lt;- fa(cpdata, nfactors=3, rotate='oblimin', fm=\"ml\")\nconduct_3\n\nFactor Analysis using method =  ml\nCall: fa(r = cpdata, nfactors = 3, rotate = \"oblimin\", fm = \"ml\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n                     ML2   ML3   ML1   h2    u2 com\nbreaking_curfew    -0.04  0.72  0.17 0.67 0.328 1.1\nvandalism           0.02  0.02  0.93 0.90 0.097 1.0\nskipping_school     0.03  0.53  0.11 0.39 0.606 1.1\nbullying            0.87  0.03 -0.04 0.75 0.246 1.0\nspreading_rumours   0.88 -0.02  0.02 0.77 0.227 1.0\nfighting            0.59 -0.05  0.07 0.35 0.645 1.0\nlying               0.02  0.83 -0.07 0.64 0.363 1.0\nusing_weapon        0.61  0.11 -0.02 0.42 0.579 1.1\nstealing            0.03  0.74 -0.04 0.53 0.473 1.0\nthreatening_others  0.60 -0.04  0.05 0.36 0.640 1.0\n\n                       ML2  ML3  ML1\nSS loadings           2.65 2.14 1.01\nProportion Var        0.26 0.21 0.10\nCumulative Var        0.26 0.48 0.58\nProportion Explained  0.46 0.37 0.17\nCumulative Proportion 0.46 0.83 1.00\n\n With factor correlations of \n     ML2  ML3  ML1\nML2 1.00 0.40 0.33\nML3 0.40 1.00 0.65\nML1 0.33 0.65 1.00\n\nMean item complexity =  1\nTest of the hypothesis that 3 factors are sufficient.\n\ndf null model =  45  with the objective function =  4.22 with Chi Square =  1876\ndf of  the model are 18  and the objective function was  0.02 \n\nThe root mean square of the residuals (RMSR) is  0.01 \nThe df corrected root mean square of the residuals is  0.02 \n\nThe harmonic n.obs is  450 with the empirical chi square  5.26  with prob &lt;  1 \nThe total n.obs was  450  with Likelihood Chi Square =  9.69  with prob &lt;  0.94 \n\nTucker Lewis Index of factoring reliability =  1.01\nRMSEA index =  0  and the 90 % confidence intervals are  0 0.008\nBIC =  -100\nFit based upon off diagonal values = 1\nMeasures of factor score adequacy             \n                                                   ML2  ML3  ML1\nCorrelation of (regression) scores with factors   0.95 0.93 0.95\nMultiple R square of scores with factors          0.89 0.86 0.91\nMinimum correlation of possible factor scores     0.79 0.71 0.81\n\n\n\n\n\n\nQuestion 7\n\n\nWrite a few short paragraphs that summarises your choices, your method, and the results from your chosen optimal factor structure for the 10 conduct problems.\n\n\n\n\n\n\nHints\n\n\n\n\n\nWrite about the process that led you to the number of factors you chose. Discuss the patterns of loadings and provide definitions of the factors.\nThere is no one correct way to write this up!\nTry thinking of it like this: If you were describing what you did to another researcher who wanted to reproduce your workflow and understand your decisions, what would you say?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. The main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description).\nAn example summary would be:\n\nFirst we checked whether the data were suitable for factor analysis. We checked normality using visual inspection of histograms, linearity by inspecting linear and loess lines for the pairwise associations between variables, and factorability using a KMO test, which yielded an overall KMO of \\(.83\\) and no variable KMOs \\(&lt;.50\\).\nWe conducted an exploratory factor analysis to inform the structure of a new test for conduct problems. To help us decide how many factors to retain, we used a scree plot, parallel analysis (using principal components analysis; PA-PCA), and the MAP test. All three methods suggested that we retain two factors. However, we also checked a one-factor solution and a three-factor solution to confirm that the two-factor solution was indeed most appropriate. We didn’t want to risk our two-factor solution blurring important distinctions between factors, nor including a minor secondary factor that would be more sensibly combined with the primary one.\nWe extracted all factors using maximum likelihood estimation and (for the two- and three-factor solutions) an oblimin rotation, because we expected that the factors would be correlated.\nOur two-factor solution accounted for 53% of the variance in the items. This suggests that the two-factor solution is the most effective of the three soluions we tried (the one-factor solution explained only 35% of variance, while the three-factor solution explained 58%, only 5% more than the two-factor solution). Both of the factors in the two-factor solution were well-determined, each including five loadings above 0.3, and they have a correlation of \\(r=.42\\).\nThe factor loadings above 0.3 are shown in Table 11. Based on the pattern of factor loadings, we interpreted the two factors as ‘aggressive conduct problems’ (factor ML1) and ‘non-aggressive conduct problems’ (factor ML2).\n\n\n\n\nTable 1: Factor Loadings\n\n\n\n\n\n\n\nML1\nML2\n\n\n\n\nspreading_rumours\n0.88\n\n\n\nbullying\n0.87\n\n\n\nusing_weapon\n0.61\n\n\n\nthreatening_others\n0.60\n\n\n\nfighting\n0.59\n\n\n\nvandalism\n\n0.86\n\n\nskipping_school\n\n0.76\n\n\nstealing\n\n0.70\n\n\nlying\n\n0.65\n\n\nbreaking_curfew\n\n0.62"
  },
  {
    "objectID": "08ex.html#footnotes",
    "href": "08ex.html#footnotes",
    "title": "W8 Exercises: EFA",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou should provide the table of factor loadings. It is conventional to omit factor loadings \\(&lt;|0.3|\\); however, be sure to ensure that you mention this in a table note.↩︎\nmissing this step leads to the jingle-jangle fallacies, because we would now go out and tell everyone that this scale measures ‘creativity’!↩︎"
  },
  {
    "objectID": "05ex.html",
    "href": "05ex.html",
    "title": "W5 Exercises: Bringing it all together",
    "section": "",
    "text": "Take your pick!\n\nQuestion 1\n\n\nYou can find all the datasets that we have seen (and more!) as an additional doc in the readings page.\nFor each one, there is a quick explanation of the study design which also details the research aims of the project.\nPick one of the datasets and, in your groups:\n\nexplore the data, and do any required cleaning (most of them are clean already)\nconduct an analysis to address the research aims\nwrite a short description of the sample data (see Chapter 11 #the-sample-data)\nwrite a short explanation of your methods (see Chapter 11 #the-methods)\nwrite a short summary of your results, along with suitable visualisations and tables (see Chapter 11 #the-results)\nPost some of your writing on Piazza and we can collectively discuss it!\n\n\nEach of the datasets contains some tags that give an indication of the type of study. Anything with either “#binomial-outcome” or “#non-linear” you can ignore as we have not covered this in DAPR3.\n\n\n\n\n\n\n\n\n\n\n\nFlashcards: lm to lmer\nIn a simple linear regression, there is only considered to be one source of random variability: any variability left unexplained by a set of predictors (which are modelled as fixed estimates) is captured in the model residuals.\nMulti-level (or ‘mixed-effects’) approaches involve modelling more than one source of random variability - as well as variance resulting from taking a random sample of observations, we can identify random variability across different groups of observations. For example, if we are studying a patient population in a hospital, we would expect there to be variability across the our sample of patients, but also across the doctors who treat them.\nWe can account for this variability by allowing the outcome to be lower/higher for each group (a random intercept) and by allowing the estimated effect of a predictor vary across groups (random slopes).\n\nBefore you expand each of the boxes below, think about how comfortable you feel with each concept.\nThis content is very cumulative, which means often going back to try to isolate the place which we need to focus efforts in learning.\nStudy tip: Choose a topic in one of the boxes below, but don’t expand the box yet. Instead, grab a pen and paper, and first spend two minutes writing down everything you can think of about that topic. After those two minutes, you can expand the box and see if you covered the key information. Write down anything you missed. Another day, try writing about the same topic again, and see if you can include anything you missed the first time.\nWhy does this work? Research on learning has shown that if you test yourself at recalling ideas (and especially if you repeatedly test yourself across several days or weeks), you’ll get better at remembering and understanding these ideas in the long term.\n\n\n\n\n\n\n\nSimple Linear Regression\n\n\n\n\n\n\nFormula:\n\n\\(y_i = b_0 + b_1 x_i + \\epsilon_i\\)\n\nR command:\n\nlm(outcome ~ predictor, data = dataframe)\n\nNote: this is the same as lm(outcome ~ 1 + predictor, data = dataframe). The 1 + (which represents the intercept, \\(b_0\\)) is always there, even if we don’t explicitly write it. (It’s possible to tell R to remove it, e.g., by explicitly writing 0 + predictor, but realistically you probably won’t ever want to fit a model with no intercept.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-level data\n\n\n\n\n\nIf our dataset contains multiple observations from the same subject, for example, or multiple observations of the same stimulus, then those data points are not independent. For example:\n\nSubject A may behave a particular consistent way, and Subject B may behave a different particular consistent way. So, one data point from Subject A will be closer to the other data points from Subject A than it will be to the data points from Subject B.\nSimilarly, Stimulus A may provoke a particular reaction, while Stimulus B may provoke a different particular reaction. So a data point from Stimulus A will look more like the other data from Stimulus A than it will to the data from Stimulus B.\n\nThis is one way for data points to be non-independent. But a simple regression assumes independence. So, we need some extra way to take this non-independence into account. And we do that using random effects.\nIn other words: when our data is ‘clustered’ or ‘grouped’ such that datapoints are no longer independent, but belong to some grouping such as that of multiple observations from the same subject, we have multiple sources of random variability. A simple regression does not capture this.\nTo illustrate how a simple regression falls short: If we separate out our data to show an individual plot for each grouping (in this data the grouping is by subjects), we can see how the fitted regression line from lm() is assumed to be the same for each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom intercepts\n\n\n\n\n\nBy including a random-intercept term, we are letting our model estimate some variability around the average intercept parameter. The variability reflects how the intercept should be adjusted (nudged up or down) to better suit the data from different members of each group.\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = b_{0i} + b_{1} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(b_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\nWhere the expected values of \\(\\zeta_{0}\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\) and \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\n(“Random intercepts” are called “random” because the adjustments to the intercept are modelled as random samples from an underlying normal distribution with mean 0 and standard deviation \\(\\sigma_0\\).)\n\nRemember, variance is just standard deviation squared!\n\nWe can now see that the intercept estimate \\(b_{0i}\\) for a particular group \\(i\\) is represented by the combination of a mean estimate for the parameter (\\(\\gamma_{00}\\)) and a random effect (that is, an adjustment, a nudge) for that group (\\(\\zeta_{0i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 | grouping), data = dataframe)\n\n\nNotice how the fitted line of the random intercept model has been adjusted for each subject.\nEach subject’s line has been moved up or down. The intercepts can be different, but the slopes are all the same.\n\n\n\n\n\n\n\n\n\nAnother way of comparing the estimates of the random intercept model is to plot all the by-subject lines (the colourful lines) and overlay the fixed effect line (the thick green line):\n\n\n\n\n\n\n\n\n\nHere, we see that the green fixed effect line is the average of all the by-subject lines, and that all the by-subject lines have the exact same slope as the fixed effect line—they’ve just been nudged up or down.\n\n\n\n\n\n\n\n\n\nPooling & Shrinkage\n\n\n\n\n\nIf you think about it, we might have done a similar thing to the random intercept with the tools we already had at our disposal, by using lm(y~x+subject). This would give us a coefficient for the difference between each subject and the reference level intercept, or we could extend this to lm(y~x*subject) to give us an adjustment to the slope for each subject.\nHowever, the estimate of these models will be slightly different. The green lines are the predictions from the model with a random intercept by subject. The purple lines (sometimes hidden behind the green one) are the predictions from the model with a fixed effect of subject.\n\n\n\n\n\n\n\n\n\nWhy is the purple line for sub_373 below the green line? Why is the purple line for sub_374 above the green line? The short answer: “shrinkage”. The green lines (which come from the random intercept model) are “shrunk” closer to the average line than the purple lines (which come from the fixed effect model).\n\nIn general, when we use multilevel models, the estimates for each member of a group are shrunk toward the average. (How much they are shrunk by depends on how variable the members of a group are, as well as how many data points each member contributes.)\nShrinkage is a benefit of multilevel models for a couple reasons:\n\nShrinkage makes our estimates a bit more conservative (it’s harder to get extreme estimates when they are shrunk toward the average).\nShrinkage means that we can make a more educated guess about the estimates for members of a group, based on the estimates from everyone else in the group.\n\nYou could think of it like this: In the example above, the multilevel model “borrows information” or “borrows strength” from subjects sub_352, sub_369, and all the others, to inform its estimates for sub_373 and about sub_374. “Borrowing information” or “borrowing strength” also gets termed “partial pooling”, because we are partially combining (that is, partially pooling) all the information across members of a group to get an average, but still allowing those group members to vary a little.\n\n\n\n\n\n\n\n\nmodel\npooling\nexplanation\n\n\n\n\nlm(y~x)\ncomplete pooling\nall information from all members of a group is combined (pooled) together, and a line is fit that doesn’t take grouping into account at all\n\n\nlm(y~group + x)\nno pooling\ninformation is divided between the members of the group, and differnences between group members are estimated. Observations from group \\(i\\) contribute only to estimates about group \\(i\\)\n\n\nlmer(y~x+(1\ngroup))\npartial pooling\n\n\n\n\n\n\n\n\n\n\n\n\nRandom slopes\n\n\n\n\n\n\nFormula:\nLevel 1:\n\n\\(y_{ij} = b_{0i} + b_{1i} x_{ij} + \\epsilon_{ij}\\)\n\nLevel 2:\n\n\\(b_{0i} = \\gamma_{00} + \\zeta_{0i}\\)\n\n\\(b_{1i} = \\gamma_{10} + \\zeta_{1i}\\)\n\nWhere the expected values of \\(\\zeta_0\\), \\(\\zeta_1\\), and \\(\\epsilon\\) are 0, and their variances are \\(\\sigma_{0}^2\\), \\(\\sigma_{1}^2\\), \\(\\sigma_\\epsilon^2\\) respectively. We will further assume that these are normally distributed.\n(Like random intercepts, “random slopes” are called “random” because the adjustments to the slope are modelled as random samples from an underlying normal distribution with mean 0 and standard deviation \\(\\sigma_1\\).)\n\nRemember, variance is just standard deviation squared!\n\nAs with the intercept \\(b_{0i}\\), the slope of the predictor \\(b_{1i}\\) is now modelled by a mean \\(\\gamma_{10}\\) and a random effect for each group (\\(\\zeta_{1i}\\)).\nR command:\n\nlmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\nNote: this is the same as lmer(outcome ~ predictor + (predictor | grouping), data = dataframe) . Like in the fixed-effects part, the 1 + is assumed in the random-effects part.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel parameters: Fixed effects\n\n\n\n\n\nFixed effects are the parameters estimated for the average group, around which the random effects can vary.\nThe plot below show the fitted values for each subject from the random slopes model lmer(outcome ~ predictor + (1 + predictor | grouping), data = dataframe)\n\n\n\n\n\n\n\n\n\nThe thick green line shows the fixed intercept and slope around which the members of the group all vary randomly.\nThe fixed effects are the parameters that define the thick green line, and we can extract them using the fixef() function:\n\nfixef(random_slopes_model)\n\n(Intercept)          x1 \n    405.790      -0.672 \n\n\n\n\n\n\n\n\n\n\n\nModel parameters: Variance components\n\n\n\n\n\nVariance components are the variances and covariances of the random effects. A multilevel model estimates variance components as well as fixed effects.\nThe reason we call random effects “random” is because the adjustments to the fixed effects are modelled as random samples from an underlying normal distribution. The width of these distributions represents how much the group tends to deviate from each fixed effect. We can think of the width of these distributions as the variance components.\n\n\n\n\n\n\n\n\n\n\nWe can extract these using the VarCorr() function, and we can also see them in the “random effects” part of the summary() output from a model.\nLooking at these variance components, we can ask:\n\nHow much do members of a group vary around the fixed intercept? And around the fixed slope? (This is answered by the standard deviation.)\nDo members of a group with higher intercepts also have higher slopes? (This is the correlation between random intercept and random slope.)\n\n\nVarCorr(random_slopes_model)\n\n Groups   Name        Std.Dev. Corr \n subject  (Intercept) 72.72         \n          x1           1.36    -0.35\n Residual             25.74         \n\n\n\nRemember, variance is just standard deviation squared!\n\n\n\n\n\n\n\n\n\n\nExtracting group-member-specific random effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can extract each group member’s deviations from the group average (i.e., from the fixed effect) using the ranef() function.\n\nranef(random_slopes_model)\n\n$subject\n        (Intercept)      x1\nsub_308       31.33 -1.4400\nsub_309      -28.83  0.4184\nsub_310        2.71  0.0599\nsub_330       59.40  0.3853\nsub_331       74.96  0.1739\nsub_332       91.09 -0.2346\nsub_333       97.85 -0.1906\nsub_334      -54.19 -0.5585\nsub_335      -16.90  0.9207\nsub_337       52.22 -1.1660\nsub_349      -67.76 -0.6844\nsub_350       -5.82 -1.2379\nsub_351       61.20  0.0550\nsub_352       -7.91 -0.6650\nsub_369      -47.64 -0.4681\nsub_370      -33.12 -1.1100\nsub_371       77.58 -0.2040\nsub_372      -36.39 -0.4583\nsub_373     -197.58  1.7990\nsub_374      -52.20  4.6051\n\nwith conditional variances for \"subject\" \n\n\nWhat do these numbers mean?\nThese are the differences between the line that fits each subject’s data and the line defined by the fixed intercept and the fixed slope. So the first entry, the line that fits the data from sub_308 has an intercept that’s 31.33 higher than the fixed intercept and a slope that is 1.44 below the fixed slope.\nIf we wanted to find the parameters of each subject’s line, we could compute them with this information and the model’s fixed effects. But the next drop-down box shows us how to find those parameters more simply.\n\n\n\n\n\n\n\n\n\n\n\nExtracting group-member-specific coefficients\n\n\n\n\n\nWe can see the estimated intercept and slope for each subject specifically, using the coef() function.\n\ncoef(random_slopes_model)\n\n$subject\n        (Intercept)     x1\nsub_308         437 -2.112\nsub_309         377 -0.254\nsub_310         409 -0.612\nsub_330         465 -0.287\nsub_331         481 -0.498\nsub_332         497 -0.907\nsub_333         504 -0.863\nsub_334         352 -1.231\nsub_335         389  0.248\nsub_337         458 -1.838\nsub_349         338 -1.357\nsub_350         400 -1.910\nsub_351         467 -0.617\nsub_352         398 -1.337\nsub_369         358 -1.140\nsub_370         373 -1.782\nsub_371         483 -0.876\nsub_372         369 -1.131\nsub_373         208  1.127\nsub_374         354  3.933\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nLet’s confirm that the outcome of coef() is the same as taking the fixed effects and then adding each subject’s random effects (as seen in the previous drop-down box).\n\ncbind(\n  int = fixef(random_slopes_model)[1] + \n    ranef(random_slopes_model)$subject[,1],\n  slope = fixef(random_slopes_model)[2] + \n    ranef(random_slopes_model)$subject[,2]\n)\n\n      int  slope\n [1,] 437 -2.112\n [2,] 377 -0.254\n [3,] 409 -0.612\n [4,] 465 -0.287\n [5,] 481 -0.498\n [6,] 497 -0.907\n [7,] 504 -0.863\n [8,] 352 -1.231\n [9,] 389  0.248\n[10,] 458 -1.838\n[11,] 338 -1.357\n[12,] 400 -1.910\n[13,] 467 -0.617\n[14,] 398 -1.337\n[15,] 358 -1.140\n[16,] 373 -1.782\n[17,] 483 -0.876\n[18,] 369 -1.131\n[19,] 208  1.127\n[20,] 354  3.933\n\n\nYes, both of these outcomes are the same.\n\n\n\n\n\n\n\n\n\nPlotting random effects\n\n\n\n\n\nThe quick and easy way to plot your random effects is to use the dotplot.ranef.mer() function in lme4.\n\nrandoms &lt;- ranef(random_slopes_model, condVar=TRUE)\ndotplot.ranef.mer(randoms)\n\n$subject\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssumptions, Influence\n\n\n\n\n\nFor a simple linear model, \\(\\color{red}{y} = \\color{blue}{b_0 + b_1(x)} \\color{black}{+ \\varepsilon}\\), we distinguish between the systematic model part (\\(b_0 + b_1(x)\\)) and the errors, which randomly vary around the systematic model (\\(\\varepsilon\\)). In words, we can summarise the linear model expression as \\(\\color{red}{\\text{outcome}} = \\color{blue}{\\text{model}}\\) \\(+ \\text{error}\\).\nIn the multi-level model, another source of error (i.e., of variation) come from our random effects: \\(\\color{red}{\\text{outcome}}\\) = \\(\\color{blue}{\\text{model}}\\) \\(+ \\text{group-error} + \\text{individual-error}\\). As such, random effects are another form of residual. Our assumptions of “zero mean, constant variance” apply at both levels of residuals (see Figure 1).\n\n\n\n\n\n\n\n\nFigure 1: The black dashed lines show our model assumptions. We assume that the intercept adjustments are normally distributed (the large normal distribution at the y axis), and we further assume that the residuals between the individual data points and the subject-level line are also normally distributed with equal variance across the range of the predictor (the smaller normal distributions around one of the subject-level lines).\n\n\n\n\n\n\nWe can assess these normality of both resid(model) and ranef(model) by constructing plots using functions such as hist(), qqnorm() and qqline().\n\nWe can also use plot(model, type=c(\"p\",\"smooth\")) to give us our residuals vs fitted plot (smooth line should be horizontal at approx zero, showing zero mean).\n\nplot(model, form = sqrt(abs(resid(.))) ~ fitted(.), type = c(\"p\",\"smooth\")) will give us our scale-location plot (smooth line should be horizontal, showing constant variance).\n\nWe can also use the check_model() function from the performance package to get lots of info at once:\n\nlibrary(performance)\ncheck_model(random_slopes_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference\n\n\n\n\n\nTo get p-values for our coefficients, there are lots of different ways (see Optional Chapter 3 if you’re interested).\nFor DAPR3, we are recommending using the “Satterthwaite” method, which can be done by re-fitting the model using the lmerTest package:\n\nrandom_slopes_model2 &lt;- lmerTest::lmer( outcome ~ 1 + x1 + (1+x1|subject), data=dat)\nsummary(random_slopes_model2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: outcome ~ 1 + x1 + (1 + x1 | subject)\n   Data: dat\n\nREML criterion at convergence: 1862\n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.069 -0.417 -0.014  0.431  5.226 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n subject  (Intercept) 5287.68  72.72         \n          x1             1.86   1.36    -0.35\n Residual              662.33  25.74         \nNumber of obs: 185, groups:  subject, 20\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)  405.790     16.666  18.045   24.35    3e-15 ***\nx1            -0.672      0.313  16.757   -2.15    0.047 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n   (Intr)\nx1 -0.370\n\n\nIf we want to test multiple coefficients at once, then we can conduct model comparisons by doing a likelihood ratio test. We start with a full model, and then create a more restricted comparison model by removing the bits that we want to test.\nIn the example below, the full model is outcome ~ 1 + x1 + x2 + (1 + x1 | subject). If we wanted to test whether the predictors x1 and x2 contribute significantly to the model, then we would create a restricted model for comparison by removing those terms. That gives us outcome ~ 1 + (1 + x1 | subject).\nYou’ll notice that x1 still appears in the random effect: we’re still getting a random slope over x1 by subject, even though x1 is no longer in the fixed effects. This looks a little weird, but when doing model comparison, it’s OK to have random slopes of predictors that aren’t in the fixed effects.\n\nmodel2 &lt;- lmer( outcome ~ 1 + x1 + x2+ (1+x1|subject), data=dat)\nmodel2.0 &lt;- lmer( outcome ~ 1 + (1+x1|subject), data=dat)\n\nanova(model2.0, model2)\n\n\n\n\n\n\n\n\n\n\nVisualising group-member-specific fitted values\n\n\n\n\n\nThe model fitted (or “model predicted”) values can be obtained a couple ways.\n\npredict() returns just the predicted values, which is a little inconvenient because you’d have to map them back to the original data that you gave to the model.\nbroom.mixed::augment() returns the predicted values as well as the data that you gave to the model.\n\nWe would typically like to plot the fitted values for each member of the grouping variable (e.g., for each subject):\n\nlibrary(broom.mixed)\naugment(random_slopes_model) |&gt;\n  ggplot(aes(x=x1, y=.fitted, group=subject))+\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising fixed effects along with the group-member-specific fitted values\n\n\n\n\n\nIf we want to plot the fixed effects from our model, we have to do something else. A good option is to use the effects package to construct a dataframe of the linear prediction across the values of a predictor, plus confidence intervals. We can then pass this to ggplot(), giving us all the control over the aesthetics.\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\nggplot(ef, aes(x=x1,y=fit, ymin=lower,ymax=upper))+\n  geom_line()+\n  geom_ribbon(alpha=.3)\n\n\n\n\n\n\n\n\nWe might then want to combine this with our plot of fitted values to make a plot that shows both the estimates for the average group (this is the fixed effects part) and the amount to which groups vary around that average (this we can see with the fitted values plot)\n\n# when you want more control\nlibrary(effects)\nef &lt;- as.data.frame(effect(term=\"x1\",mod=random_slopes_model))\n\naugment(random_slopes_model) |&gt;\n  ggplot(aes(x=x1))+\n  geom_line(aes(y=.fitted,group=subject), alpha=.1) + \n  geom_line(data = ef, aes(y=fit))+\n  geom_ribbon(data = ef, aes(y=fit,ymin=lower,ymax=upper), \n              col=\"red\", fill=\"red\",alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNested variables\n\n\n\n\n\nIn the following schematic, we have observations from several children, who are each from a particular school.\n\n\n\n\n\n\n\n\n\nEvery school contains multiple children, and each child appears only in a single school. So, the child variable is “nested” within the school variable.\nIn R, we specify random effects for nested variables using the syntax\n(1 | school) + (1 | child:school)\nor\n(1 | school) + (1 | school:child)\nThe specific order of the variables around the colon symbol doesn’t matter. (But if you think about nesting like “child within school”, then maybe the child:school order is more intuitive.)\n\n\n\n\n\n\n\n\n\nCrossed variables\n\n\n\n\n\nIn the following schematic, we have observations from several participants who have each taken part in several tasks.\n\n\n\n\n\n\n\n\n\nBecause we have data from every participant for every task, we describe the participant and task variables as “fully crossed”.\n\nIn contrast: the following schematic shows observations from several participants, who have each only done a subset of the tasks.\n\n\n\n\n\n\n\n\n\nWe have some data from every participant, and we have some data from every task, but it’s not the case that every participant took part in every task. In this case, we describe the participant and task variables as “partially crossed” or “not fully crossed”.\n\nIn R, we specify random effects for both fully crossed and partially crossed variables as follows:\n(1 | ppt) + (1 | task)\n\n\n\n\n\n\n\n\n\n\nMLM in a nutshell\n\n\n\n\n\nMLM allows us to model effects in the linear model as varying for different groups.\nImportant terminology note: in this box, “group” is used to mean “a single member of a grouping variable”, e.g., an individual participant in a study, whereas in the flash cards above, “group” was used to mean the grouping variable itself, e.g., participant.\nThe coefficients we remember from simple linear models (the \\(b\\)’s) are now modelled as a distribution: their mean is the effect for the average group, and the variance defines how much the groups vary around the mean. We can see this in Figure 2, where both the intercept and the slope of the line are modelled as varying by-groups. Figure 2 shows the overall line in blue, with a given group’s line in green.\n\n\n\n\n\n\n\n\nFigure 2: Multilevel Model. Each group (e.g. the group in the green line) deviates from the overall fixed effects (the blue line), and the individual observations (green points) deviate from their groups line\n\n\n\n\n\nThe formula notation for these models involves separating out our effects \\(b\\) into two parts: the overall effect \\(\\gamma\\) + the group deviations \\(\\zeta_i\\):\n\\[\n\\begin{align}\n& \\text{for observation }j\\text{ in group }i \\\\\n\\quad \\\\\n& \\text{Level 1:} \\\\\n& \\color{red}{y_{ij}}\\color{black} = \\color{blue}{b_{0i} \\cdot 1 + b_{1i} \\cdot x_{ij}}\\color{black} + \\varepsilon_{ij} \\\\\n& \\text{Level 2:} \\\\\n& \\color{blue}{b_{0i}}\\color{black} = \\gamma_{00} + \\color{orange}{\\zeta_{0i}} \\\\\n& \\color{blue}{b_{1i}}\\color{black} = \\gamma_{10} + \\color{orange}{\\zeta_{1i}} \\\\\n\\quad \\\\\n& \\text{Where:} \\\\\n& \\gamma_{00}\\text{ is the population intercept, and }\\color{orange}{\\zeta_{0i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{00} \\\\\n& \\gamma_{10}\\text{ is the population slope, and }\\color{orange}{\\zeta_{1i}}\\color{black}\\text{ is the deviation of group }i\\text{ from }\\gamma_{10} \\\\\n\\end{align}\n\\]\nThe group-specific deviations \\(\\zeta_{0i}\\) from the overall intercept are assumed to be normally distributed with mean \\(0\\) and variance \\(\\sigma_0^2\\). Similarly, the deviations \\(\\zeta_{1i}\\) of the slope for group \\(i\\) from the overall slope are assumed to come from a normal distribution with mean \\(0\\) and variance \\(\\sigma_1^2\\). The correlation between random intercepts and slopes is $= ({0i}, {1i}):\n\\[\n\\begin{bmatrix} \\zeta_{0i} \\\\ \\zeta_{1i} \\end{bmatrix}\n\\sim N\n\\left(\n    \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n    \\begin{bmatrix}\n        \\sigma_0^2 & \\rho_{01}1 \\\\\n        \\rho_{01} & \\sigma_1^2\n    \\end{bmatrix}\n\\right)\n\\]\nThe random errors, independently from the random effects, are assumed to be normally distributed with a mean of zero\n\\[\n\\epsilon_{ij} \\sim N(0, \\sigma_\\epsilon^2)\n\\]\nWe fit these models using the R package lme4, and the function lmer(). Think of it like building your linear model lm(y ~ 1 + x), and then allowing effects (i.e. things on the right hand side of the ~ symbol) to vary by the grouping of your data. We specify these by adding (vary these effects | by these groups) to the model:\n\nlibrary(lme4)\nm1 &lt;- lmer(y ~ x + (1 + x | group), data = df)\nsummary(m1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 + x | group)\n   Data: df\n\nREML criterion at convergence: 638\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4945 -0.5722 -0.0135  0.6254  2.3912 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n group    (Intercept) 2.262    1.504        \n          x           0.796    0.892    0.55\n Residual             4.367    2.090        \nNumber of obs: 132, groups:  group, 20\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)    1.726      0.967    1.78\nx              1.151      0.297    3.88\n\nCorrelation of Fixed Effects:\n  (Intr)\nx -0.552\n\n\nThe summary of the lmer output returns estimated values for\nFixed effects:\n\n\\(\\widehat \\gamma_{00} = 1.726\\)\n\\(\\widehat \\gamma_{10} = 1.151\\)\n\nVariability of random effects:\n\n\\(\\widehat \\sigma_{0} = 1.504\\)\n\\(\\widehat \\sigma_{1} = 0.892\\)\n\nCorrelation of random effects:\n\n\\(\\widehat \\rho = 0.546\\)\n\nResiduals:\n\n\\(\\widehat \\sigma_\\epsilon = 2.09\\)"
  },
  {
    "objectID": "03ex.html",
    "href": "03ex.html",
    "title": "W3 Exercises: Nested and Crossed Structures",
    "section": "",
    "text": "Data: gadeduc.csv\nThis is synthetic data from a randomised controlled trial, in which 30 therapists randomly assigned patients (each therapist saw between 2 and 28 patients) to a control or treatment group, and monitored their scores over time on a measure of generalised anxiety disorder (GAD7 - a 7 item questionnaire with 5 point likert scales).\nThe control group of patients received standard sessions offered by the therapists. For the treatment group, 10 mins of each sessions was replaced with a specific psychoeducational component, and patients were given relevant tasks to complete between each session. All patients had monthly therapy sessions. Generalised Anxiety Disorder was assessed at baseline and then every visit over 4 months of sessions (5 assessments in total).\nThe data are available at https://uoepsy.github.io/data/lmm_gadeduc.csv\nYou can find a data dictionary below:\n\n\n\n\nTable 1: Data Dictionary: lmm_gadeduc.csv\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\npatient\nA patient code in which the labels take the form &lt;Therapist initials&gt;_&lt;group&gt;_&lt;patient number&gt;.\n\n\nvisit_0\nScore on the GAD7 at baseline\n\n\nvisit_1\nGAD7 at 1 month assessment\n\n\nvisit_2\nGAD7 at 2 month assessment\n\n\nvisit_3\nGAD7 at 3 month assessment\n\n\nvisit_4\nGAD7 at 4 month assessment\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nUh-oh… these data aren’t in the same shape as the other datasets we’ve been giving you..\nCan you get it into a format that is ready for modelling?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nIt’s wide, and we want it long.\n\nOnce it’s long. “visit_0”, “visit_1”,.. needs to become the numbers 0, 1, …\nOne variable (patient) contains lots of information that we want to separate out. There’s a handy function in the tidyverse called separate(), check out the help docs!\n\n\n\n\n\n\n\n\n\n1 - reshaping\n\n\n\nSolution 1. Here’s the data. We have one row per patient, but we have multiple observations for each patient across the columns..\n\ngeduc = read_csv(\"https://uoepsy.github.io/data/lmm_gadeduc.csv\")\nhead(geduc)\n\n# A tibble: 6 × 6\n  patient      visit_0 visit_1 visit_2 visit_3 visit_4\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 VC_Control_1      24      24      26      29      28\n2 VC_Control_2      24      26      28      29      30\n3 VC_Control_3      25      29      27      29      30\n4 VC_Control_4      24      25      25      26      26\n5 VC_Control_5      28      28      27      29      28\n6 VC_Control_6      26      28      25      27      28\n\n\nWe can make it long by taking the all the columns from visit_0 to visit_4 (that is, from the second column 2 to the last column last_col()) and shoving their values into one variable called GAD, and keeping the name of the column they come from as another variable called visit:\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\")\n\n# A tibble: 2,410 × 3\n   patient      visit     GAD\n   &lt;chr&gt;        &lt;chr&gt;   &lt;dbl&gt;\n 1 VC_Control_1 visit_0    24\n 2 VC_Control_1 visit_1    24\n 3 VC_Control_1 visit_2    26\n 4 VC_Control_1 visit_3    29\n 5 VC_Control_1 visit_4    28\n 6 VC_Control_2 visit_0    24\n 7 VC_Control_2 visit_1    26\n 8 VC_Control_2 visit_2    28\n 9 VC_Control_2 visit_3    29\n10 VC_Control_2 visit_4    30\n# ℹ 2,400 more rows\n\n\nThis is step 1 of our data wrangling. In the next step, we’ll pipe the result of pivot_longer() into mutate().\nIn general, building up and running your data wrangling pipeline step by step, the way we’re illustrating here, is a good way to make sure each step of your code really is doing what you think it’s doing.\n\n\n\n\n\n2 - time is numeric\n\n\n\nSolution 2. Now we know how to get our data long, we need to sort out our time variable (visit) and make it into numbers.\nWe can replace all occurrences of the string \"visit_\" in our data with nothingness \"\", and then convert what remains—the visit number—to numeric.\n\ngeduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) \n\n# A tibble: 2,410 × 3\n   patient      visit   GAD\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 VC_Control_1     0    24\n 2 VC_Control_1     1    24\n 3 VC_Control_1     2    26\n 4 VC_Control_1     3    29\n 5 VC_Control_1     4    28\n 6 VC_Control_2     0    24\n 7 VC_Control_2     1    26\n 8 VC_Control_2     2    28\n 9 VC_Control_2     3    29\n10 VC_Control_2     4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\n\n3 - splitting up the patient variable\n\n\n\nSolution 3. Finally, we need to sort out the patient variable. It contains 3 bits of information that we will want to have separated out. It has the therapist (their initials), then the group (treatment or control), and then the patient number. These are all separated by an underscore “_“.\nThe separate() function takes a column and separates it into several things (as many things as we give it), splitting them by some user defined separator such as an underscore:\n\ngeduc_long &lt;- geduc |&gt; \n  pivot_longer(2:last_col(), names_to=\"visit\",values_to=\"GAD\") |&gt;\n  mutate(\n    visit = as.numeric(gsub(\"visit_\",\"\",visit))\n  ) |&gt;\n  separate(patient, into=c(\"therapist\",\"group\",\"patient\"), sep=\"_\")\n\nAnd we’re ready to go!\n\ngeduc_long\n\n# A tibble: 2,410 × 5\n   therapist group   patient visit   GAD\n   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 VC        Control 1           0    24\n 2 VC        Control 1           1    24\n 3 VC        Control 1           2    26\n 4 VC        Control 1           3    29\n 5 VC        Control 1           4    28\n 6 VC        Control 2           0    24\n 7 VC        Control 2           1    26\n 8 VC        Control 2           2    28\n 9 VC        Control 2           3    29\n10 VC        Control 2           4    30\n# ℹ 2,400 more rows\n\n\n\n\n\n\nQuestion 2\n\n\nVisualise the data. Does it look like the treatment had an effect over time? Does it look like the treatment worked when used by every therapist?\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nremember, stat_summary() is very useful for aggregating data inside a plot.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. Here’s the overall picture. The average score on the GAD7 at each visit gets more and more different between the two groups. The treatment looks effective..\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nLet’s split this up by therapist, so we can see the averages across each therapist’s set of patients.\nThere’s clear variability between therapists in how well the treatment worked. For instance, the therapists EU and OD don’t seem to have much difference between their groups of patients.\n\nggplot(geduc_long, aes(x = visit, y = GAD, col = group)) +\n  stat_summary(geom=\"pointrange\") +\n  facet_wrap(~therapist)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFit a model to test if the psychoeducational treatment is associated with greater improvement in anxiety over time.\nStep 1: Choose the appropriate fixed effects.\nStep 2: Think about the grouping structure in the data.\nStep 3: Choose the appropriate random effects.\nNote that the patient variable does not uniquely specify the individual patients. That is, patient “1” from therapist “AO” is a different person from patient “1” from therapist “BJ”.\n\n\n\n\n\n1 - fixed effects\n\n\n\nSolution 5. We want to know if how anxiety (GAD) changes over time (visit) is different between treatment and control (group).\nHopefully this should hopefully come as no surprise1 - it’s an interaction!\n\nlmer(GAD ~ visit * group + ...\n       ...\n     data = geduc_long)\n\n\n\n\n\n\n2 - grouping structure\n\n\n\nSolution 6. We have multiple observations for each of the 482 patients, and those patients are nested within 30 therapists.\nNote that in our data, the patient variable does not uniquely specify the individual patients. i.e. patient “1” from therapist “AO” is a different person from patient “1” from therapist “BJ”. To correctly group the observations into different patients (and not ‘patient numbers’), we need to have therapist:patient.\nSo we capture therapist-level differences in ( ... | therapist) and the patients-within-therapist-level differences in ( ... | therapist:patient):\n\nlmer(GAD ~ visit * group + ...\n       ( ... | therapist) + \n       ( ... | therapist:patient),\n     data = geduc_long)\n\n\n\n\n\n\n3 - random effects\n\n\n\nSolution 7. Note that each patient can change differently in their anxiety levels over time - i.e. the slope of visit could vary by participant.\nLikewise, some therapists could have patients who change differently from patients from another therapist, so visit|therapist can be included.\nEach patient is in one of the two groups - they’re either treatment or control. So we can’t say that “differences in anxiety due to treatment varies between patients”, because for any one patient the “difference in anxiety due to treatment” is not defined in our study design.\nHowever, therapists see multiple different patients, some of which are in the treatment group, and some of which are in the control group. So the treatment effect could be different for different therapists!\n\nmod1 &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|therapist:patient),\n             geduc_long)\n\n\n\n\n\n\n4 - interpreting the model\n\n\n\nSolution 8. The question asked whether the psychoeducational treatment (in the variable group) is associated with greater improvement in anxiety (GAD) over time (visit).\nLet’s take a look at the model summary to find out.\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: GAD ~ visit * group + (1 + visit * group | therapist) + (1 +  \n    visit | therapist:patient)\n   Data: geduc_long\n\nREML criterion at convergence: 8696\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7051 -0.5341  0.0147  0.5412  2.9954 \n\nRandom effects:\n Groups            Name                 Variance Std.Dev. Corr             \n therapist:patient (Intercept)          1.448    1.203                     \n                   visit                1.014    1.007    0.07             \n therapist         (Intercept)          1.602    1.266                     \n                   visit                0.211    0.459    -0.09            \n                   groupTreatment       0.082    0.286    -0.13  0.02      \n                   visit:groupTreatment 0.154    0.392    -0.14 -0.20 -0.49\n Residual                               0.706    0.840                     \nNumber of obs: 2410, groups:  therapist:patient, 482; therapist, 30\n\nFixed effects:\n                     Estimate Std. Error t value\n(Intercept)           25.1910     0.2528   99.63\nvisit                 -0.5780     0.1120   -5.16\ngroupTreatment         0.0313     0.1377    0.23\nvisit:groupTreatment  -0.8375     0.1233   -6.79\n\nCorrelation of Fixed Effects:\n            (Intr) visit  grpTrt\nvisit       -0.073              \ngroupTrtmnt -0.278  0.029       \nvst:grpTrtm -0.067 -0.442 -0.160\n\n\nThe question focuses on the fixed effects, so let’s look at those coefficients and their t-values.\n\nvisit: We see a negative association between visit and GAD: the more therapy visits you have - when you are in the control condition - the less your anxiety. The t-value here is more extreme than 2 (which is, as a rule of thumb, approximately the t-value that represents the boundary of the 95% CI). So we likely have a significant negative association between time and anxiety. If this association is significant, that means we can reject the null hypothesis that there’s no change in anxiety over time.\ngroupTreatment: We see a very small positive association between group and GAD, but the error is much bigger than the estimate itself, and consequently, the t-value is also pretty close to 0. I doubt we can reject the null hypothesis that, on average, there’s no difference between the treatment and control groups at baseline (when visit = 0).\nvisit:groupTreatment: We see a negative coefficient for the interaction between visit and groupTreatment, accompanied by a fairly extreme t-value. We likely have a significant negative interaction. This means that we can reject the null hypothesis that there’s no difference between groups as time goes on. And the negative coefficient means that, as visits increase, the GAD of the treatment group decreases more than the GAD of the control group does. (Note: Interactions are REALLY hard to interpret just based on model coefficients. The best way to interpret them is to look at plots of the data.)\n\nTo see whether our guesses about significance based on the t-values were on the right track, we can re-fit the model using lmerTest.\n\nmod1_test &lt;- lmerTest::lmer(\n  GAD ~ visit*group + \n    (1+visit*group|therapist) +\n    (1+visit|therapist:patient),\n  geduc_long\n)\nsummary(mod1_test)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: GAD ~ visit * group + (1 + visit * group | therapist) + (1 +  \n    visit | therapist:patient)\n   Data: geduc_long\n\nREML criterion at convergence: 8696\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.7051 -0.5341  0.0147  0.5412  2.9954 \n\nRandom effects:\n Groups            Name                 Variance Std.Dev. Corr             \n therapist:patient (Intercept)          1.448    1.203                     \n                   visit                1.014    1.007    0.07             \n therapist         (Intercept)          1.602    1.266                     \n                   visit                0.211    0.459    -0.09            \n                   groupTreatment       0.082    0.286    -0.13  0.02      \n                   visit:groupTreatment 0.154    0.392    -0.14 -0.20 -0.49\n Residual                               0.706    0.840                     \nNumber of obs: 2410, groups:  therapist:patient, 482; therapist, 30\n\nFixed effects:\n                     Estimate Std. Error      df t value Pr(&gt;|t|)    \n(Intercept)           25.1910     0.2528 28.7236   99.63  &lt; 2e-16 ***\nvisit                 -0.5780     0.1120 25.7026   -5.16  2.3e-05 ***\ngroupTreatment         0.0313     0.1377 21.4424    0.23     0.82    \nvisit:groupTreatment  -0.8375     0.1233 18.8876   -6.79  1.8e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) visit  grpTrt\nvisit       -0.073              \ngroupTrtmnt -0.278  0.029       \nvst:grpTrtm -0.067 -0.442 -0.160\n\n\nYes, the effects deemed “significant” based on Satterthwaite’s method are the ones we expected.\nSo yes, it looks like the treatment group does improve more over time, compared to the control group, taking into account all the variability introduced by individual patients and therapists.\n\n\n\n\nQuestion 4\n\n\nFor each of the models below, what is wrong with the random effect structure?\n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. \n\nmodelA &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist)+\n               (1+visit|patient),\n             geduc_long)\n\nThe patient variable doesn’t capture the different patients within therapists, so this actually fits crossed random effects and treats all data where patient==1 as from the same group (even if this includes several different patients’ worth of data from different therapists!)\n\nmodelB &lt;- lmer(GAD ~ visit*group + \n               (1+visit*group|therapist/patient),\n             geduc_long)\n\nUsing the / here means we have the same random slopes fitted for therapists and for patients-within-therapists.\nConcretely, (1+visit*group|therapist/patient) is shorthand for (1 + visit*group | therapist) (which is fine) + (1 + visit*group | patient:therapist) (which is not fine, because we don’t have data for every participant for both groups.\nIn other words, the effect of group can’t vary by patient, so this doesn’t work, hence why we need to split them up into (...|therapist)+(...|therapist:patient).\n\n\n\n\nQuestion 5\n\n\nLet’s suppose that I don’t want the psychoeducation treatment, I just want the standard therapy sessions that the ‘Control’ group received. Which therapist should I go to?\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou don’t need to fit a new model here, you can use the one you fitted above.\nranef() and dotplot.ranef.mer() will help! You can read about ranef in Chapter 2 #making-model-predictions.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. The three best therapists to go to are SZ, AO, or IT.\nWhy?\n\nWe said we don’t care about getting the treatment, so we can ignore the parameters groupTreatment and visit:groupTreatment.\n(Intercept) doesn’t tell us about the effect that each therapist has over time, just how good they’re estimated to be at visit 0.\nvisit is the parameter that tells us about how anxiety changes over time. And because lower GAD is better, we want scores to decrease over time.\n\nSo we can look at which therapists have the most negative slope estimated for visit:\n\nranef(mod1)$therapist |&gt;\n  arrange(visit)\n\n   (Intercept)   visit groupTreatment visit:groupTreatment\nSZ      0.9726 -0.6888        0.24166             -0.22320\nAO      1.1433 -0.6187        0.12344             -0.36512\nIT     -1.1505 -0.5899       -0.07236              0.31511\nYS     -0.4870 -0.5669       -0.06644             -0.03350\nGW      1.2185 -0.4012        0.08380             -0.22423\nYF      1.6864 -0.3062       -0.21447              0.27606\nBT      0.3180 -0.2006        0.02727             -0.27354\nCX     -2.0326 -0.1830        0.07968              0.04233\nYE     -0.0753 -0.1195       -0.11639              0.37761\nBJ      0.4484 -0.1189        0.13246             -0.17041\nOD      0.3721 -0.1095       -0.00822              0.01029\nLI      1.2295 -0.1060        0.05089             -0.30193\nWB     -1.3593 -0.0762        0.12760             -0.22965\nXA     -0.5062 -0.0623       -0.03204              0.28860\nOI     -0.2767 -0.0181       -0.21767              0.33782\nTV     -1.9303  0.0405       -0.04086              0.11940\nDF     -0.4995  0.0754        0.12759              0.03544\nDJ     -1.4707  0.0854        0.06194              0.02057\nPM      0.2754  0.1224       -0.02785              0.04959\nMV      0.9641  0.1595       -0.01262              0.15925\nOE     -0.2371  0.1873        0.13168              0.01462\nRW      0.3232  0.2004        0.10068             -0.31089\nLO      1.8762  0.2146       -0.05419              0.00335\nMY     -0.5791  0.2166       -0.02693              0.08901\nKD      0.4017  0.2669        0.01437             -0.36765\nEU     -1.3717  0.3149       -0.03285              0.26114\nKI      2.7613  0.3277       -0.29719              0.17487\nCS     -1.2525  0.4870       -0.00583             -0.00779\nXQ     -1.3365  0.5703       -0.27314              0.33068\nVC      0.5744  0.8969        0.19601             -0.39784\n\n\nDouble check using the plot:\n\ndotplot.ranef.mer(ranef(mod1))$therapist\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nRecreate this plot.\nThe faint lines represent the model estimated lines for each patient. The points and ranges represent our fixed effect estimates and their uncertainty.\nMake sure you’re plotting model estimates, not the raw data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nyou can get the patient-specific lines using augment() from the broom.mixed package, and the fixed effects estimates using effect() from the effects package.\nremember that the “patient” column doesn’t group observations into unique patients.\nremember you can pull multiple datasets into ggplot:\n\n\nggplot(data = dataset1, aes(x=x,y=y)) + \n  geom_point() + # points from dataset1\n  geom_line(data = dataset2) # lines from dataset2\n\n\nsee more in Chapter 2 #visualising-models\n\n\n\n\n\n\n\n\n\n1 - the relevant parts\n\n\n\nSolution 11. The effects package will give us the fixed effect estimates:\n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\nWe want to get the fitted values for each patient. We can get fitted values using augment(). But the patient variable doesn’t capture the unique patients, it just captures their numbers (which aren’t unique to each therapist).\nSo we can create a new column called upatient which pastes together the therapists initials and the patient numbers\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  )\n\n# A tibble: 2,410 × 17\n     GAD visit group   therapist patient upatient .fitted .resid  .hat .cooksd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;   &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1    24     0 Control VC        1       VC1         24.2 -0.198 0.454 0.0210 \n 2    24     1 Control VC        1       VC1         25.3 -1.28  0.239 0.239  \n 3    26     2 Control VC        1       VC1         26.4 -0.360 0.186 0.0128 \n 4    29     3 Control VC        1       VC1         27.4  1.56  0.294 0.508  \n 5    28     4 Control VC        1       VC1         28.5 -0.522 0.563 0.284  \n 6    24     0 Control VC        2       VC2         24.8 -0.843 0.454 0.383  \n 7    26     1 Control VC        2       VC2         26.2 -0.171 0.239 0.00426\n 8    28     2 Control VC        2       VC2         27.5  0.502 0.186 0.0250 \n 9    29     3 Control VC        2       VC2         28.8  0.174 0.294 0.00633\n10    30     4 Control VC        2       VC2         30.2 -0.153 0.563 0.0246 \n# ℹ 2,400 more rows\n# ℹ 7 more variables: .fixed &lt;dbl&gt;, .mu &lt;dbl&gt;, .offset &lt;dbl&gt;, .sqrtXwt &lt;dbl&gt;,\n#   .sqrtrwt &lt;dbl&gt;, .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;\n\n\n\n\n\n\n\n2 - constructing the plot\n\n\n\nSolution 12. \n\nlibrary(effects)\nlibrary(broom.mixed)\neffplot &lt;- effect(\"visit*group\",mod1) |&gt;\n  as.data.frame()\n\naugment(mod1) |&gt; \n  mutate(\n    upatient = paste0(therapist,patient),\n    .after = patient # place the column next to the patient col\n  ) |&gt;\n  ggplot(aes(x=visit,y=.fitted,col=group))+\n  stat_summary(geom=\"line\", aes(group=upatient,col=group), alpha=.1)+\n  geom_pointrange(data=effplot, aes(y=fit,ymin=lower,ymax=upper,col=group))+\n  labs(x=\"- Month -\",y=\"GAD7\")"
  },
  {
    "objectID": "03ex.html#footnotes",
    "href": "03ex.html#footnotes",
    "title": "W3 Exercises: Nested and Crossed Structures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nif it does, head back to where we learned about interactions in the single level regressions lm(). It’s just the same here.↩︎"
  },
  {
    "objectID": "01ex.html",
    "href": "01ex.html",
    "title": "W1: Regression Refresher",
    "section": "",
    "text": "Workplace Pride\n\nData: lmm_jsup.csv\nA questionnaire was sent to all UK civil service departments, and the lmm_jsup.csv dataset contains all responses that were received. Some of these departments work as hybrid or ‘virtual’ departments, with a mix of remote and office-based employees. Others are fully office-based.\nThe questionnaire included items asking about how much the respondent believe in the department and how it engages with the community, what it produces, how it operates and how treats its people. A composite measure of ‘workplace-pride’ was constructed for each employee. Employees in the civil service are categorised into 3 different roles: A, B and C. The roles tend to increase in responsibility, with role C being more managerial, and role A having less responsibility. We also have data on the length of time each employee has been in the department (sometimes new employees come straight in at role C, but many of them start in role A and work up over time).\nWe’re interested in whether the different roles are associated with differences in workplace-pride.\nDataset: https://uoepsy.github.io/data/lmm_jsup.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\ndepartment_name\nName of government department\n\n\ndept\nDepartment Acronym\n\n\nvirtual\nWhether the department functions as hybrid department with various employees working remotely (1), or as a fully in-person office (0)\n\n\nrole\nEmployee role (A, B or C)\n\n\nseniority\nEmployees seniority point. These map to roles, such that role A is 0-4, role B is 5-9, role C is 10-14. Higher numbers indicate more seniority\n\n\nemployment_length\nLength of employment in the department (years)\n\n\nwp\nComposite Measure of 'Workplace Pride'\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and provide some descriptive plots and statistics of each individual variable.\n\n\n\n\n\n\nHints\n\n\n\n\n\nDon’t remember how to do descriptives? Think back to previous courses – it’s time for some means, standard deviations, mins and maxes. For categorical variables we can do counts or proportions.\nWe’ve seen various functions such as summary(), and also describe() from the psych package.\nFor continuous variables, histograms are a good first port of call: try hist(). For categorical variables, you could try plotting the outcome of table().\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. Here’s the dataset:\n\nlibrary(tidyverse) # for data wrangling\nlibrary(psych) \n\njsup &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\nLet’s take just the numeric variables and get some descriptives:\n\njsup |&gt; \n  select(employment_length, wp) |&gt; \n  describe()\n\n                  vars   n mean   sd median trimmed  mad  min  max range  skew\nemployment_length    1 295 12.6 4.28   13.0    12.6 4.45 0.00 30.0  30.0  0.08\nwp                   2 295 25.5 5.27   25.4    25.5 5.93 6.34 38.5  32.1 -0.05\n                  kurtosis   se\nemployment_length     0.38 0.25\nwp                   -0.14 0.31\n\n\nAnd make frequency tables for the categorical ones:\n\ntable(jsup$role)\n\n\n  A   B   C \n109  95  91 \n\n\nI’m going to use dept rather than department_name as the output will be easier to see:\n\ntable(jsup$dept)\n\n\n   ACE    CMA    CPS    FSA    GLD   HMRC    NCA   NS&I  OFGEM OFQUAL OFSTED \n    17     21     13     25     17     16     20     20     15      5     17 \n OFWAT    ORR    SFO   UKSA   UKSC \n    16     17     18     45     13 \n\ntable(jsup$virtual)\n\n\n  0   1 \n175 120 \n\n\n\n\n\n\nQuestion 2\n\n\nAre there differences in ‘workplace-pride’ between people in different roles?\nFirst, plot these two variables together. Based on this plot and your training from DAPR2, try to answer these questions:\n\nIf you fit a model to this data, how would the predictor be coded?\nWhat coefficients would the model estimate?\nWould the sign of the coefficients be positive or negative?\n\nOnce you’ve made a good effort to predict the answers to these questions, fit a model and see if your predictions are borne out. (If your predictions are different from the outcomes, reflect on why the outcomes are the way they are.)\n\n\n\n\n\n\nHints\n\n\n\n\n\ndoes y [continuous variable] differ by x [three groups]?\nlm(y ~ x)?\nBy default, R uses treatment coding (aka dummy coding), and the reference level is the one that comes first in the alphabet.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\nmod1 &lt;- lm(wp ~ role, data = jsup)\n\nRather than doing summary(model) - I’m just going to use the broom package to pull out some of the stats in nice tidy dataframes.\nThe glance() function will give us things like the \\(R^2\\) values and \\(F\\)-statistic (basically all the stuff that is at the bottom of the summary()):\n\nlibrary(broom)\nglance(mod1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.216         0.211  4.68      40.3 3.44e-16     2  -872. 1753. 1768.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nThe tidy() function will give us the coefficients, standard errors, t-statistics and p-values. It’s the same information, just neater!\n\ntidy(mod1)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    28.1      0.448     62.6  3.66e-171\n2 roleB          -2.24     0.657     -3.41 7.33e-  4\n3 roleC          -5.95     0.665     -8.95 4.38e- 17\n\n\nAlternatively, we can get some quick confidence intervals for our coefficients:\n\nconfint(mod1)\n\n            2.5 % 97.5 %\n(Intercept) 27.17 28.933\nroleB       -3.54 -0.949\nroleC       -7.26 -4.638\n\n\nIt looks like roles do differ in their workplace pride. Specifically, compared to people in role A, people who are in roles B and C on average report less pride in the workplace.\n\n\n\n\n\nQuestion 3\n\n\nOne possibility: Something about the roles themselves makes people report differences in workplace pride.\nAnother possibility: People who are newer to the company feel more pride (they’re less jaded), and people in Role A tend to be newer. So something else is at play, but the model above makes it look like it’s about role.\nIn other words, if we were to compare people in each role but hold constant their employment_length, might we see something different?\nMake a plot that shows all these relevant variables. Based on that plot, have a guess at the following questions:\n\nWhat coefficients will be estimated by a model fit to this data?\nWould the sign of the coefficients be positive or negative?\n\nOnce you’ve made a good effort to predict the answers to these questions, fit a model and see. (If your predictions are different from the outcomes, reflect on why the outcomes are the way they are.)\n\n\n\n\n\n\nHints\n\n\n\n\n\nSo we want to adjust for how long people have been part of the company..\nRemember - if we want to estimate the effect of x on y while adjusting for z, we can do lm(y ~ z + x).\nFor the plot - put something on the x, something on the y, and colour it by the other variable.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\nmod2 &lt;- lm(wp ~ employment_length + role, data = jsup)\n\ntidy(mod2)\n\n# A tibble: 4 × 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)         36.1      0.709     50.9   6.90e-147\n2 employment_length   -0.834    0.0637   -13.1   4.32e- 31\n3 roleB                0.510    0.563      0.906 3.65e-  1\n4 roleC               -0.704    0.663     -1.06  2.89e-  1\n\n\nNote that, after adjusting for employment length, there are no significant differences in wp between roles B or C compared to A.\nIf we plot the data to show all these variables together, we can kind of see why! Given the pattern of wp against employment_length, the wp for different roles are pretty much where we would expect them to be if role doesn’t make any difference (i.e., if role doesn’t shift your wp up or down).\n\nggplot(jsup, aes(x=employment_length,y=wp,col=role))+\n  geom_point(size=3,alpha=.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 4\n\n\nLet’s take a step back and remember what data we actually have. We’ve got 295 people in our dataset, from 16 departments.\nDepartments may well differ in the general amount of workplace-pride people report. People love to say that they work in the “National Crime Agency”, but other departments might not elicit such pride (*cough* HM Revenue & Customs *cough*). We need to be careful not to mistake department differences as something else (like differences due to the job role).\nMake a couple of plots to look at:\n\nhow many of each role we have from each department\nhow departments differ in their employees’ pride in their workplace\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nggplot(jsup, aes(x = role)) + \n  geom_bar()+\n  facet_wrap(~dept)\n\n\n\n\n\n\n\n\nIn this case, it looks like most of the departments have similar numbers of each role, apart from the UKSA (“UK Statistics Authority”), where we’ve got loads more of role A, and very few role C..\nNote also that in the plot below, the UKSA is, on average, full of employees who take a lot of pride in their work. Is this due to the high proportion of people in role A? or is the effect of role we’re seeing more due to differences in departments?\n\nggplot(jsup, aes(x = dept, y = wp)) +\n  geom_boxplot() +\n  scale_x_discrete(labels = label_wrap_gen(35)) + \n  coord_flip()\n\n\n\n\n\n\n\n\nEven if we had perfectly equal numbers of roles in each department, we’re also adjusting for other things such as employment_length, and the extent to which this differs by department can have trickle-on effects on our coefficient of interest (the role coefficients).\n\n\n\n\nQuestion 5\n\n\nAdjusting for both length of employment and department, are there differences in ‘workplace-pride’ between the different roles? Fit a model to find out.\nCan you make a plot of all four of the variables involved in our model?\n\n\n\n\n\n\nHints\n\n\n\n\n\nMaking the plot might take some thinking. We’ve now added dept into the mix, so a nice way might be to use facet_wrap() to make the same plot as the one we did previously, but for each department.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 5. \n\nmod3 &lt;- lm(wp ~ employment_length + dept + role, data = jsup)\ntidy(mod3)\n\n# A tibble: 19 × 5\n   term              estimate std.error statistic   p.value\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)        36.4       0.631    57.6    7.17e-156\n 2 employment_length  -0.882     0.0344  -25.7    4.71e- 75\n 3 deptCMA            -3.80      0.649    -5.85   1.39e-  8\n 4 deptCPS            -0.217     0.730    -0.298  7.66e-  1\n 5 deptFSA             4.74      0.625     7.60   4.71e- 13\n 6 deptGLD             0.0582    0.682     0.0853 9.32e-  1\n 7 deptHMRC           -3.79      0.692    -5.47   1.02e-  7\n 8 deptNCA            -3.85      0.655    -5.88   1.18e-  8\n 9 deptNS&I           -0.574     0.654    -0.878  3.81e-  1\n10 deptOFGEM          -0.648     0.705    -0.919  3.59e-  1\n11 deptOFQUAL         -4.94      1.01     -4.89   1.71e-  6\n12 deptOFSTED         -5.88      0.683    -8.61   5.52e- 16\n13 deptOFWAT          -1.21      0.692    -1.75   8.17e-  2\n14 deptORR            -2.85      0.681    -4.18   3.98e-  5\n15 deptSFO            -1.36      0.672    -2.02   4.47e-  2\n16 deptUKSA            4.28      0.576     7.43   1.32e- 12\n17 deptUKSC           -2.31      0.732    -3.16   1.77e-  3\n18 roleB               1.42      0.303     4.68   4.47e-  6\n19 roleC               1.31      0.366     3.59   3.92e-  4\n\n\nIn a way, adding predictors to our model is kind of like splitting up our plots by that predictor to see the patterns. This becomes more and more difficult (/impossible) as we get more variables, but right now we can split the data into all the constituent parts.\n\nggplot(jsup, aes(x = employment_length, y = wp, col = role)) +\n  geom_point(size=3,alpha=.4)+\n  facet_wrap(~dept)\n\n\n\n\n\n\n\n\nThe association between wp and employment_length is clear in all these little sub-plots - there’s a downward trend. The department differences can be seen too: UKSA is generally a bit higher, HMRC and UKSC a bit lower, and so on. By default, the model captures these coefficients as ‘differences from the reference group’, so all these coefficients are in relation to the “ACE” department.\nSeeing the role differences is a bit harder in this plot, but think about what you would expect to see if there were no differences in roles (i.e. imagine if they were all in role A). Take for instance the FSA department, where this is easiest to see - for the people who are in role C, for people of their employment length we would expect their wp to be lower if they were in role A. Likewise for those in role B. Across all these departments, the people in role B and C (green and blue dots respectively) are a bit higher than we would expect. This is what the model coefficients tell us!\n\n\n\n\nQuestion 6\n\n\nNow we’re starting to acknowledge the grouped structure of our data - these people in our dataset are related to one another in that some belong to dept 1, some dept 2, and so on..\nLet’s try to describe our sample in a bit more detail.\n\nhow many participants do we have, and from how many departments?\nhow many participants are there, on average, from each department? what is the minimum and maximum?\nwhat is the average employment length for our participants?\nhow many departments are ‘virtual departments’ vs office-based?\n\nwhat is the overall average reported workplace-pride?\nhow much variation in workplace-pride is due to differences between departments?\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe first lot of these questions can be answered using things like count(), summary(), table(), mean(), min() etc. See 1: Clustered Data #determining-sample-sizes\nFor the last one, we can use the ICC! See 1: Clustered Data #icc\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. How many respondents do we have, and from how many departments?\n\nnrow(jsup)\n\n[1] 295\n\nlength(table(jsup$dept))\n\n[1] 16\n\n\nHow many respondents are there, on average, from each dept? What is the minimum and maximum number of people in any one department?\n\njsup |&gt;\n  count(dept) |&gt; \n  summarise(min=min(n),\n            max=max(n),\n            median=median(n)\n  )\n\n# A tibble: 1 × 3\n    min   max median\n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n1     5    45     17\n\n\nWhat is the average employment length of respondents?\n\nmean(jsup$employment_length)\n\n[1] 12.6\n\n\nHow many departments are virtual vs office based? This requires a bit more than just table(jsup$virtual), because we are describing a variable at the department level.\n\njsup |&gt; \n  group_by(virtual) |&gt;\n  summarise(\n    ndept = n_distinct(dept)\n  )\n\n# A tibble: 2 × 2\n  virtual ndept\n    &lt;dbl&gt; &lt;int&gt;\n1       0    11\n2       1     5\n\n\nWhat is the overall average ‘workplace-pride’? What is the standard deviation?\n\nmean(jsup$wp)\n\n[1] 25.5\n\nsd(jsup$wp)\n\n[1] 5.27\n\n\nFinally, how much variation in workplace-pride is attributable to department-level differences?\n\nICC::ICCbare(x = dept, y = wp, data = jsup)\n\n[1] 0.439\n\n\n\n\n\n\nQuestion 7\n\n\nWhat if we would like to know whether, when adjusting for differences due to employment length and department and roles, workplace-pride differs between people working in virtual-departments compared to office-based ones?\nCan you add this to the model? What happens?\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Let’s add the virtual predictor to our model. Note that we don’t actually get a coefficient here - it is giving us an NA!\n\nmod4 &lt;- lm(wp ~ employment_length + dept + role + virtual, data = jsup)\n\nsummary(mod4)\n\n\nCall:\nlm(formula = wp ~ employment_length + dept + role + virtual, \n    data = jsup)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.690 -1.404 -0.027  1.178  5.054 \n\nCoefficients: (1 not defined because of singularities)\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        36.3522     0.6310   57.61  &lt; 2e-16 ***\nemployment_length  -0.8817     0.0344  -25.66  &lt; 2e-16 ***\ndeptCMA            -3.7969     0.6490   -5.85  1.4e-08 ***\ndeptCPS            -0.2173     0.7304   -0.30  0.76627    \ndeptFSA             4.7448     0.6245    7.60  4.7e-13 ***\ndeptGLD             0.0582     0.6822    0.09  0.93212    \ndeptHMRC           -3.7859     0.6924   -5.47  1.0e-07 ***\ndeptNCA            -3.8503     0.6549   -5.88  1.2e-08 ***\ndeptNS&I           -0.5737     0.6537   -0.88  0.38095    \ndeptOFGEM          -0.6479     0.7050   -0.92  0.35885    \ndeptOFQUAL         -4.9413     1.0104   -4.89  1.7e-06 ***\ndeptOFSTED         -5.8846     0.6831   -8.61  5.5e-16 ***\ndeptOFWAT          -1.2087     0.6917   -1.75  0.08169 .  \ndeptORR            -2.8452     0.6813   -4.18  4.0e-05 ***\ndeptSFO            -1.3550     0.6719   -2.02  0.04469 *  \ndeptUKSA            4.2820     0.5759    7.43  1.3e-12 ***\ndeptUKSC           -2.3131     0.7325   -3.16  0.00177 ** \nroleB               1.4179     0.3029    4.68  4.5e-06 ***\nroleC               1.3148     0.3663    3.59  0.00039 ***\nvirtual                 NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.98 on 276 degrees of freedom\nMultiple R-squared:  0.867, Adjusted R-squared:  0.859 \nF-statistic:  100 on 18 and 276 DF,  p-value: &lt;2e-16\n\n\nSo what is happening? If we think about it, if we separate out “differences due to departments” then there is nothing left to compare between departments that are virtual vs office based. Adding the between-department predictor of virtual doesn’t explain anything more - the residual sums of squares doesn’t decrease at all:\n\nanova(\n  lm(wp ~ employment_length + dept + role, data = jsup),\n  lm(wp ~ employment_length + dept + role + virtual, data = jsup)\n)\n\nAnalysis of Variance Table\n\nModel 1: wp ~ employment_length + dept + role\nModel 2: wp ~ employment_length + dept + role + virtual\n  Res.Df  RSS Df Sum of Sq F Pr(&gt;F)\n1    276 1084                      \n2    276 1084  0         0         \n\n\nAnother way of thinking about this: knowing the average workplace-pride for the department that someone is in tells me what to expect about that person’s workplace pride. But once I know their department’s average workplace-pride, knowing whether it is ‘virtual’ or ‘office-based’ doesn’t tell me anything new, for the very fact that the virtual/office-based distinction comes from comparing different departments.\nBut we’re not really interested in these departments specifically! What would be nice would be if we can look at the relevant effects of interest (things like role and virtual), but then just think of the department differences as just some sort of random variation. So we want to think of departments in a similar way to how we think of our individual employees - they vary randomly around what we expect - only they’re at a different level of observation."
  },
  {
    "objectID": "00prereq.html",
    "href": "00prereq.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Install/Update R & RStudio\nMake sure you have installed both R and RStudio on your computer. You may have done this previously for DAPR2, in which case it is probably worth doing some updates.\nPlease make sure to read and follow the instructions below slowly and carefully!!\n\nFor instructions on how to install R and RStudio, click here\nFor instructions on how to update R and RStudio, click here\n\n\n\nUpdate Packages\nIt’s worth keeping packages up to date, so it might be worth updating all your packages.\nRunning this code will update all your packages. Just put it into the console (bottom left bit of RStudio):\n\noptions(pkgType = \"binary\")\nupdate.packages(ask = FALSE)\n\n\n\nNew packages!\nNow it is probably worth installing a few of the packages that we will be using in DAPR3. There are a few that we will need. For each one, check whether you have it already installed, because there’s not much point wasting time re-installing something you already have!\n\ntidyverse : for organising data\n\nlme4 : for fitting generalised linear mixed effects models\nbroom.mixed : tidying methods for mixed models\neffects : for tabulating and graphing effects in linear models\nlmerTest: for quick p-values from mixed models\nparameters: various inferential methods for mixed models\npsych: for factor analysis\nlavaan: for latent variable models"
  },
  {
    "objectID": "00tutors.html",
    "href": "00tutors.html",
    "title": "Tutors",
    "section": "",
    "text": "In Week 1, start all labs with a quick:\n“hello. almost all of you are well practiced at DAPR labs now. take a seat, say hello to your colleagues, open Rstudio, and get started on the exercises on Learn.\nwork together with people on your table - explain stuff to one another, troubleshoot together etc.\nfor any questions, we have lovely tutors wandering around, just grab one, or put your hand up etc, and they will be able to help.\nThe groups for the report will not be set until week 3, so for now just sit with whoever you want”"
  },
  {
    "objectID": "00tutors.html#tutor-guidelines",
    "href": "00tutors.html#tutor-guidelines",
    "title": "Tutors",
    "section": "Tutor guidelines",
    "text": "Tutor guidelines\n\ntry to do less:\n\nstanding around chatting\nusing your phone\nsitting with one student for too long\n\ntry to do more:\n\nmake sure every table gets an introduction to a tutor (“hello, i’m X, how are you getting on? etc”)\nmake sure tables don’t get ignored (even if no hands are going up)\nif all is quiet, try actively asking a table 1) where they’re up to in the exercises, 2) if they need any help etc."
  },
  {
    "objectID": "02ex.html",
    "href": "02ex.html",
    "title": "W2 Exercises: Introducing MLM",
    "section": "",
    "text": "These first set of exercises are not “how to do analyses with multilevel models” - they are designed to get you thinking, and help with an understanding of how these models work.\n\n\nQuestion 1\n\n\nRecall the data from last week’s exercises. Instead of looking at the roles A, B and C, we’ll look in more fine grained detail at the seniority. This is mainly so that we have a continuous variable to work with as it makes this illustration easier.\nThe chunk of code below shows a function for plotting that you might not be familiar with - stat_summary(). This takes the data in the plot and “summarises” the Y-axis variable into the mean at every unique value on the x-axis. So below, rather than having a lot of individual data points that represent every employee’s wp (workplace pride), we let stat_summary() compute the mean (the points) plus and minus the standard error (the vertical lines) of all the wp observations at each value of seniority:\n\nlibrary(tidyverse)\njsup &lt;- read_csv(\"https://uoepsy.github.io/data/lmm_jsup.csv\")\n\nggplot(jsup, aes(x = seniority, y = wp, col = role)) +\n  stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nBelow is some code that fits a model of the workplace-pride predicted by seniority level. Line 2 then gets the ‘fitted’ values from the model and adds them as a new column to the dataset, called pred_lm. The fitted values are what the model predicts the workplace pride to be for every value of seniority.\nLines 4-7 then plot the data, split up by each department, and adds lines showing the model fitted values.\nRun the code and check that you get a plot. What do you notice about the lines?\n\nlm_mod &lt;- lm(wp ~ seniority, data = jsup)\njsup$pred_lm &lt;- predict(lm_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_lm), col = \"red\")\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. We should get something like this:\n\nlm_mod &lt;- lm(wp ~ seniority, data = jsup)\njsup$pred_lm &lt;- predict(lm_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_lm), col = \"red\")\n\n\n\n\n\n\n\n\nNote that the lines are exactly the same for each department. This makes total sense, because the model (which is where we’ve got the lines from) completely ignores the department variable!\n\n\n\n\nQuestion 2\n\n\nBelow are 3 more code chunks that all 1) fit a model, then 2) add the fitted values of that model to the plot.\nThe first model is a ‘no-pooling’ approach, similar to what we did in last week’s exercises - adding in dept as a predictor.\nThe second and third are multilevel models. The second fits random intercepts by-department, and the third fits random intercepts and slopes of seniority.\nCopy each chunk and run through the code. Pay attention to how the lines differ.\n\n\nCode\nfe_mod &lt;- lm(wp ~ dept + seniority, data = jsup)\njsup$pred_fe &lt;- predict(fe_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_fe), col = \"blue\")\n\n\n\n\nCode\nlibrary(lme4)\nri_mod &lt;- lmer(wp ~ seniority + (1|dept), data = jsup)\njsup$pred_ri &lt;- predict(ri_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_ri), col = \"green\")\n\n\n\n\nCode\nrs_mod &lt;- lmer(wp ~ seniority + (1 + seniority|dept), data = jsup)\njsup$pred_rs &lt;- predict(rs_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_rs), col = \"orange\")\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. With the first model, wp ~ seniority + dept, we are saying the following things:\n\nWe allow the model to estimate an association between workplace pride and seniority. This line is estimated without respect to department, or in other words, for the “average” department.\nWe also allow the model to take that line and shift it up and down for each different department. The slope of the line stays the same. All that’s changing is the vertical position of the line.\n\nLook at UKSA and FSA: the lines are shifted up compared to the others.\nLook at UKSC and OFSTED: the lines are shifted down compared to the others.\n\n\n\nfe_mod &lt;- lm(wp ~ dept + seniority, data = jsup)\njsup$pred_fe &lt;- predict(fe_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_fe), col = \"blue\")\n\n\n\n\n\n\n\n\nWith the second model, wp ~ seniority + (1|dept), we are saying the following things:\n\nWe allow the model to estimate an association between workplace pride and seniority.\nWith (1|dept), we allow the model to adjust the intercept of that line, depending on the scores of each department.\n\nIn other words, the wp ~ seniority bit is modelling the average line for all departments, and the (1|dept) bit is saying “now nudge that line up or down so that it fits the data of each individual department better”.\nThis is very similar to the fixed-effects-based model above. The difference is that the nudges up and down are now drawn from a single distribution, so they are all slightly closer to the mean of departments than the changes that the model above made.\nPeople often call these “random intercepts by department”. You may also hear “intercept adjustments by department”.\nIf we look at this model’s summary, we can see how it combines all these individual adjustments into a distribution and give us some summary statistics. This way, we can see how big the adjustments tend to be.\n\n\nSide note: Why use random effects instead of fixed effects, if they do the same thing?\n\nThey don’t really do the same thing: random effects can also include random slopes, which the fixed-effect model cannot do. We’ll see random slopes next.\nFixed effects are generally used for predictors that we have particular hypotheses/predictions about. Random effects are used when we have other sources of non-independence we need to tell the model about.\n\n\nlibrary(lme4)\nri_mod &lt;- lmer(wp ~ seniority + (1|dept), data = jsup)\njsup$pred_ri &lt;- predict(ri_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_ri), col = \"green\")\n\n\n\n\n\n\n\n\nFinally, with the third model, wp ~ seniority + (1 + seniority|dept), we are saying the following things:\n\nWe allow the model to estimate an association between workplace pride and seniority, for departments on average. That’s the wp ~ seniority bit, like before.\nWith (1 + seniority|dept), we allow the model to adjust the intercept of that line AND to adjust the slope of that line for each individual department.\n\nIn other words, the wp ~ seniority bit is modelling the average line for all departments, and the (1 + seniority|dept) bit is saying “now nudge that line up or down AND change how steep it is, so that it fits the data of each individual department better”.\nPeople often call these “random intercepts by department and random slopes over seniority by department”. You may also hear “intercept adjustments by department and adjustments to the slope of seniority by department”.\nIf we look at this model’s summary, we can see how it combines all these individual adjustments into a distribution of intercept adjustments and a distribution of slope adjustments, and give us some summary statistics about both. This way, we can see how big the adjustments tend to be.\n\n\nSo in this next plot, the height of the lines is changing, but additionally, each department’s association between seniority and workplace pride can be different. Some departments (OFQUAL, OFSTED, ORR) have a negative association, some have a flatter association (e.g, FSA, UKSA etc).\n\nrs_mod &lt;- lmer(wp ~ seniority + (1 + seniority|dept), data = jsup)\njsup$pred_rs &lt;- predict(rs_mod)\n\nggplot(jsup, aes(x = seniority)) + \n  geom_point(aes(y = wp), size=1, alpha=.3) +\n  facet_wrap(~dept) +\n  geom_line(aes(y=pred_rs), col = \"orange\")\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\nFrom the previous questions you should have a model called ri_mod.\nBelow is a plot of the fitted values from that model. Rather than having a separate facet for each department as we did above, I have put them all on one plot. The thick black line is the average intercept and slope of the departments lines.\nIdentify the parts of the plot that correspond to A1-4 in the summary output of the model below\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nChoose from these options:\n\nwhere the black line cuts the y axis (at x=0)\n\nthe slope of the black line\n\nthe standard deviation of the distances from all the individual datapoints (employees) to the line for the department in which it works.\n\nthe standard deviation of the distances from all the individual department lines to the black line\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1 = the standard deviation of the distances from all the individual department lines to the black line\n\nA2 = the standard deviation of the distances from all the individual datapoints (employees) to the line for the department in which it works.\nA3 = where the black line cuts the y axis\n\nA4 = the slope of the black line\n\n\n\n\n\nOptional Extra\n\n\nBelow is the model equation for the ri_mod model.\nIdentify the part of the equation that represents each of A1-4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n\\text{For Employee }j\\text{ from Dept }i & \\\\\n\\text{Level 1 (Employee):}& \\\\\n\\text{wp}_{ij} &= b_{0i} + b_1 \\cdot \\text{seniority}_{ij} + \\epsilon_{ij} \\\\\n\\text{Level 2 (Dept):}& \\\\\nb_{0i} &= \\gamma_{00} + \\zeta_{0i} \\\\\n\\text{Where:}& \\\\\n\\zeta_{0i} &\\sim N(0,\\sigma_{0}) \\\\\n\\varepsilon &\\sim N(0,\\sigma_{e}) \\\\\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nChoose from:\n\n\\(\\sigma_{\\varepsilon}\\)\n\n\\(b_{1}\\)\n\n\\(\\sigma_{0}\\)\n\n\\(\\gamma_{00}\\)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. \n\nA1 = \\(\\sigma_{0}\\)\n\nA2 = \\(\\sigma_{\\varepsilon}\\)\n\nA3 = \\(\\gamma_{00}\\)\n\nA4 = \\(b_{1}\\)"
  },
  {
    "objectID": "02ex.html#footnotes",
    "href": "02ex.html#footnotes",
    "title": "W2 Exercises: Introducing MLM",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is like taking predict() from the model, and then then grouping by age, and calculating the mean of those predictions. However, we can do this more easily using augment() and then some fancy stat_summary() in ggplot↩︎\nprovided that the confidence intervals and p-values are constructed using the same methods↩︎"
  },
  {
    "objectID": "04ex.html",
    "href": "04ex.html",
    "title": "W4 Exercises: Centering",
    "section": "",
    "text": "Hangry\n\nData: hangry1.csv\nThe study is interested in evaluating whether levels of hunger are associated with levels of irritability (i.e., “the hangry hypothesis”). 81 participants were recruited into the study. Once a week for 5 consecutive weeks, participants were asked to complete two questionnaires, one assessing their level of hunger, and one assessing their level of irritability. The time and day at which participants were assessed was at a randomly chosen hour between 7am and 7pm each week.\nThe data are available at: https://uoepsy.github.io/data/hangry1.csv.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nq_irritability\nScore on irritability questionnaire (0:100)\n\n\nq_hunger\nScore on hunger questionnaire (0:100)\n\n\nppt\nParticipant\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRemember that what we’re interested in is “whether levels of hunger are associated with levels of irritability (i.e., the hangry hypothesis).”\nRead in the data, call the data frame hangry, and fit the model below. How well does it address the research question?\n\nmod1 &lt;- lmer(q_irritability ~ q_hunger + \n                (1 + q_hunger | ppt), \n                data = hangry)\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nAlways plot your data! It’s tempting to just go straight to interpreting coefficients of this model, but in order to understand what a model says we must have a theory about how the data are generated.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. \n\nhangry &lt;- read_csv(\"https://uoepsy.github.io/data/hangry1.csv\")\n\nmod1 &lt;- lmer(q_irritability ~ q_hunger + \n                (1 + q_hunger | ppt), \n                data = hangry)\n\nThe model above will give us that same old formulaic expression of “for people on average, a 1 unit increase in q_hunger is associated with a 0.17 increase in q_irritability”.\nThe problem is that in trying to estimate what does q_irritability do as q_hunger increases, we’re ignoring the fact that people tend to have different average levels of q_hunger:\n\nggplot(hangry, aes(x = q_hunger, y = q_irritability, group = ppt)) +\n  geom_point() +\n  geom_line(alpha=.4) + \n  facet_wrap(~ppt)\n\n\n\n\n\n\n\n\nSo if we try to interpret the fixed effect of our model above as “what happens to a persons’ irritability when they are 1 more hungry?”, we’re not accurately estimating this because our model doesn’t account for the fact that the numbers in q_hunger mean very different things for different people - for person 1 a hunger score of 60 might be “I’m really hungry”, but for person 2 (who usually has a hunger score in the 80s or 90s), 60 could mean “I’m not very hungry at all”.\n\n\n\n\nQuestion 2\n\n\n\n\n\n\n\n\nwithin effects, between effects, and smushed effects\n\n\n\n\n\nResearch Question: are levels of hunger associated with levels of irritability (i.e., the hangry hypothesis)?\nThink about the relationship between irritability and hunger. How should we interpret this research aim?\nIs it:\n\n“Are people more irritable if they are, on average, more hungry than other people?”\n\n“Are people more irritable if they are, for them, more hungry than they usually are?”\n\nSome combination of both a. and b.\n\nThis is just one demonstration of how the statistical methods we use can constitute an integral part of our development of a research project, and part of the reason that data analysis for scientific cannot be so easily outsourced after designing the study and collecting the data.\nAs our data currently is currently stored, the relationship between q_irritability and the raw scores on the hunger questionnaire q_hunger represents some ‘total effect’ of hunger on irritability. This is a bit like interpretation c. above - it’s a combination of both the ‘within’ ( b. ) and ‘between’ ( a. ) effects. The problem with this is that this isn’t necessarily all that meaningful. It may tell us that ‘being higher on the hunger questionnaire is associated with being more irritable’, but how can we apply this information? It is not specifically about the comparison between hungry people and less hungry people, and nor is it a good estimation of how person \\(i\\) changes when they are more hungry than usual. It is both these things smushed together.\nTo disaggregate the ‘within’ and ‘between’ effects of hunger on irritability, we can group-mean center. For ‘between’, we are interested in how irritability is related to the average hunger levels of a participant, and for ‘within’, we are asking how irritability is related to a participants’ relative levels of hunger (i.e., how far above/below their average hunger level they are).\n\n\n\nAdd to the data these two columns:\n\na column which contains the average hungriness score for each participant. Call this column hunger_btwn_ppts, since we’ll use it to look at the between-person effect of hunger on irritability.\na column which contains the deviation from each person’s hunger score to that person’s average hunger score. (In other words, each hunger score minus that person’s average hunger score.) Call this column hunger_wi_ppts, since we’ll use it to look at the within-person effect of hunger on irritability.\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll find group_by() |&gt; mutate() very useful here, as seen in Chapter 10 #group-mean-centering.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. \n\nhangry &lt;- \n    hangry |&gt; group_by(ppt) |&gt;\n        mutate(\n            hunger_btwn_ppts = mean(q_hunger),\n            hunger_wi_ppts = q_hunger - hunger_btwn_ppts\n        )\nhead(hangry)\n\n# A tibble: 6 × 5\n# Groups:   ppt [2]\n  q_irritability q_hunger ppt   hunger_btwn_ppts hunger_wi_ppts\n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1             42       52 N2p1              39.2          12.8 \n2             24       47 N2p1              39.2           7.8 \n3             17        8 N2p1              39.2         -31.2 \n4             26       47 N2p1              39.2           7.8 \n5             27       42 N2p1              39.2           2.80\n6             17       48 N2p2              39.6           8.4 \n\n\n\n\n\n\nQuestion 3\n\n\nFor each of the new variables you just added, plot the irritability scores against those variables.\n\nDoes it look like hungry people are more irritable than less hungry people? (This is the between effect, because we’re looking at differences between people’s average hungriness.)\nDoes it look like when people are more hungry than normal, they are more irritable? (This is the within effect, because we’re looking at deviations within each individual person’s hungriness.)\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou might find stat_summary() useful here for plotting the between effect (see Chapter 10 #group-mean-centering)\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 3. Let’s start just by making a scatterplot of each person’s average hunger, plotted against all of their irritability scores.\n\nggplot(hangry,aes(x=hunger_btwn_ppts,y=q_irritability))+\n  geom_point()\n\n\n\n\n\n\n\n\nThe reason we kind of see the points forming vertical columns is because each person’s mean hunger score is located at a single point on the x axis, while their irritability scores take on different values.\nThis is a lot of data, though. We might find it easier to look at a plot where each participant is represented as their mean irritability, along with an indication of the standard error of their irritability scores:\n\nggplot(hangry,aes(x=hunger_btwn_ppts,y=q_irritability))+\n    stat_summary(geom=\"pointrange\")\n\n\n\n\n\n\n\n\nIt’s hard to see any clear relationship between a persons’ average hunger and their irritability scores here.\nIt is also a bit difficult to get at the relationship between participant-centered hunger and irritability, because there are a lot of different lines (one for each participant). To make it easier to get an idea of what’s happening, we’ll make the plot fit a simple lm() (a straight line) for each participants’ data.\n\nggplot(hangry,aes(x=hunger_wi_ppts,y=q_irritability, group=ppt)) +\n  geom_point(alpha = .2) + \n  geom_smooth(method=lm, se=FALSE, lwd=.2)\n\n\n\n\n\n\n\n\nIt looks like most of these lines are sloping upwards, but there’s a fair bit of variability in people’s slopes: some people’s lines are going up, some are going down.\nSo we can actually make a guess at what we’re going to see when we model. We’ll probably have a positive fixed effect of hunger_wi_ppts (i.e. A below will be positive), and the by-participant variation in these slopes will be quite large relative to the fixed effect (i.e B below will be quite large in comparison to A)\nRandom effects:\n Groups   Name             Variance Std.Dev. Corr \n ppt      (Intercept)           ...      ... \n          hunger_wi_ppts        ...      *B*\n Residual                       ...      ...    \n\n...\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)      ...        ...     ...\nhunger_wi_ppts   *A*        ...     ...\n...              ...        ...     ...\n\n\n\n\nQuestion 4\n\n\nWe have taken the raw hunger scores and separated them into two parts (raw hunger scores = participants’ average hunger score + observation-level deviations from those averages). Those two parts represent two different aspects of the relationship between hunger and irritability.\nAdjust your model specification to include these two separate variables (hunger_btwn_ppts and hunger_wi_ppts) as predictors, instead of the raw hunger scores.\nInclude the appropriate random effects.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nWe can only put one of these variables in the random effects (1 + hunger | participant). Think about the fact that each participant has only one value for their average hungriness.\n\nIf the model fails to converge, and if it’s a fairly simple model (i.e one or two random slopes), then often you can switch optimizer (see Chapter 2 #convergence-warnings-singular-fits). For instance, try adding control = lmerControl(optimizer = \"bobyqa\") to the model.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. With the defaults, this model doesn’t converge\n\nhangrywb &lt;- lmer(q_irritability ~ hunger_btwn_ppts + hunger_wi_ppts + \n                (1 + hunger_wi_ppts | ppt), \n                data = hangry)\n\nWarning message:\nIn checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,  :\n  Model failed to converge with max|grad| = 0.0027028 (tol = 0.002, component 1)\nChanging the optimizer helps:\n\nhangrywb &lt;- lmer(q_irritability ~ hunger_btwn_ppts + hunger_wi_ppts + \n                (1 + hunger_wi_ppts | ppt), \n                data = hangry,\n                control = lmerControl(optimizer = \"bobyqa\"))\n\n\n\n\n\n\n\noptional - why change the optimizer?\n\n\n\n\n\nNote that the max|grad| convergence error of the initial model was very close to the tolerance (see Chapter 8 #non-convergence for an explanation of what this tolerance is).\nThe fact that it is close indicates that we may be quite close to a solution, so it’s worth investigating if this is simply an optimizer problem.\nOne other thing to do would be to consider all available optimizers, see which ones converge, and compare estimates across them. If the estimates are the same (or pretty close), and some of these converge, then it gives us more trust in our model. We can do this with the code below. We can see that 5 optimizers don’t give error messages, and that they all give pretty much the same estimated fixed effects. We can go further and compare random effects variances too, but we won’t do that here.\n\n# fit with all optimizers\nallopts = allFit(hangrywb)\n\nbobyqa : [OK]\nNelder_Mead : [OK]\nnlminbwrap : [OK]\nnloptwrap.NLOPT_LN_NELDERMEAD : [OK]\nnloptwrap.NLOPT_LN_BOBYQA : \n\n\n[OK]\n\n\n\n# error messages from each optimizer \n# (NULL here means no message, which is good)\nsummary(allopts)$msgs\n\n$bobyqa\nNULL\n\n$Nelder_Mead\nNULL\n\n$nlminbwrap\nNULL\n\n$nloptwrap.NLOPT_LN_NELDERMEAD\nNULL\n\n$nloptwrap.NLOPT_LN_BOBYQA\n[1] \"Model failed to converge with max|grad| = 0.0027028 (tol = 0.002, component 1)\"\n\n# fixed effect estimates for all optimizers\nsummary(allopts)$fixef\n\n                              (Intercept) hunger_btwn_ppts hunger_wi_ppts\nbobyqa                               17.6         -0.00644          0.187\nNelder_Mead                          17.6         -0.00644          0.187\nnlminbwrap                           17.6         -0.00644          0.187\nnloptwrap.NLOPT_LN_NELDERMEAD        17.6         -0.00647          0.187\nnloptwrap.NLOPT_LN_BOBYQA            17.6         -0.00648          0.187\n\n\n\n\n\n\n\n\n\nQuestion 5\n\n\nWrite down what each of the fixed effects means.\n\n\n\n\n\nSolution\n\n\n\nSolution 5. Here are the fixed effects:\n\nfixef(hangrywb)\n\n     (Intercept) hunger_btwn_ppts   hunger_wi_ppts \n        17.62005         -0.00644          0.18663 \n\n\n\n\n\n\n\n\n\n\nterm\nest\ninterpretation\n\n\n\n\n(Intercept)\n17.620\nestimated irritability score for someone with an average hunger of 0, and not deviating from that average (i.e. hunger_wi_ppts = 0)\n\n\nhunger_btwn_ppts\n-0.006\nestimated difference in irritability between two people who differ in average hunger level by 1 (e.g., a person with average hunger of 11 vs someone with average hunger level of 10), when they are at their average (hunger_wi_ppts = 0)\n\n\nhunger_wi_ppts\n0.187\nestimated change in irritability score for every 1 more hungry a person is than they normally are\n\n\n\n\n\n\n\n\n\n\n\nQuestion 6\n\n\nHave a go at also writing an explanation for yourself of the random effects part of the output (i.e., the stuff that comes out when you run the code VarCorr(MODELNAMEHERE)).\nThere’s no formulaic way to interpret these, but have a go at describing in words what they represent, and how that adds to the picture your model describes.\nDon’t worry about making it read like a report - just write yourself an explanation!\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\nVarCorr(hangrywb)\n\n Groups   Name           Std.Dev. Corr \n ppt      (Intercept)    6.992         \n          hunger_wi_ppts 0.366    -0.08\n Residual                4.772         \n\n\n\n\n\n\n\n\n\n\nterm\nest\ninterpretation\n\n\n\n\nsd__(Intercept)\n6.992\nParticipant level variability in irritability when they are at their average hunger level - i.e. when everybody is at their own average level of hunger, they vary in their irritability scores with a standard deviation of 7.\n\n\nsd__hunger_wi_ppts\n0.366\nParticipants vary quite a bit in how deviations from hunger are associated with irritability. They vary around the fixed effect of 0.19 with a standard deviation of 0.37. To think about what this means, imagine a normal distribution that is centered on 0.19 and has a standard deviation of 0.36. A fairly large portion of that distribution would fall below zero (i.e. have a negative slope). And we would also expect some slopes that are e.g., .5, .6 etc.\n\n\ncor__(Intercept).hunger_wi_ppts\n-0.080\nThis estimate is basically zero, but it represents the relationship between participants' relative standing at the intercept and their relative standing on the slopes. So participants who are more irritable than others when at their average hunger, tend to have very very slightly more negative slopes. In other words, people who are more irritable when at average hunger become irritable *slightly* slower, as they get hungrier, compared to people who are less irritable\n\n\nsd__Observation\n4.772\nthe residual variance doesn't really have much of an interpretation - it really just represents all the leftover stuff that the model doesn't explain. If we imagine all of the individual participant lines, 4.77 represents how spread out (on the scale of irritability scores) the individual observations are around those lines\n\n\n\n\n\n\n\n\n\n\n\n\nHangry 2\n\nQuestion 7\n\n\nA second dataset on the same variables is available at: https://uoepsy.github.io/data/hangry2.csv.\nThese data are from people who were following a five-two diet, while the original dataset were from people who were not following any diet. (On the five-two diet, people eat normally for five days a week, but then restrict their intake for two days a week.)\nCombine the datasets together so we can fit a model to see if the hangry effect differs between people on diets vs those who aren’t.\nCall the new dataframe hangryfull.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nSomething like bind_rows() might help here. If you’ve not seen it before, remember that you can look up the help documentation in the bottom-right panel of RStudio.\nBe sure to keep an indicator of which group the data are in in a column called diet.\n\nFor example, use mutate() to add an identifier column to each one before binding.\nOr use the .id argument of bind_rows() to identify the original data frame. (Check the documentation to see how to use .id!)\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Here are our two datasets:\n\nhangry1 &lt;- read_csv(\"https://uoepsy.github.io/data/hangry1.csv\")\nhangry2 &lt;- read_csv(\"https://uoepsy.github.io/data/hangry2.csv\")\n\nIf we simply bind them together using bind_rows() like this…\n\nhangryfull &lt;- \n  bind_rows(\n    hangry1, \n    hangry2\n  )\n\n… then we wouldn’t know which data was from which group!\nThere are a couple ways of adding identifiers to keep the non-diet data and the diet data apart.\nOne way: Add a new identifier variable to each data frame first.\n\nhangryfull &lt;- \n  bind_rows(\n    hangry1 |&gt; mutate(diet = \"N\"), \n    hangry2 |&gt; mutate(diet = \"Y\")\n  )\nhead(hangryfull)\n\n# A tibble: 6 × 4\n  q_irritability q_hunger ppt   diet \n           &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1             42       52 N2p1  N    \n2             24       47 N2p1  N    \n3             17        8 N2p1  N    \n4             26       47 N2p1  N    \n5             27       42 N2p1  N    \n6             17       48 N2p2  N    \n\n\nAnother way: Within bind_rows(), you can assign each data frame a name that represents its value in the identifier column. And you can give the identifier column a name using the argument .id, as follows:\n\nhangryfull &lt;-\n  bind_rows(\n  'N' = hangry1, \n  'Y' = hangry2,\n  .id = 'diet'\n  )\n\nBoth methods have exactly the same result! You can pick your favourite way.\n\n\n\n\nQuestion 8\n\n\nDoes the relationship between hunger and irritability depend on whether or not people are following the five-two diet?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWhich relationship between hunger and irritability are we talking about? The between effect or the within effect? It could be both!\nTo fit the model we want, we’ll need to create those two variables (hunger_btwn_ppts and hunger_wi_ppts) for this combined dataset again.\nThis model will also require a variable that tells us whether people were on the diet or not (the identifier variable from before). We’ll also need to figure out whether we can get random slopes over the new diet variable. Can we include random slopes for each participant over diet? Why or why not?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 8. \n\nhangryfull &lt;- \n    hangryfull |&gt; group_by(ppt) |&gt;\n        mutate(\n            hunger_btwn_ppts = mean(q_hunger),\n            hunger_wi_ppts = q_hunger - hunger_btwn_ppts\n        )\n\nhangrywbdiet &lt;- lmer(q_irritability ~ (hunger_btwn_ppts + hunger_wi_ppts) * diet + \n                (1 + hunger_wi_ppts | ppt), \n                data = hangryfull,\n                control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(hangrywbdiet, corr = FALSE)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: q_irritability ~ (hunger_btwn_ppts + hunger_wi_ppts) * diet +  \n    (1 + hunger_wi_ppts | ppt)\n   Data: hangryfull\nControl: lmerControl(optimizer = \"bobyqa\")\n\nREML criterion at convergence: 2735\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.4138 -0.5906 -0.0454  0.5426  2.3954 \n\nRandom effects:\n Groups   Name           Variance Std.Dev. Corr \n ppt      (Intercept)    48.083   6.934         \n          hunger_wi_ppts  0.145   0.381    -0.01\n Residual                23.305   4.828         \nNumber of obs: 405, groups:  ppt, 81\n\nFixed effects:\n                        Estimate Std. Error t value\n(Intercept)             17.13095    5.14648    3.33\nhunger_btwn_ppts         0.00386    0.10533    0.04\nhunger_wi_ppts           0.18577    0.07560    2.46\ndietY                  -10.85470    6.53568   -1.66\nhunger_btwn_ppts:dietY   0.46590    0.13354    3.49\nhunger_wi_ppts:dietY     0.38141    0.10139    3.76\n\n\n\n\n\n\nQuestion 9\n\n\nConstruct two plots, one for each of the interactions that the model estimates. This model is a bit of a confusing one, so plotting may help a bit with understanding what those interactions represent.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\neffects(terms, mod) |&gt; as.data.frame() |&gt; ggplot(.....)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 9. The xlevels bit here just gives us the little dataframe to plot with more levels at it, so that it gives us smoother lines. Try it with and without to see what I mean!\n\nlibrary(effects)\neffect(\"hunger_btwn_ppts*diet\", hangrywbdiet, xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=hunger_btwn_ppts, y=fit,col=diet))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=diet),alpha=.4)+\n  labs(x=\"participants' average hunger level\")\n\n\n\n\n\n\n\n\nWe saw in our original model that for the reference level of diet, the “N” group, there was no association between how hungry a person is on average and their irritability. This is the red line we see in the plot above. In our full model this is the hunger_btwn_ppts coefficient.\nWe also saw the interaction hunger_btwn_ppts:dietY indicates that irritability is estimated to increase by 0.47 more for those in the diet than it does for those not on the diet. So the blue line is should be going up more steeply than the red line (which is flat). And it is!\n\neffect(\"hunger_wi_ppts*diet\", hangrywbdiet, xlevels=20) |&gt;\n  as.data.frame() |&gt;\n  ggplot(aes(x=hunger_wi_ppts, y=fit,col=diet))+\n  geom_line()+\n  geom_ribbon(aes(ymin=lower,ymax=upper,fill=diet),alpha=.4)+\n  labs(x=\"increase from participants' average hunger level\")\n\n\n\n\n\n\n\n\nFrom the coefficient of hunger_wi_ppts we get the estimated amount by which irritability increases for every 1 more hungry that a person becomes (when they’re in the diet “N” group). This is the slope of the red line - the hunger_wi_ppts coefficient from our full model.\nThe interaction hunger_wi_ppts:fivetwo1 gave us the adjustment to get from the red line to the blue line. It is positive which matches with the fact that the blue line is steeper in this plot. In other words, if people are on a diet, then as their hunger increases relative to their average, they become irritable faster than people who are not on a diet.\n\n\n\n\nQuestion 10\n\n\nRun significance tests for the fixed effects, and write up the results.\n\n\n\n\n\nSolution\n\n\n\nSolution 10. \n\nhangrywbdiet.p &lt;- lmerTest::lmer(q_irritability ~ (hunger_btwn_ppts + hunger_wi_ppts) * diet + \n                (1 + hunger_wi_ppts | ppt), \n                data = hangryfull,\n                control=lmerControl(optimizer=\"bobyqa\"))\n\nsummary(hangrywbdiet.p)$coefficients\n\n                        Estimate Std. Error   df t value Pr(&gt;|t|)\n(Intercept)             17.13095     5.1465 77.0  3.3287 0.001341\nhunger_btwn_ppts         0.00386     0.1053 77.0  0.0367 0.970834\nhunger_wi_ppts           0.18577     0.0756 65.4  2.4573 0.016659\ndietY                  -10.85470     6.5357 77.0 -1.6608 0.100813\nhunger_btwn_ppts:dietY   0.46590     0.1335 77.0  3.4888 0.000806\nhunger_wi_ppts:dietY     0.38141     0.1014 68.5  3.7617 0.000352\n\n\nTo investigate the association between irritability and hunger, and whether this relationship is different depending on whether or not participants are on the five-two diet, a multilevel linear model was fitted.\nTo disaggregate between the differences in irritability due to people being in general more/less hungry, and those due to people being more/less hungry than usual for them, irritability was regressed onto both participants’ average hunger scores and their relative hunger levels. Both of these were allowed to interact with whether or not participants were on the five-two diet. Random intercepts and slopes over relative-hunger level were included for participants. The model was fitted with restricted maximum likelihood estimation with the lme4 package (Bates et al., 2015), using the bobyqa optimiser. \\(P\\)-values were obtained using the Satterthwaite approximation for degrees of freedom.\nResults indicate that for people on no diet, being more hungry than normal was associated with greater irritability (\\(b = 0.19,\\ SE = 0.08,\\ t(65.41) = 2.46,\\ p=0.017\\)), and that this association was further increased for those following the five-two diet (\\(b = 0.38,\\ SE = 0.1,\\ t(68.49) = 3.76,\\ p&lt;0.001\\)).\nFor those not on a specific diet, there was no evidence for an association between irritability and being generally a more hungry person (\\(p=0.971\\)). However, there was a significant interaction between average hunger and being on the five-two diet (\\(b = 0.47,\\ SE = 0.13,\\ t(77) = 3.49,\\ p&lt;0.001\\)), suggesting that when dieting, hungrier people tend to be more irritable than less hungry people.\nResults suggest that the ‘hangry hypothesis’ may occur within people (when a person is more hungry than they usually are, they tend to be more irritable), but not necessarily between hungry/less hungry people. Dieting was found to increase the association with irritability of both between-person hunger and within-person hunger."
  },
  {
    "objectID": "07ex.html",
    "href": "07ex.html",
    "title": "W7 Exercises: Scale Scores & PCA",
    "section": "",
    "text": "Gambler’s fallacy\n\nDataset: gamblers.csv\nA researcher is interested in assessing if people who gamble will tend to lose more if they are more ‘impulsive’, and whether this might depend on whether they are gambling online or in a casino.\nThey recruited 482 participants (248 in a casino, and 234 on an online gambling site). Each participant filled out a 6 question measure of “impulsivity”, and then their total net gains (or losses) for the day were recorded (in £). All people were only playing the game BlackJack.\nOur research question: does greater impulsivity lead to bigger losses when comparing online gamblers to casino gamblers?\nDataset: The data can be found at https://uoepsy.github.io/data/gamblers.csv\n\n\n\n\nTable 1: gamblers.csv Data Dictionary\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nonline\nwhether the person was gambling in a casino or online\n\n\nimp_1\nI often act on the spur of the moment without thinking.\n\n\nimp_2\nI find it hard to resist temptations.\n\n\nimp_3\nI make decisions quickly, even when they have serious consequences.\n\n\nimp_4\nI find it hard to stay focused on tasks that take a long time to finish.\n\n\nimp_5\nI prefer safe activities rather than risky things just for fun.\n\n\nimp_6\nI am usually patient and can wait for what I want.\n\n\ngain\nnet losses or gains upon leaving/logging out\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data and have a look at it.\n\nWhat does each row represent?\n\nWhat measurement(s) show us a person’s impulsivity?\n\n\n\n\n\n\nSolution\n\n\n\nSolution 1. Here’s the data:\n\ngdat &lt;- read_csv(\"https://uoepsy.github.io/data/gamblers.csv\")\nhead(gdat)\n\n# A tibble: 6 × 8\n  online imp_1                      imp_2          imp_3 imp_4 imp_5 imp_6  gain\n  &lt;chr&gt;  &lt;chr&gt;                      &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n1 casino Disagree                   Neither Disag… Disa… Neit… Neit… Neit…     0\n2 casino Disagree                   Disagree       Disa… Neit… Agree Agree   -12\n3 casino Neither Disagree nor Agree Neither Disag… Neit… Neit… Neit… Disa…   -35\n4 casino Neither Disagree nor Agree Agree          Neit… Agree Agree Neit…   -20\n5 online Disagree                   Neither Disag… Disa… Neit… Neit… Neit…   -14\n6 casino Neither Disagree nor Agree Disagree       Disa… Stro… Neit… Neit…   -28\n\n\nEach row is a participant, for each person there are 6 columns all measuring the construct of “impulsivity”.\nAnd for each of those columns, there’s a whole load of words in there!\n\n\n\n\nQuestion 2\n\n\nFirst things first, our questionnaire software has given us the responses all in the descriptors used for each point of the likert scale, which is a bit annoying.\nConvert them all to numbers, which we can then work with.\n\n\n\nWhat we have\nWhat we want\n\n\n\n\nStrongly Agree\n5\n\n\nAgree\n4\n\n\nAgree\n4\n\n\nStrongly Disagree\n1\n\n\nNeither Disagree nor Agree\n3\n\n\nAgree\n4\n\n\nDisagree\n2\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee 1: Data Wrangling for Questionnaires#variable-recoding.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 2. We want to turn all of the variables from imp_1 to imp_6 into numbers.\nTo do it with one variable:\n\ngdat |&gt; mutate(\n  imp_1 = case_match(imp_1,\n                     \"Strongly Disagree\" ~ 1,\n                     \"Disagree\" ~ 2,\n                     \"Neither Disagree nor Agree\" ~ 3,\n                     \"Agree\" ~ 4,\n                     \"Strongly Agree\" ~ 5\n  )\n)\n\nAnd we can do it to all at once with across().\n\ngdat &lt;- gdat |&gt; mutate(\n  across(c(imp_1:imp_6),\n         ~case_match(.,\n                     \"Strongly Disagree\" ~ 1,\n                     \"Disagree\" ~ 2,\n                     \"Neither Disagree nor Agree\" ~ 3,\n                     \"Agree\" ~ 4,\n                     \"Strongly Agree\" ~ 5\n         ))\n  )\n\nhead(gdat)\n\n# A tibble: 6 × 8\n  online imp_1 imp_2 imp_3 imp_4 imp_5 imp_6  gain\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 casino     2     3     2     3     3     3     0\n2 casino     2     2     2     3     4     4   -12\n3 casino     3     3     3     3     3     2   -35\n4 casino     3     4     3     4     4     3   -20\n5 online     2     3     2     3     3     3   -14\n6 casino     3     2     2     1     3     3   -28\n\n\n\n\n\n\nQuestion 3\n\n\nJust looking at the impulsivity questions, create a correlation matrix of 6 variables.\nWhat do you notice? Does it make sense given the wording of the questions?\n\n\n\n\n\nSolution\n\n\n\nSolution 3. \n\ncor(gdat[,2:7])\n\n       imp_1  imp_2  imp_3  imp_4  imp_5  imp_6\nimp_1  1.000  0.470  0.555  0.044 -0.464 -0.453\nimp_2  0.470  1.000  0.414  0.321 -0.204 -0.305\nimp_3  0.555  0.414  1.000  0.102 -0.406 -0.461\nimp_4  0.044  0.321  0.102  1.000 -0.244 -0.334\nimp_5 -0.464 -0.204 -0.406 -0.244  1.000  0.390\nimp_6 -0.453 -0.305 -0.461 -0.334  0.390  1.000\n\n\nCorrelations are all positive except for those with Q5 and Q6. Q5 and Q6 are positively related, but they are negatively related to the other questions.\nIf you’re like Elizabeth and you have a hard time making sense of a matrix full of numbers, then the function heatmap() might be your friend. It represents the correlation matrix as colours. Cells representing strong positive correlations appear darker, and cells representing strong negative correlations appear lighter.\n\ncor(gdat[,2:7]) |&gt;\n  heatmap()\n\n\n\n\n\n\n\n\nThere’s a dark diagonal line down the middle, which represents the perfect correlation between an item and itself. The cells at the intersections of Q5 and Q6 are fairly dark, and the cells at the intersections of Q1–Q4 are also fairly dark (the positive correlations). Where Q5 and Q6 meet Q1–Q4, the cells are fairly light (the negative correlations). In addition to the bracketing structure at the plot margins (in technical terms a “dendrogram”), this visual approach to a correlation matrix shows that Q5 and Q6 tend to pattern together in one way, and that the other four Qs tend to pattern together in another way.\nThis makes sense given the way the questions are worded - if people are impulsive, they will be more likely to disagree to Q5 and Q6, but agree with the others:\n\nqitems\n\n[1] \"I often act on the spur of the moment without thinking.\"                 \n[2] \"I find it hard to resist temptations.\"                                   \n[3] \"I make decisions quickly, even when they have serious consequences.\"     \n[4] \"I find it hard to stay focused on tasks that take a long time to finish.\"\n[5] \"I prefer safe activities rather than risky things just for fun.\"         \n[6] \"I am usually patient and can wait for what I want.\"                      \n\n\n\n\n\n\nQuestion 4\n\n\nReverse score questions 5 and 6.\n\n\n\n\n\n\nHints\n\n\n\n\n\n\nSee 1: Data Wrangling for Questionnaires#reverse-coding\nBe careful!! if you have some code that reverse scores a question, and you run it twice, you will essentially reverse-reverse score the question, and it goes back to the original ordering!\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 4. There’s only 2, so let’s do this individually for each question:\n\ngdat &lt;- gdat |&gt; \n  mutate(\n    imp_5 = 6 - imp_5,\n    imp_6 = 6 - imp_6,\n)\nhead(gdat)\n\n# A tibble: 6 × 8\n  online imp_1 imp_2 imp_3 imp_4 imp_5 imp_6  gain\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 casino     2     3     2     3     3     3     0\n2 casino     2     2     2     3     2     2   -12\n3 casino     3     3     3     3     3     4   -35\n4 casino     3     4     3     4     2     3   -20\n5 online     2     3     2     3     3     3   -14\n6 casino     3     2     2     1     3     3   -28\n\n\n\n\n\n\nQuestion 5\n\n\nTake a look at the correlation of the impulsivity questions again.\nWhat has changed?\n\n\n\n\n\nSolution\n\n\n\nSolution 5. The negative correlations are now positive!\n\ncor(gdat[,2:7])\n\n      imp_1 imp_2 imp_3 imp_4 imp_5 imp_6\nimp_1 1.000 0.470 0.555 0.044 0.464 0.453\nimp_2 0.470 1.000 0.414 0.321 0.204 0.305\nimp_3 0.555 0.414 1.000 0.102 0.406 0.461\nimp_4 0.044 0.321 0.102 1.000 0.244 0.334\nimp_5 0.464 0.204 0.406 0.244 1.000 0.390\nimp_6 0.453 0.305 0.461 0.334 0.390 1.000\n\n\n\n\n\n\nQuestion 6\n\n\nWe’re finally getting somewhere! Let’s create a score for “impulsivity” and add it as a new column onto the existing data frame.\nThe description of the questionnaire says that we should take the sum of the scores on each question, to get an overall measure of impulsivity.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe function rowSums() should help us here! See an example in 1: Data Wrangling for Questionnaires#row-scoring\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. \n\ngdat$impulsivity &lt;- rowSums(gdat[,2:7])\n\n\n\n\n\nQuestion 7\n\n\nProvide some descriptive statistics for the impulsivity scale scores of people at the two locations (online vs casino).\n\n\n\n\n\n\nHints\n\n\n\n\n\nThe describe() and describeBy() functions from the psych package can often pretty useful for this kind of thing. Alternatively, data |&gt; group_by(...) |&gt; summarise(....)!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. \n\nlibrary(psych)\ndescribeBy(gdat$impulsivity, group=gdat$online)\n\n\n Descriptive statistics by group \ngroup: casino\n   vars   n mean   sd median trimmed  mad min max range  skew kurtosis  se\nX1    1 248 16.8 3.22     17    16.8 2.97   7  25    18 -0.11    -0.13 0.2\n------------------------------------------------------------ \ngroup: online\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis  se\nX1    1 234 19.8 3.12     20    19.8 2.97  11  30    19 0.09    -0.06 0.2\n\n\nThe tidyverse way:\n\ngdat |&gt;\n  group_by(online) |&gt;\n  summarise(\n    mean = mean(impulsivity),\n    median = median(impulsivity),\n    min = min(impulsivity),\n    max = max(impulsivity),\n    sd = sd(impulsivity)\n  )\n\n# A tibble: 2 × 6\n  online  mean median   min   max    sd\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 casino  16.8     17     7    25  3.22\n2 online  19.8     20    11    30  3.12\n\n\n\n\n\n\nQuestion 8\n\n\nDoes greater impulsivity lead to bigger losses when comparing online gamblers to casino gamblers?\nUsing the scale scores that you just computed, create a plot to show how impulsivity is associated with gains/losses of gamblers in the two places (casino vs online).\n\n\n\n\n\nSolution\n\n\n\nSolution 8. Something like this should do the trick:\n\nggplot(gdat,aes(x=impulsivity,y=gain,col=online))+\n  geom_point(size = 3, alpha = .3)+\n  geom_smooth(method=lm)\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nBased on the plot in the previous question, if you fit the model lm(gain ~ impulsivity * online) to this data (where impulsivity is the scale score), what coefficients would the model estimate? would the sign of each coefficient be positive or negative?\nOnce you’ve made a good effort to predict the answers to these questions, fit the model and see if your predictions are borne out. (If your predictions are different from the outcomes, reflect on why the outcomes are the way they are.)\n\n\n\n\n\nSolution\n\n\n\nSolution 9. Here’s the model:\n\nmod1 &lt;- lm(gain ~ online * impulsivity, data = gdat)\n\nIt’s going to estimate 4 things:\n\ntibble(\n  estimate = c(\"intercept\",\"onlineonline\",\"impulsivity\",\"onlineonline:impulsivity\"),\n  prediction = c(\"around zero/slightly negative\",\"positive\",\"negative\",\"negative\"),\n  explanation = c(\"the 'online' variable is coded with casino as the reference level, so the intercept is going to be the height of the casino line where impulsivity is 0. so it looks like it will be around 0, or a bit below.\",\n                 \"this coefficient will tell us the difference between casino and online when impulsivity is zero. the blue line in the plot is going to be higher than the red line when impulsivity is zero, so this coefficient will be positive\",\n                 \"this is going to be how gains/losses change when impulsivity increases, specifically for the casino group. so in my plot it is the slope of the red line. it's going to be decreasing\",\n                 \"this is going to be how the association between impulsivity and gains/losses changes when we move from casino to online. We know this association is negative in the casino group, and the online group looks like it is even more steeply downwards, so this is going to be a negative coefficient\"\n                 )\n) |&gt; gt::gt()\n\n\n\n\n\n\n\nestimate\nprediction\nexplanation\n\n\n\n\nintercept\naround zero/slightly negative\nthe 'online' variable is coded with casino as the reference level, so the intercept is going to be the height of the casino line where impulsivity is 0. so it looks like it will be around 0, or a bit below.\n\n\nonlineonline\npositive\nthis coefficient will tell us the difference between casino and online when impulsivity is zero. the blue line in the plot is going to be higher than the red line when impulsivity is zero, so this coefficient will be positive\n\n\nimpulsivity\nnegative\nthis is going to be how gains/losses change when impulsivity increases, specifically for the casino group. so in my plot it is the slope of the red line. it's going to be decreasing\n\n\nonlineonline:impulsivity\nnegative\nthis is going to be how the association between impulsivity and gains/losses changes when we move from casino to online. We know this association is negative in the casino group, and the online group looks like it is even more steeply downwards, so this is going to be a negative coefficient\n\n\n\n\n\n\n\n\nsummary(mod1)$coefficients\n\n                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                -2.415      3.488  -0.692 0.489038\nonlineonline               12.192      5.563   2.192 0.028884\nimpulsivity                -0.737      0.204  -3.610 0.000338\nonlineonline:impulsivity   -0.533      0.297  -1.793 0.073646\n\n\n\n\n\n\nQuestion 10\n\n\nTake a look again at the wordings of the questions on impulsivity. Do you think they equally represent the construct of ‘impulsivity’?\nIf you’re stuck, think about whether each question might be measuring something else, in addition to (or instead of?) impulsivity.\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nimp_1\nI often act on the spur of the moment without thinking.\n\n\nimp_2\nI find it hard to resist temptations.\n\n\nimp_3\nI make decisions quickly, even when they have serious consequences.\n\n\nimp_4\nI find it hard to stay focused on tasks that take a long time to finish.\n\n\nimp_5\nI prefer safe activities rather than risky things just for fun.\n\n\nimp_6\nI am usually patient and can wait for what I want.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is a very subjective question. “Impulsivity” will mean subtly different things to each one of us. The idea is that we want to get at whatever idea it is that is shared across us when we use this word. To me, one of these questions feels a little less closely linked to being an ‘impulsive’ behaviour than the others.\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 10. I’m going to rank them in order of how much I think each question captures “impulsivity” to me.\nI hope that some of you disagree with me about this ranking - that’s what makes measurement frustrating fun!\n\n\n\n\n\n\n\n\nvariable\ndescription\nmy_thoughts\n\n\n\n\nimp_1\nI often act on the spur of the moment without thinking.\nclearly impulsivity\n\n\nimp_3\nI make decisions quickly, even when they have serious consequences.\ncould be impulsivity, could be that you're really good at making decisions\n\n\nimp_6\nI am usually patient and can wait for what I want.\nsimilar to imp_2, impatience and impulsivity kind of go hand in hand, but this is not quite so clearly the definition of impulsivity as the first two\n\n\nimp_2\nI find it hard to resist temptations.\n'temptations' here makes me immediately think of edible temptations! which is one manifestation of impulsivity i guess!\n\n\nimp_5\nI prefer safe activities rather than risky things just for fun.\nis risk taking the same as impulsivity? you can take calculated risks? people do 'risky' sports like climbing for fun, but not out of impulsivity?\n\n\nimp_4\nI find it hard to stay focused on tasks that take a long time to finish.\nthis doesn't really feel like it is as clearly impulsivity. lots of things can distract us from tasks. boredom?\n\n\n\n\n\n\n\n\n\n\n\nQuestion 11\n\n\nOkay, so if we’re not very happy that our 6 questions are equally representative of “impulsivity” (or maybe groups of questions capture distinct aspects of the construct?), we might not want to work with the plain old sum of impulsivity scores that we used above.\nWhat are we going to do?\nLet’s start by doing a Principal Component Analysis (PCA) on the 6 original items, and extracting 6 components.\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee Chapter 3: PCA walkthrough for the demonstration!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 11. \n\nimppca &lt;- principal(gdat[,2:7], nfactors = 6, rotate = \"none\")\nimppca\n\nPrincipal Components Analysis\nCall: principal(r = gdat[, 2:7], nfactors = 6, rotate = \"none\")\nStandardized loadings (pattern matrix) based upon correlation matrix\n       PC1   PC2   PC3   PC4   PC5   PC6 h2       u2 com\nimp_1 0.78 -0.38 -0.09  0.10 -0.32  0.35  1 -4.4e-16 2.4\nimp_2 0.66  0.16 -0.66  0.19 -0.08 -0.26  1 -1.1e-15 2.6\nimp_3 0.76 -0.31 -0.07 -0.16  0.54  0.06  1 -8.9e-16 2.3\nimp_4 0.41  0.87  0.04  0.07  0.10  0.24  1  5.6e-16 1.7\nimp_5 0.67 -0.05  0.56  0.45  0.01 -0.19  1 -1.1e-15 2.9\nimp_6 0.73  0.14  0.23 -0.56 -0.22 -0.17  1 -4.4e-16 2.5\n\n                       PC1  PC2  PC3  PC4  PC5  PC6\nSS loadings           2.78 1.04 0.81 0.59 0.46 0.32\nProportion Var        0.46 0.17 0.14 0.10 0.08 0.05\nCumulative Var        0.46 0.64 0.77 0.87 0.95 1.00\nProportion Explained  0.46 0.17 0.14 0.10 0.08 0.05\nCumulative Proportion 0.46 0.64 0.77 0.87 0.95 1.00\n\nMean item complexity =  2.4\nTest of the hypothesis that 6 components are sufficient.\n\nThe root mean square of the residuals (RMSR) is  0 \n with the empirical chi square  0  with prob &lt;  NA \n\nFit based upon off diagonal values = 1\n\n\n\n\n\n\nQuestion 12\n\n\nTake a look at the ‘variance accounted for’ by each component (you could use a scree plot to show this too!), and think back to our research question, which has absolutely nothing to do with whether “impulsivity” is one thing, or two things, or 6 things…\nHow many components do you want to keep?\n\n\n\n\n\nSolution\n\n\n\nSolution 12. Our research question, remember is asking “does greater impulsivity lead to bigger losses when comparing online gamblers to casino gamblers?”\nWe’re getting bogged down in the weeds of what do we even mean by ‘impulsivity’?? I would make a case that our research question kind of pre-supposes that “impulsivity” is just one thing. If we reduce these 6 questions down to two or more things, then our research question becomes a little bit more complex to answer, with lots of nuance about what sort of impulsivity we’re talking about. So from a purely pragmatic standpoint, I am really hoping we can just keep one thing, and call that thing “impulsivity”!\nAs it stands, the scree plot is in our favour. It shows a kink (or ‘elbow’, if you’d prefer) at 2 components, suggesting that we would be fine to keep just one.\n\nscree(gdat[,2:7])\n\n\n\n\n\n\n\n\nThe numbers going into the scree plot are shown in the ‘variance accounted for’ bit of the PCA. These tell us that if we keep just one component, then we are capturing 47% of the variability in the questionnaire. If we kept two, then we would capture 64%, 3 would capture 77%, and so on, until we just keep 6 and we’re capturing 100%.\n\nimppca$Vaccounted\n\n                        PC1   PC2   PC3   PC4    PC5    PC6\nSS loadings           2.782 1.043 0.810 0.594 0.4552 0.3158\nProportion Var        0.464 0.174 0.135 0.099 0.0759 0.0526\nCumulative Var        0.464 0.637 0.772 0.872 0.9474 1.0000\nProportion Explained  0.464 0.174 0.135 0.099 0.0759 0.0526\nCumulative Proportion 0.464 0.637 0.772 0.872 0.9474 1.0000\n\n\nThere’s no “right” answer here as to how much we should keep. 47% makes it feel like we’re losing more than we’re capturing (which we are), but that might just be what we have to do!\n\n\n\n\nQuestion 13\n\n\nExtract the scores for the first principal component, and attach them to your dataset as a new set of scores for “impulsivity”.\nAttend also to the loadings for that first component - is it related more to the questions you felt were more clearly asked about ‘impulsivity’?\n\n\n\n\n\n\nHints\n\n\n\n\n\nTo extract the scores, see Chapter 3: PCA walkthrough #scores .\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 13. Here are the scores added to our data:\n\ngdat$pc1 &lt;- imppca$scores[,1]\n\nAnd here are the loadings for that first component. It’s less related to imp_4 and imp_2, and more related to imp_1 and imp_3 (this kind of fits in with my view that these two questions are more obviously asking about “impulsivity” to me).\n\nsort(imppca$loadings[,1])\n\nimp_4 imp_2 imp_5 imp_6 imp_3 imp_1 \n0.411 0.660 0.670 0.733 0.762 0.782 \n\n\nThe nice result of this is that, if we use these scores and loadings in subsequent analyses, the questions that are better at targeting the construct we care about are going to be weighted more heavily than the questions that are worse at targeting that construct. The PCA scores are therefore a more accurate way of representing impulsivity than the un-weighted summed score we computed above.\n\n\n\n\nQuestion 14\n\n\nUsing your PCA scores, not the old summed scale scores, create a plot that shows the relationship between impulsivity and financial loss or gain in the two different locations (casino and online). What changes, compared to the old plot?\nNext, fit a new linear model that uses the PCA scores, not the old summed scale scores, to address the question of how impulsivity might affect gains in different locations. What changes, compared to the old model?\n\n\n\n\n\nSolution\n\n\n\nSolution 14. \n\nlibrary(patchwork)\n\np1 &lt;- ggplot(gdat,aes(x=impulsivity,y=gain,col=online))+\n  geom_point(size = 3, alpha = .3)+\n  geom_smooth(method=lm) +\n  labs(title=\"scale scores\")\n\np2 &lt;- ggplot(gdat,aes(x=pc1,y=gain,col=online))+\n  geom_point(size = 3, alpha=.3)+\n  geom_smooth(method=lm) +\n  labs(title=\"PC scores\")\n\np1 + p2 + plot_layout(guides=\"collect\")\n\n\n\n\n\n\n\n\nThere are more different pc1 values than there are different scale score values. The scale score values can only be integers (that is, whole numbers), while the pc1 values can be decimal numbers too. This makes sense, because if the principal component scores are some weighting of each variable, then we’re going to end up with far more possible scores.\n\nmod2 &lt;- lm(gain ~ online * pc1, data = gdat)\n\nsjPlot::tab_model(mod1, mod2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \ngain\ngain\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n-2.41\n-9.27 – 4.44\n0.489\n-15.91\n-17.32 – -14.51\n&lt;0.001\n\n\nonline [online]\n12.19\n1.26 – 23.12\n0.029\n2.72\n0.68 – 4.75\n0.009\n\n\nimpulsivity\n-0.74\n-1.14 – -0.34\n&lt;0.001\n\n\n\n\n\nonline [online] ×\nimpulsivity\n-0.53\n-1.12 – 0.05\n0.074\n\n\n\n\n\npc1\n\n\n\n-2.70\n-4.09 – -1.30\n&lt;0.001\n\n\nonline [online] × pc1\n\n\n\n-2.16\n-4.19 – -0.12\n0.038\n\n\nObservations\n482\n482\n\n\nR2 / R2 adjusted\n0.091 / 0.085\n0.105 / 0.099\n\n\n\n\n\n\n\nNote that the better measurement of “impulsivity” by the PCA (weighting our scores more towards imp_1 and imp_3) results in a significant interaction here.\nIt’s also worth noting that the online [online] coefficient in the new model is also significant, but that is because the PCA scores are standardised, whereas the scale scores are not. So “0” means something very different on those two measures."
  },
  {
    "objectID": "09ex.html",
    "href": "09ex.html",
    "title": "W9 Exercises: CFA",
    "section": "",
    "text": "Exercises for the Enthusiastic\n\nDataset: radakovic_das.csv\nApathy is lack of motivation towards goal-directed behaviours. It is pervasive in a majority of psychiatric and neurological diseases, and impacts everyday life. Traditionally, apathy has been measured as a one-dimensional construct but is in fact composed of different types of demotivation.\nThe Dimensional Apathy Scale (DAS) is a multidimensional assessment for demotivation, in which 3 subtypes of apathy are assessed:\n\nExecutive: lack of motivation for planning, attention or organisation\nEmotional: lack of emotional motivation (indifference, affective or emotional neutrality, flatness or blunting)\nInitiation: lack of motivation for self-generation of thoughts and/or actions\n\nThe DAS measures these subtypes of apathy and allows for quick and easy assessment, through self-assessment, observations by informants/carers or administration by researchers or healthcare professionals.\nYou can find data for the DAS when administered to 250 healthy adults at https://uoepsy.github.io/data/radakovic_das.csv, and information on the items is below.\n\n\n\n\n\n\nDAS Dictionary\n\n\n\n\n\nAll items are measured on a 6-point Likert scale of Always (0), Almost Always (1), Often (2), Occasionally (3), Hardly Ever (4), and Never (5). Certain items (indicated in the table below with a - direction) are reverse scored to ensure that higher scores indicate greater levels of apathy.\n\n\n\n\n\n\n\n\nitem\ndirection\ndimension\nquestion\n\n\n\n\n1\n+\nExecutive\nI need a bit of encouragement to get things started\n\n\n2\n-\nInitiation\nI contact my friends\n\n\n3\n-\nEmotional\nI express my emotions\n\n\n4\n-\nInitiation\nI think of new things to do during the day\n\n\n5\n-\nEmotional\nI am concerned about how my family feel\n\n\n6\n+\nExecutive\nI find myself staring in to space\n\n\n7\n-\nEmotional\nBefore I do something I think about how others would feel about it\n\n\n8\n-\nInitiation\nI plan my days activities in advance\n\n\n9\n-\nEmotional\nWhen I receive bad news I feel bad about it\n\n\n10\n-\nExecutive\nI am unable to focus on a task until it is finished\n\n\n11\n+\nExecutive\nI lack motivation\n\n\n12\n+\nEmotional\nI struggle to empathise with other people\n\n\n13\n-\nInitiation\nI set goals for myself\n\n\n14\n-\nInitiation\nI try new things\n\n\n15\n+\nEmotional\nI am unconcerned about how others feel about my behaviour\n\n\n16\n-\nInitiation\nI act on things I have thought about during the day\n\n\n17\n+\nExecutive\nWhen doing a demanding task, I have difficulty working out what I have to do\n\n\n18\n-\nInitiation\nI keep myself busy\n\n\n19\n+\nExecutive\nI get easily confused when doing several things at once\n\n\n20\n-\nEmotional\nI become emotional easily when watching something happy or sad on TV\n\n\n21\n+\nExecutive\nI find it difficult to keep my mind on things\n\n\n22\n-\nInitiation\nI am spontaneous\n\n\n23\n+\nExecutive\nI am easily distracted\n\n\n24\n+\nEmotional\nI feel indifferent to what is going on around me\n\n\n\n\n\n\n\nHere are the item numbers that correspond to each dimension.\n\nExecutive: 1, 6, 10, 11, 17, 19, 21, 23\nEmotional: 3, 5, 7, 9, 12, 15, 20, 24\nInitiation: 2, 4, 8, 13, 14, 16, 18, 22\n\n\n\n\n\n\nQuestion 1\n\n\nRead in the data. It will need a little bit of tidying before we can get to fitting a CFA.\nHere’s what you should consider doing:\n\nRename the variables to easy-to-read strings like q1, q2, q3, etc.\nSet up a data dictionary that records the text of the item q1 corresponds to, the text that q2 corresponds to, etc.\nRecode the Likert scale labels to numbers.\nReverse-code the questions with a negative direction. Note, you don’t need to this, as they’ll just end up with loadings in the opposite direction, but I would strongly recommend it for interpretation purposes.\n\nCheck if there is missing data and if there is, removing those observations.\n\nRemember that most of the actions needed for working with those sort of data are described in the Chapter on Data Wrangling for Questionnaires.\n\n\n\n\n\n1 - Read and check\n\n\n\nSolution 1. First let’s just read in the dataset:\n\nrdas &lt;- read_csv(\"https://uoepsy.github.io/data/radakovic_das.csv\")\nhead(rdas)\n\n# A tibble: 6 × 24\n  I need a bit of encouragement …¹ `I contact my friends` I express my emotion…²\n  &lt;chr&gt;                            &lt;chr&gt;                  &lt;chr&gt;                 \n1 Often                            Almost Always          Almost Always         \n2 Almost Always                    Hardly Ever            Occasionally          \n3 Often                            Occasionally           Occasionally          \n4 Hardly Ever                      Occasionally           Almost Always         \n5 Occasionally                     Hardly Ever            Occasionally          \n6 Occasionally                     Occasionally           Almost Always         \n# ℹ abbreviated names: ¹​`I need a bit of encouragement to get things started`,\n#   ²​`I express my emotions`\n# ℹ 21 more variables: `I think of new things to do during the day` &lt;chr&gt;,\n#   `I am concerned about how my family feel` &lt;chr&gt;,\n#   `I find myself staring in to space` &lt;chr&gt;,\n#   `Before I do something I think about how others would feel about it` &lt;chr&gt;,\n#   `I plan my days activities in advance` &lt;chr&gt;, …\n\n\nThe names we’re getting are useful in that they show the items, but they’re horrible to have to use in R, so we will ideally replace them with easy to use names. Note also that the data is being read in as the actual response option - e.g., “Almost Always” - and we want to treat these as a numeric scale. So those will have to change too.\n\n\n\n\n\n2 - Renaming variables\n\n\n\nSolution 2. I like to make a “data dictionary” whenever I get data like this. While I want to rename the variables to make it easier for me to use, I also want to keep track of what the questions were.\nHere I make a “tibble” (the function data.frame() would work too, tibble is just tidyverse version). I indicate what I am going to rename things as (“q1”,“q2”, …, “q24”), and then I have the current names of the variables\n\nrdas_dict &lt;- tibble(\n  variable = paste0(\"q\",1:24),\n  item = names(rdas)\n)\n\nDoing this is really useful because I can’t keep track in my head of what “q5” was.\nIf I want to know, then I can just do:\n\nrdas_dict[5,]\n\n# A tibble: 1 × 2\n  variable item                                   \n  &lt;chr&gt;    &lt;chr&gt;                                  \n1 q5       I am concerned about how my family feel\n\n\nNow let’s actually change the names in our data to what we said we would:\n\nnames(rdas) &lt;- paste0(\"q\", 1:24)\n\n\n\n\n\n\n3 - Recoding responses\n\n\n\nSolution 3. Okay, so we have all our data in words, not numbers. Views on how to treat Likert data are mixed, but it’s very common to treat it as continuous in Psychology.\nLet’s check the response values we have. Just in question 1 for now:\n\nunique(rdas$q1)\n\n[1] \"Often\"         \"Almost Always\" \"Hardly Ever\"   \"Occasionally\" \n[5] \"Always\"        NA              \"Never\"        \n\n\nA little trick that we can use to find the unique values in an entire dataset is to quickly convert the dataframe into one big long vector. Technically, a dataframe is a “list of vectors”, and the function unlist() will remove this structure.\nSo we can find all the unique values in all the questions with:\n\nunique(unlist(rdas))\n\n[1] \"Often\"         \"Almost Always\" \"Hardly Ever\"   \"Occasionally\" \n[5] \"Always\"        NA              \"Never\"         \"[NO ENTRY]\"   \n\n\nPerfect. So we know we have uniformity of spelling. It happens less often these days as questionnaire software is improving, but you might occasionally encounter typos in some of the questions, or things with and without capital letters (R is a bit thick, and doesn’t recognise that “Often” and “often” are the same thing).\nNote that we have the 6 responses that we would expect given the description of the scale, but we also have some NA values, and some [NO ENTRY] values. Not sure how those got there.\nWe want to turn each “Always” in to 0, each “Almost Always” in to 1, “Often” in to 2, and so on. If we simply leave out the “[NO ENTRY]”, then this will be turned into a missing value NA, which is handy.\n\nrdas &lt;- rdas |&gt; \n  mutate(across(q1:q24, ~case_match(.,\n    \"Always\" ~ 0,\n    \"Almost Always\" ~ 1,\n    \"Often\" ~ 2,\n    \"Occasionally\" ~ 3,\n    \"Hardly Ever\" ~ 4,\n    \"Never\" ~ 5\n  )))\nhead(rdas)\n\n# A tibble: 6 × 24\n     q1    q2    q3    q4    q5    q6    q7    q8    q9   q10   q11   q12   q13\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     2     1     1     4     1     3     1     4     2     2     3     2     3\n2     1     4     3     4    NA     0    NA     2    NA     2     1     2     3\n3     2     3     3     4     3     3     2     2     4     1     3     2     3\n4     4     3     1     3     1    NA     2     2    NA     2     3     4     2\n5     3     4     3     5     2     2     2     3     3     2     2     2     1\n6     3     3     1     0     1     2     1     1     0     2     1     4     3\n# ℹ 11 more variables: q14 &lt;dbl&gt;, q15 &lt;dbl&gt;, q16 &lt;dbl&gt;, q17 &lt;dbl&gt;, q18 &lt;dbl&gt;,\n#   q19 &lt;dbl&gt;, q20 &lt;dbl&gt;, q21 &lt;dbl&gt;, q22 &lt;dbl&gt;, q23 &lt;dbl&gt;, q24 &lt;dbl&gt;\n\n\n\n\n\n\n\n4 - Reverse Coding\n\n\n\nSolution 4. According to the table of items, the ones which need to be reverse scored are:\n\nreversed &lt;- c(2,3,4,5,7,8,9,10,13,14,16,18,20,22)\n\nFor these items, we want 5s to become 0s, 4s become 1s, and so on.\nThe tidyverse solution shown in the readings and solutions to previous labs will work just fine, but if you’re curious, here’s a different way to accomplish the same thing using functions from base R:\n\nrdas[, reversed] &lt;- apply(\n  rdas[, reversed], MARGIN = 2, function(x) 5-x)\n\nNote: The above code works nicely because our dataset is currently ordered such that the first column is item 1, 2nd column is item 2, and so on. This means we can use numbers to index the appropriate variables, rather than names. It would need adjusting if, for instance, our first column contained “participant ID”, and our items only began later.\n\n\n\n\n\n5 - Removing missingness\n\n\n\nSolution 5. We haven’t learned about more sophisticated methods of handling missing data, so for now we will just remove any rows in which there is missingness - i.e., we’ll do “listwise deletion”:\n\ncompl_rdas &lt;- na.omit(rdas)\n\n\n\n\n\nQuestion 2\n\n\nSpecify the model.\nFor reference, check out the example in the readings.\n\ndasmod &lt;- \"\n\n\n\n\n\n\"\n\nChallenge: Before you estimate the model, how many degrees of freedom do you think the model will have? (The readings will help here!)\n\n\n\n\n\n\nHints\n\n\n\n\n\nYou’ll have to use the data dictionary to see which items are associated with which dimensions. And we can also see the data dictionary for which ones are negatively worded towards apathy (questions 2, 3, 4, 5, 7, 8, 9, 10, 13, 14, 16, 18, 20, and 22).\n\n\n\n\n\n\n\nHere is the model structure, according to the list of items in the dictionary.\nI’m calling my factors “Em”, “Ex”, and “BCI” for “emotional”, “executive” and “behavioural/cognitive initiation” respectively. The factor correlations will be estimated by default, but I like to write things explicitly.\n\ndasmod = \"\nEx =~ q1 + q6 + q10 + q11 + q17 + q19 + q21 + q23\nEm =~ q3 + q5 + q7 + q9 + q12 + q15 + q20 + q24\nBCI =~ q2 + q4 + q8 + q13 + q14 + q16 + q18 + q22\nEm ~~ Ex\nEm ~~ BCI\nEx ~~ BCI\n\"\n\n\n\n\n\nDegrees of freedom is computed as the number of “knowns” minus the number of “unknowns”.\nLet’s start with figuring out the number of “knowns”: the number of values in the dataset. This number comes from the observed covariance matrix. Let’s imagine a smaller dataset with only five items. It’ll create a covariance matrix like this:\nvar\ncovar   var\ncovar   covar   var\ncovar   covar   covar   var\ncovar   covar   covar   covar   var\nHow many values are in this matrix? In the first row, there’s 1, plus the second row with 2, plus the third row with 3, plus the fourth row with 4, plus the fifth row with 5. In other words, there are\n\nsum(1:5)\n\n[1] 15\n\n\nvalues in this covariance matrix.\nFor the present scenario with 24 items, we will have\n\nsum(1:24)\n\n[1] 300\n\n\nvalues in the covariance matrix. (Twenty-four of these will be each item’s own variance, and the other 276 will be covariances between items.)\nNow let’s look at the number of “unknowns”: the number of parameters the model has to estimate. This number comes from the number of latent variables and how they relate to each item.\n\nEach latent variable has its own variance, and there are three latent variables, so the model will have three latent factor variances.\nEach item will load onto one latent variable, and there are 24 items, so the model will have 24 factor loadings.\nEach item will have residual factor variances, and there are 24 items, so the model will have 24 residual factor variances.\n\nAdding these up, we get\n\n3 + 24 + 24\n\n[1] 51\n\n\nunknown parameters.\nFinally, let’s subtract the knowns from the unknowns to get the degrees of freedom:\n\n300 - 51\n\n[1] 249\n\n\n\n\n\n\nQuestion 3\n\n\nEstimate the model using cfa().\nYou can choose whether you want to standardise the latent factors or fix the first loading of each factor to be 1 (it’s the same model, just scaled differently).\nExamine the model fit - does it fit well?\nWhat modifications do the modification indices suggest? Are the top three suggestions theoretically reasonable, in your opinion?\n\n\n\n\n\n\nHints\n\n\n\n\n\nThere’s a whole section on “model fit” in the CFA chapter!\nAnd there’s also a whole section on model modifications.\n\n\n\n\n\n\n\nLet’s fit the model!\n\ndasmod.est = cfa(dasmod, compl_rdas, std.lv=TRUE)\n\nThe standard test of model fit is a chi-squared test which compares the observed covariance matrix to the model-implied covariance matrix. Ideally, these two matrices will be fairly similar, so we want a non-significant result.\nWe can get the test statistic and p-value from the model’s chi-squared test as follows:\n\nsummary(dasmod.est)$test$standard\n\n$test\n[1] \"standard\"\n\n$stat\n[1] 275\n\n$stat.group\n[1] 275\n\n$df\n[1] 249\n\n$refdistr\n[1] \"chisq\"\n\n$pvalue\n[1] 0.125\n\n\nSo a chi-squared test with 249 degrees of freedom results in a test statistic of 274.8, associated with a p-value of 0.125. The take-away is that our observed covariance matrix is not significantly different from the model-implied covariance matrix—yay!\nNext, let’s check the additional measures of global fit:\n\nfitmeasures(dasmod.est)[c(\"srmr\",\"rmsea\",\"tli\",\"cfi\")]\n\n  srmr  rmsea    tli    cfi \n0.0536 0.0217 0.9691 0.9722 \n\n\nAll looks pretty good! Cut-offs for SRMR tend to vary, with some using &lt;0.08, or &lt;0.09, and some being stricter with &lt;0.05. Remember, these criteria are somewhat arbitrary.\nModification indices suggest a whole bunch of items that could have some associations beyond that modelled in the factors, but these are all weak correlations at around 0.2.\n\nmodindices(dasmod.est, sort=TRUE) |&gt; head()\n\n    lhs op rhs    mi    epc sepc.lv sepc.all sepc.nox\n132  q6 ~~  q3 15.90  0.222   0.222    0.284    0.284\n113  q1 ~~  q9 11.93 -0.172  -0.172   -0.267   -0.267\n216 q19 ~~ q20 10.89  0.168   0.168    0.256    0.256\n103  q1 ~~  q6  7.45  0.149   0.149    0.220    0.220\n358  q4 ~~  q8  7.42  0.199   0.199    0.207    0.207\n105  q1 ~~ q11  6.76 -0.149  -0.149   -0.237   -0.237\n\n\nThese are the top 3 being suggested. I can’t see any obvious link between any of these that would make me think they are related beyond their measuring of ‘apathy’.\n\nrdas_dict[c(3,6),]\n\n# A tibble: 2 × 2\n  variable item                             \n  &lt;chr&gt;    &lt;chr&gt;                            \n1 q3       I express my emotions            \n2 q6       I find myself staring in to space\n\nrdas_dict[c(1,9),]\n\n# A tibble: 2 × 2\n  variable item                                               \n  &lt;chr&gt;    &lt;chr&gt;                                              \n1 q1       I need a bit of encouragement to get things started\n2 q9       When I receive bad news I feel bad about it        \n\nrdas_dict[c(19,20),]\n\n# A tibble: 2 × 2\n  variable item                                                                \n  &lt;chr&gt;    &lt;chr&gt;                                                               \n1 q19      I get easily confused when doing several things at once             \n2 q20      I become emotional easily when watching something happy or sad on TV\n\n\n\n\n\n\nQuestion 4\n\n\nAre the (standardised) loadings all “big enough”?\nThere’s no clear threshold that people use here - it depends a lot on the field, and on the wordings of specific items. As a minimum, the same value we used in EFA (\\(\\geq|0.3|\\)) would be nice.\n\n\n\n\n\n\nHints\n\n\n\n\n\nSee Chapter 5#interpretation.\n\n\n\n\n\n\n\nI’m not going to print all of this right now because there’s so much output, but here’s how we would find standardised loadings. We can find them in the Std.all column.\n\nsummary(dasmod.est, std = TRUE)\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Ex =~                                                                 \n    q1                0.679    0.064   10.623    0.000    0.679    0.694\n    q6                0.461    0.076    6.078    0.000    0.461    0.433\n    ...\n    ...\nThe standardised loadings are all (just) greater than \\(|0.3|\\). Questions 13 and 15 are very close…\n\nrdas_dict[c(13,15),]\n\n# A tibble: 2 × 2\n  variable item                                                     \n  &lt;chr&gt;    &lt;chr&gt;                                                    \n1 q13      I set goals for myself                                   \n2 q15      I am unconcerned about how others feel about my behaviour\n\n\n\n\n\n\nQuestion 5\n\n\nDo the factors correlate in the way you would expect?\nIs more emotional apathy associated with more executive apathy? and with more initiation apathy?\n\n\n\n\n\n\nHints\n\n\n\n\n\nIf you didn’t reverse code the appropriate items, then this might get confusing, because we’d have to look at factor loadings to know in which direction the factor is going (i.e., are higher numbers “more apathy” or “less apathy”?).\nIf you did reverse code the appropriate items, then you’re golden, because you made them all point towards “more” apathy.\n\n\n\n\n\n\n\nHere are the correlations we’re interested in. Note that what we are seeing is that while Em and BCI are positively correlated with one another, being high on Ex is associated with being low on both Em and BCI.\n\nsummary(dasmod.est, std = TRUE)\n\n...\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  Ex ~~                                                                 \n    Em                0.159    0.083    1.927    0.054    0.159    0.159\n  Em ~~                                                                 \n    BCI               0.263    0.085    3.095    0.002    0.263    0.263\n  Ex ~~                                                                 \n    BCI               0.642    0.064   10.010    0.000    0.642    0.642\n\n\n\n\n\n\n\nQuestion 6\n\n\nMake a diagram of the model.\n\n\n\n\n\n\nHints\n\n\n\n\n\nFor a quick look at the structure of the model, try the semPaths() function from the semPlot package Chapter 5 CFA#making diagrams.\nIf you were going to use this sort of diagram in a proper write-up, though, it’d be better to make a nicer graphic manually (e.g., in Powerpoint, your favourite graphics software, or semdiag).\n\n\n\n\n\n\n\nHere’s one example:\n\nlibrary(semPlot)\nsemPaths(dasmod.est, whatLabels = \"std\", rotation=2)\n\n\n\n\n\n\n\n\nThere are lots of options in semPaths(), so if you can make your graphic more elaborate than this one, then be our guest!\n\n\n\n\nOptional Question 7\n\n\n\n\nImagine that you’re a clinician administering the DAS to a patient. In clinical settings, it’s common practice to skip the complex factor analysis we’ve been doing here and just create a sum score or a mean score that describe a patient’s responses. Then clinicians can check whether the score is above some threshold to see whether there’s cause for concern.\nFor each of the dimensions of apathy in the data, calculate sum scores for each of the 250 participants.\n\n\n\n\n\n\nHints\n\n\n\n\n\nGood ol’ rowSums() to the rescue!\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 6. Here are the sets of items associated with each dimension:\n\nExitems &lt;- c(1,6,10,11,17,19,21,23)\nEmitems &lt;- c(3,5,7,9,12,15,20,24)\nBCIitems &lt;- c(2,4,8,13,14,16,18,22)\n\nAgain, because the item numbers correspond to the column positions in our data, we can just do rowSums indexing on those column numbers to get our scores:\n\ncompl_rdas$ExSCORE &lt;- rowSums(compl_rdas[,Exitems])\ncompl_rdas$EmSCORE &lt;- rowSums(compl_rdas[,Emitems])\ncompl_rdas$BCIScore &lt;- rowSums(compl_rdas[,BCIitems])\n\n\n\n\n\nOptional Question 8\n\n\nHow might you think about a sum/mean score in terms of a diagram?\n\n\n\n\n\n\nHints\n\n\n\n\n\nWhat does a sum or mean score imply about how each item is weighted compared to the others? How is this different from what a more sophisticated method like EFA or CFA can do?\n\n\n\n\n\n\n\n\nSolution\n\n\n\nSolution 7. Computing sum scores can feel like a ‘model free’ calculation, but actually it does pre-suppose a factor structure, and a much more constrained one than those we have been estimating. Specifically, we’re assuming that all items contribute equally to an underlying factor, rather than being weighted differently, and that all items also have the same variance as one another.\n\n\n\n\n\n\n\n\n\nFor a full explanation of this idea, see “Thinking twice about sum scores”, McNeish & Wolf 2020.\n\n\n\n\n\n“DOOM” Scrolling\n\nDataset: doom.csv\nThe “Domains of Online Obsession Measure” (DOOM) is a fictitious scale that aims to assess the sub types of addictions to online content. It was developed to measure 2 separate domains of online obsession: items 1 to 9 are representative of the “emotional” relationships people have with their internet usage (i.e. how it makes them feel), and items 10 to 15 reflect “practical” relationship (i.e., how it connects or interferes with their day-to-day life). Each item is measured on a 7-point likert scale from “strongly disagree” to “strongly agree”.\nWe administered this scale to 476 participants in order to assess the validity of the 2 domain structure of the online obsession measure that we obtained during scale development.\nThe data are available at https://uoepsy.github.io/data/doom.csv, and the table below shows the individual item wordings.\n\n\n\n\n\n\n\n\nvariable\nquestion\n\n\n\n\nitem_1\ni just can't stop watching videos of animals\n\n\nitem_2\ni spend hours scrolling through tutorials but never actually attempt any projects.\n\n\nitem_3\ncats are my main source of entertainment.\n\n\nitem_4\nlife without the internet would be boring, empty, and joyless\n\n\nitem_5\ni try to hide how long i’ve been online\n\n\nitem_6\ni avoid thinking about things by scrolling on the internet\n\n\nitem_7\neverything i see online is either sad or terrifying\n\n\nitem_8\nall the negative stuff online makes me feel better about my own life\n\n\nitem_9\ni feel better the more 'likes' i receive\n\n\nitem_10\nmost of my time online is spent communicating with others\n\n\nitem_11\nmy work suffers because of the amount of time i spend online\n\n\nitem_12\ni spend a lot of time online for work\n\n\nitem_13\ni check my emails very regularly\n\n\nitem_14\nothers in my life complain about the amount of time i spend online\n\n\nitem_15\ni neglect household chores to spend more time online\n\n\n\n\n\n\n\n\n\nQuestion 9\n\n\nRead in the data. It’s all been cleaned already, so we can go straight to modelling.\nAssess whether the 2 domain model of online obsession provides a good fit to this validation sample of 476 participants.\n\n\n\n\n\n\nHints\n\n\n\n\n\nThis is essentially asking you to:\n\nspecify the model\nestimate the model\nget some model fit metrics\n\n\n\n\n\n\n\n\nFrom the visual of the correlation matrix, you can see the vague outline of two groups of items correlations. Note there’s a little overlap..\n\ndoom &lt;- read_csv(\"https://uoepsy.github.io/data/doom.csv\")\nheatmap(cor(doom))\n\n\n\n\n\n\n\n\nfirst we write our model:\n\nmoddoom &lt;- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\n\"\n\nThen we fit it to the data:\n\nmoddoom.est &lt;- cfa(moddoom, data = doom)\n\nThen we inspect that fitted model object.\nLet’s look at the chi-squared test.\n\nsummary(moddoom.est)$test$standard\n\n$test\n[1] \"standard\"\n\n$stat\n[1] 270\n\n$stat.group\n[1] 270\n\n$df\n[1] 76\n\n$refdistr\n[1] \"chisq\"\n\n$pvalue\n[1] 0\n\n\nFirst bad sign: the p-value is so close to zero that it’s just been rounded down to display as 0. This means that the chi-squared test rejects the null hypothesis that the observed covariance matrix is similar to the model-implied covariance matrix. Ideally, we’d want a non-significant p-value here.\nOK, let’s extract the fit indices.\n\nfitmeasures(moddoom.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n\n rmsea   srmr    cfi    tli \n0.0733 0.0623 0.8479 0.8179 \n\n\nUh-oh.. they also don’t look great.\n\n\n\n\nQuestion 10\n\n\nAre there any areas of local misfit?\nIn other words, are there certain parameters that are not in the model (and are therefore fixed to zero) but that could improve model fit if they were estimated?\n\n\n\n\n\n\nHints\n\n\n\n\n\nCheck out the section of Chapter 5 CFA#model-modifications for how to check for little areas of misfit.\n\n\n\n\n\n\n\nI’m printing out just the head(), so that I can look at the few parameters with the greatest modification indices.\n\nmodindices(moddoom.est, sort=TRUE) |&gt;\n  head()\n\n        lhs op     rhs    mi    epc sepc.lv sepc.all sepc.nox\n47   item_1 ~~  item_3 88.70  0.560   0.560    0.459    0.459\n32     emot =~ item_10 74.37  1.690   0.817    0.640    0.640\n109  item_7 ~~  item_8 61.93  0.370   0.370    0.432    0.432\n34     emot =~ item_13 14.82 -0.655  -0.317   -0.275   -0.275\n122  item_9 ~~ item_10 12.63  0.145   0.145    0.217    0.217\n135 item_13 ~~ item_15  8.96  0.178   0.178    0.171    0.171\n\n\nThe top three parameters jump out immediately to me.\nFor one: The modification index values are all quite large. These values show how much the model’s chi-squared test statistic would change if we were to include this parameter in the model.\nFor another, consider the values in the sepc.all column.\n\nWhen the operator in the op column is ~~ (that is, when the proposed term is a correlation), then the value in sepc.all can be interpreted like a correlation coefficient. item_1 and item_3 have a suggested correlation of about 0.45, as do item_7 and item_8.\nWhen the operator is =~ (that is, when the proposed term is a new factor loading), then the value in sepc.all can be interpreted as a standardised factor loading. For emot =~ item_10, a factor loading of 0.6 is worth considering, since it’s well above our threshold for “interestingness” of 0.3.\n\n\n\n\n\nQuestion 11\n\n\nBeware: there’s a slightly blurred line here that we’re about to step over, and move from confirmatory back to ‘exploratory’.\nLook carefully at the item wordings. Do any of the suggested modifications make theoretical sense?\nAdd the top three modifications to the model. Does the new model fit well?\n\n\n\n\n\n\nmodel modifications are exploratory!!\n\n\n\n\n\nIt’s likely you will have to make a couple of modifications in order to obtain a model that fits well to this data.\nBUT… we could simply keep adding suggested parameters to our model and we will eventually end up with a perfectly fitting model.\nIt’s very important to think critically here about why such modifications may be necessary.\n\nThe initial model may have failed to capture the complexity of the underlying relationships among variables. For instance, suggested residual covariances, representing unexplained covariation among observed variables, may indicate misspecification in the initial model.\nThe structure of the construct is genuinely different in your population from the initial one in which the scale is developed (this could be a research question in and of itself - i.e. does the structure of “anxiety” differ as people age, or differ between cultures?)\n\nModifications to a CFA model should be made judiciously, with careful consideration of theory as well as quantitative metrics. The goal is to develop a model that accurately represents the underlying structure of the data while maintaining theoretical coherence and generalizability.\n\n\n\nIn this case, the likely reason for the poor fit of the “DOOM” scale, is that the person who made the items (ahem, me) doesn’t really know anything about the construct they are talking about, and didn’t put much care into constructing the items!\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nquestion\n\n\n\n\nitem_1\ni just can't stop watching videos of animals\n\n\nitem_3\ncats are my main source of entertainment.\n\n\nitem_7\neverything i see online is either sad or terrifying\n\n\nitem_8\nall the negative stuff online makes me feel better about my own life\n\n\nitem_10\nmost of my time online is spent communicating with others\n\n\n\n\n\n\n\n\nitem_1 ~~ item_3. These questions are both about animals. It would make sense that these are related over and above the underlying “emotional internet usage” factor.\nitem_7 ~~ item_8. These are both about viewing negative content online, so it makes sense here that they would be related beyond the ‘emotional’ factor.\nemot =~ item_10. This item is about communicating with others. It currently loads highly on the pract factor too. It maybe makes sense here that “communicating with others” will capture both a practical element of internet usage and an emotional one.\n\nThere are three main proposed adjustments from our initial model:\n\nitem_1 ~~ item_3\nitem_7 ~~ item_8\nemot =~ item_10\n\nPutting them all in at once could be a mistake. Partly because we want to make the minimal adjustments necessary, and partly because whenever we adjust a model it affects the modification indices, so we should add things one-by-one and re-check modindices() each time. It’s a bit like Whac-A-Mole - you make one modification and then a whole new area of misfits appears!\nLet’s adjust our model.\nLet’s put the covariance between item_1 and item_3 in. I personally went for this first because they seem more similar to me than item_7 and item_8 do.\n\nmoddoom2 &lt;- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\nitem_1 ~~ item_3\n\"\n\nThen fit it to the data:\n\nmoddoom2.est &lt;- cfa(moddoom2, data = doom)\n\nfitmeasures(moddoom2.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n\n rmsea   srmr    cfi    tli \n0.0536 0.0519 0.9199 0.9028 \n\n\nThe fit is still not great, but it’s better (it was always going to be!). And the other suggested correlations are still present in modification indices:\n\nmodindices(moddoom2.est, sort=TRUE) |&gt;\n  head()\n\n       lhs op     rhs    mi    epc sepc.lv sepc.all sepc.nox\n33    emot =~ item_10 73.22  1.941   0.804    0.630    0.630\n109 item_7 ~~  item_8 61.52  0.371   0.371    0.437    0.437\n35    emot =~ item_13 13.53 -0.728  -0.301   -0.261   -0.261\n122 item_9 ~~ item_10 12.87  0.147   0.147    0.220    0.220\n44   pract =~  item_7  8.76 -0.246  -0.240   -0.215   -0.215\n87  item_4 ~~ item_10  8.62  0.139   0.139    0.174    0.174\n\n\nLet’s add both of those other correlations into the model and fit it:\n\nmoddoom3 &lt;- \"\n# emotional domain\nemot =~ item_1 + item_2 + item_3 + item_4 + item_5 + item_6 + item_7 + item_8 + item_9\n# practical domain\npract =~ item_10 + item_11 + item_13 + item_14 + item_15\n# correlated domains (will be estimated by default)\nemot ~~ pract\nitem_1 ~~ item_3\nitem_7 ~~ item_8\n\"\n\nmoddoom3.est &lt;- cfa(moddoom3, data = doom)\n\nfitmeasures(moddoom3.est)[c(\"rmsea\",\"srmr\",\"cfi\",\"tli\")]\n\n rmsea   srmr    cfi    tli \n0.0354 0.0443 0.9654 0.9575 \n\n\nWhoop! It fits well! It may well be that if we inspect modification indices again, we still see that emot =~ item_10 would improve our model fit. I’m more reluctant to include that one as doing so will change slightly the meaning of our latent factor emot. So it becomes much harder to defend that we are testing the same theoretical model.\nThe other thing to remember is that we could simply keep adding parameters until we run out of degrees of freedom, and our model would “fit better”. But such a model would not be useful. It would not generalise well, becaue it runs the risk of being overfitted to the nuances of this specific sample."
  }
]