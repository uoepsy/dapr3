---
title: "W1: Regression Refresher"
params: 
    SHOW_SOLS: FALSE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
library(ggdist)
xaringanExtra::use_panelset()
qcounter <- function(){
  if(!exists("qcounter_i")){
    qcounter_i <<- 1
  }else{
    qcounter_i <<- qcounter_i + 1
  }
  qcounter_i
}
```

# Workplace Pride


```{r}
#| eval: false
#| echo: false
ss=round(runif(1,1e3,1e5))
set.seed(ss)
set.seed(19872)
n_groups = 16
gp = c(3,rep(1,15))
gp = gp/sum(gp)
g = c(rep(1,30),unlist(lapply(2:16, function(x) rep(x,sample(3*(3:6),1)))))
g = sample(1:n_groups,size=300,replace=T,prob=gp)
g = g[order(g)]
getxn = function(v){
  xx=rep(floor(v/3),3)
  idx = sample(1:3,1)
  xx[idx] = xx[idx] + (v %% 3)
  x = c(rep(0,xx[1]),rep(1,xx[2]),rep(2,xx[3]))
  return(x)
}
x = unlist(lapply(table(g), function(x) getxn(x)))
x[g==1] = rbinom(sum(g==1),2,.2)

b = rbinom(n_groups,1,.5)[g]
c = rnorm(length(g),x,1)

re = MASS::mvrnorm(n_groups, mu=c(0,0),Sigma=matrix(c(1,.5,.5,1),nrow=2))
re0 = re[,1][g]
rex = re[,2][g]
lp = (0 + re0) +  -2*c + (1 + rex) * x + b
y = rnorm(length(g), mean = lp, sd = 1)
#y_bin = rbinom(N, size = 1, prob = plogis(lp))
df=data.frame(x=letters[x+1],c, g = factor(g), b,y)

jsup <- df |>
  transmute(role = x, employment_length = c, dept = g, virtual = b, wp = y)

library(rvest)
mysession = read_html_live("https://en.wikipedia.org/wiki/Departments_of_the_Government_of_the_United_Kingdom")
#mysession$view()
mysession$html_elements(".wikitable")[[2]] |>
  html_table() |>
  janitor::clean_names() -> depts
depts = sample_n(depts,nrow(depts))
depts$dept = c("UKSA","FSA","CPS","ACE","HMRC","OFQUAL","GLD","ORR","CMA","OFSTED","NS&I","SFO","NCA","OFGEM","OFWAT","UKSC","GAD","TNA","HMLR","FC")


jsup$department_name = depts$non_ministerial_department[jsup$dept]
jsup$dept = depts$dept[jsup$dept]
jsup$employment_length = pmax(0,round(scale(jsup$employment_length)[,1]*4.3 + 12.5))
jsup$wp = round(scale(jsup$wp)[,1]*5.2 + 25.6, 2)
jsup$role = toupper(jsup$role)
jsup$rolespine = sample(0:4,nrow(jsup),T)
jsup = jsup |> mutate(
  rolespine = case_when(
    role=="A" ~ rolespine,
    role=="B" ~ rolespine + 5,
    role=="C" ~ rolespine + 10
  )
)

jsup$wp[jsup$dept=="OFQUAL"] = jsup$wp[jsup$dept=="OFQUAL"] - 4
jsup <- jsup[-sample(which(jsup$dept=="OFQUAL"),5),]
  

jsup = jsup |> transmute(department_name, dept, virtual, role, seniority=rolespine, employment_length, wp)

write_csv(jsup, file="../../data/lmm_jsup.csv")

# q1
# roles B and C feel much less supported!  
lm(y~x,df) |> broom::tidy()
# oh.. actually that could be due to age differences.. 
with(df,boxplot(c~x))
with(df,plot(y~c))
lm(y~c+x,df) |> broom::tidy()

# q2
# do roles differ in their perceived support?
anova(
  lm(y~c,df),
  lm(y~c+x,df)
)

# q3.. but departments may well differ in their support. some department might be nice to work in, some less so. 
# in observational data like this - we need to be careful about having mistaking department differences as something else. 
# for instance in dept 1 we happen to have a lot more A than B or C. so if dept 1 happens to be a very nice department (which it does), then our current model comparing A, B and C is going to think group A feel very supported (when actually it's just that the A's that we have are from generally happy dept)
ggplot(df,aes(x=x))+geom_bar()+facet_wrap(~g)
with(df,boxplot(y~g))

# what can we do? well, we can control for dept too!
lm(y~c+x,df) |> broom::tidy()
lm(y~c+g+x,df) |> broom::tidy()
anova(
  lm(y~c+g,df),
  lm(y~c+g+x,df)
)
lm(y~c+g+x,df) |> broom::tidy()
#lme4::lmer(y~c+x+(1|g),df) |> broom::tidy()

# okay, cool!


# so we're starting to acknowledge the grouped structure of our data - these people in our dataset are related to one another in that some belong to dept 1, some dept 2, and so on.. 

# describe the data in more detail:
# - how many ppts, from how many depts?
nrow(df)
length(table(df$g))
# - how many ppts on average from each dept? (min, max?)
df |> count(g) |> summarise(min=min(n),max=max(n),median=median(n))
# - average age of ppts?
mean(df$c)
# - how many depts are b vs not b? 
df |> count(g,b) |> count(b)

# overall average y
mean(df$y)
sd(df$y)
# how much of y variation is due to dept? (ICC)
ICC::ICCbare(g,y,df)




# what if i want to know if, beyond differences due to age and job roles, does the departments' XX influence perceived support?
anova(
  lm(y~c+g+x,df),
  lm(y~c+g+x+b,df)
)
lm(y~c+g+x+b,df) |> broom::tidy()

```

:::frame
__Data: lmm_jsup.csv__

A questionnaire was sent to all UK civil service departments, and the lmm_jsup.csv dataset contains all responses that were received. Some of these departments work as hybrid or 'virtual' departments, with a mix of remote and office-based employees. Others are fully office-based. 

The questionnaire included items asking about how much the respondent believe in the department and how it engages with the community, what it produces, how it operates and how treats its people. A composite measure of 'workplace-pride' was constructed for each employee. Employees in the civil service are categorised into 3 different roles: A, B and C. The roles tend to increase in responsibility, with role C being more managerial, and role A having less responsibility. We also have data on the length of time each employee has been in the department (sometimes new employees come straight in at role C, but many of them start in role A and work up over time). 

We're interested in whether the different roles are associated with differences in workplace-pride.  

**Dataset: [https://uoepsy.github.io/data/lmm_jsup.csv](https://uoepsy.github.io/data/lmm_jsup.csv){target="_blank"}.** 


```{r}
#| echo: false
jsup <- read_csv("https://uoepsy.github.io/data/lmm_jsup.csv")
tibble(
  variable = names(jsup),
  description = c(
    "Name of government department",
    "Department Acronym",
    "Whether the department functions as hybrid department with various employees working remotely (1), or as a fully in-person office (0)",
    "Employee role (A, B or C)",
    "Employees seniority point. These map to roles, such that role A is 0-4, role B is 5-9, role C is 10-14. Higher numbers indicate more seniority",
    "Length of employment in the department (years)",
    "Composite Measure of 'Workplace Pride'")
) |> gt::gt()
```



:::



`r qbegin(qcounter())`
Read in the data and provide some descriptive plots and statistics of each individual variable.  



::: {.callout-tip collapse="true"}
#### Hints

Don't remember how to do descriptives? Think back to previous courses â€“ it's time for some means, standard deviations, mins and maxes. For categorical variables we can do counts or proportions. 

We've seen various functions such as `summary()`, and also `describe()` from the **psych** package.  
For continuous variables, histograms are a good first port of call: try `hist()`.
For categorical variables, you could try plotting the outcome of `table()`.

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Here's the dataset:
```{r}
library(tidyverse) # for data wrangling
library(psych) 

jsup <- read_csv("https://uoepsy.github.io/data/lmm_jsup.csv")
```

Let's take just the numeric variables and get some descriptives:
```{r}
jsup |> 
  select(employment_length, wp) |> 
  describe()
```

And make frequency tables for the categorical ones: 
```{r}
table(jsup$role)
```

I'm going to use `dept` rather than `department_name` as the output will be easier to see:
```{r}
table(jsup$dept)

table(jsup$virtual)
```

`r solend()`

`r qbegin(qcounter())`
Are there differences in 'workplace-pride' between people in different roles?   

First, plot these two variables together. 
Based on this plot and your training from DAPR2, try to answer these questions:

- If you fit a model to this data, how would the predictor be coded?
- What coefficients would the model estimate?
- Would the sign of the coefficients be positive or negative?

Once you've made a good effort to predict the answers to these questions, fit a model and see if your predictions are borne out.
(If your predictions are different from the outcomes, reflect on why the outcomes are the way they are.)


::: {.callout-tip collapse="true"}
#### Hints

does y [continuous variable] differ by x [three groups]?  
`lm(y ~ x)`?  

By default, R uses treatment coding (aka dummy coding), and the reference level is the one that comes first in the alphabet.

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mod1 <- lm(wp ~ role, data = jsup)
```

Rather than doing `summary(model)` - I'm just going to use the **broom** package to pull out some of the stats in nice tidy dataframes.  

The `glance()` function will give us things like the $R^2$ values and $F$-statistic (basically all the stuff that is at the bottom of the `summary()`):  
```{r}
library(broom)
glance(mod1)
```

The `tidy()` function will give us the coefficients, standard errors, t-statistics and p-values. It's the same information, just neater!  
```{r}
tidy(mod1)
```
Alternatively, we can get some quick confidence intervals for our coefficients: 
```{r}
confint(mod1)
```

```{r}
#| include: false
res = apply(car::Confint(mod1), 2, \(x) round(x,2))
```

It looks like roles _do_ differ in their workplace pride. Specifically, compared to people in role A, people who are in roles B and C on average report _less_ pride in the workplace.  

<!-- It looks like roles _do_ differ in their workplace pride ($F(`r glance(mod1)[1,6]`,`r glance(mod1)[1,11]`)=`r glance(mod1)[1,4]`, p`r format.pval(glance(mod1)[1,5],eps=.001)`$). Compared to people in role A, people who are in roles B and C report a lot _less_ pride in the workplace ($b = `r res[2,1]`, 95\%\,\textrm{CI }[`r res[2,2]`, `r res[2,3]`]$ and $b = `r res[3,1]` [`r res[3,2]`, `r res[3,3]`]$ respectively).   -->

`r solend()`

`r qbegin(qcounter())`
One possibility: Something about the roles themselves makes people report differences in workplace pride.

Another possibility: People who are newer to the company feel more pride (they're less jaded), and people in Role A tend to be newer. So something else is at play, but the model above makes it *look like* it's about role.

In other words, if we were to compare people in each role but *hold constant* their `employment_length`, might we see something different?

Make a plot that shows all these relevant variables.
Based on that plot, have a guess at the following questions:

- What coefficients will be estimated by a model fit to this data?
- Would the sign of the coefficients be positive or negative?

Once you've made a good effort to predict the answers to these questions, fit a model and see.
(If your predictions are different from the outcomes, reflect on why the outcomes are the way they are.)


::: {.callout-tip collapse="true"}
#### Hints

So we want to _adjust_ for how long people have been part of the company..  
Remember - if we want to estimate the effect of `x` on `y` while adjusting for `z`, we can do `lm(y ~ z + x)`.  

For the plot - put something on the x, something on the y, and colour it by the other variable.  

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mod2 <- lm(wp ~ employment_length + role, data = jsup)

tidy(mod2)
```

Note that, after adjusting for employment length, there are no significant differences in `wp` between roles B or C compared to A.  

If we plot the data to show all these variables together, we can kind of see why! Given the pattern of `wp` against `employment_length`, the `wp` for different roles are pretty much where we would expect them to be if role doesn't make any difference (i.e., if role doesn't shift your `wp` up or down). 

```{r}
ggplot(jsup, aes(x=employment_length,y=wp,col=role))+
  geom_point(size=3,alpha=.3)
```

`r solend()`

`r qbegin(qcounter())`
Do roles differ in their workplace-pride, when adjusting for time in the company?  

Phrased differently: Does role contribute usefully to our model of workplace pride, or is accounting for employment length enough?

::: {.callout-tip collapse="true"}
#### Hints

This may feel like a repeat of the previous question, but note that this is not a question about _specific_ group differences. It is about whether, overall, the `role` groups differ.  So it's wanting to test the _joint_ effect of the two additional parameters we've just added to our model. (hint hint model comparison!)

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mod2a <- lm(wp ~ employment_length, data = jsup)
mod2 <- lm(wp ~ employment_length + role, data = jsup)

anova(mod2a, mod2)
```

This is no surprise given the previous question, we just now have a single test to report if we wanted to - after accounting for employment length, role does not explain a significant amount of variance in workplace pride.


`r solend()`


`r qbegin(qcounter())`
Let's take a step back and remember what data we actually have. We've got `r nrow(jsup)` people in our dataset, from `r n_distinct(jsup$dept)` departments.  

Departments may well differ in the general amount of workplace-pride people report. People love to say that they work in the "National Crime Agency", but other departments might not elicit such pride (\*cough\* HM Revenue & Customs \*cough\*). 
We need to be careful not to mistake department differences as something else (like differences due to the job role). 

Make a couple of plots to look at:

1. how many of each role we have from each department
2. how departments differ in their employees' pride in their workplace

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
ggplot(jsup, aes(x = role)) + 
  geom_bar()+
  facet_wrap(~dept)
```

In this case, it looks like most of the departments have similar numbers of each role, apart from the UKSA ("UK Statistics Authority"), where we've got _loads_ more of role A, and very few role C..  

Note also that in the plot below, the UKSA is, on average, full of employees who take a lot of pride in their work. Is this due to the high proportion of people in role A? or is the effect of role we're seeing more due to differences in departments? 

```{r}
ggplot(jsup, aes(x = dept, y = wp)) +
  geom_boxplot() +
  scale_x_discrete(labels = label_wrap_gen(35)) + 
  coord_flip()
```

Even if we had perfectly equal numbers of roles in each department, we're also adjusting for other things such as `employment_length`, and the extent to which this differs by department can have trickle-on effects on our coefficient of interest (the `role` coefficients).  

`r solend()`


`r qbegin(qcounter())`
Adjusting for both length of employment _and_ department, are there differences in 'workplace-pride' between the different roles? 
Fit a model to find out.   

Can you make a plot of all four of the variables involved in our model?  

::: {.callout-tip collapse="true"}
#### Hints

Making the plot might take some thinking. We've now added `dept` into the mix, so a nice way might be to use `facet_wrap()` to make the same plot as the one we did previously, but for each department.    

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

```{r}
mod3 <- lm(wp ~ employment_length + dept + role, data = jsup)
tidy(mod3)
```

In a way, adding predictors to our model is kind of like splitting up our plots by that predictor to see the patterns. This becomes more and more difficult (/impossible) as we get more variables, but right now we can split the data into all the constituent parts. 

```{r}
ggplot(jsup, aes(x = employment_length, y = wp, col = role)) +
  geom_point(size=3,alpha=.4)+
  facet_wrap(~dept)
```

The association between `wp` and `employment_length` is clear in all these little sub-plots - there's a downward trend. The department differences can be seen too: UKSA is generally a bit higher, HMRC and UKSC a bit lower, and so on. By default, the model captures these coefficients as 'differences from the reference group', so all these coefficients are in relation to the "ACE" department.  

Seeing the role differences is a bit harder in this plot, but think about what you would expect to see if there were no differences in roles (i.e. imagine if they were all in role A). Take for instance the FSA department, where this is easiest to see - for the people who are in role C, for people of their employment length we would expect their `wp` to be lower if they were in role A. Likewise for those in role B. Across all these departments, the people in role B and C (green and blue dots respectively) are a bit higher than we would expect. This is what the model coefficients tell us!  


`r solend()`


`r qbegin(qcounter())`
Now we're starting to acknowledge the grouped structure of our data - these people in our dataset are related to one another in that some belong to dept 1, some dept 2, and so on..  

Let's try to describe our sample in a bit more detail. 

- how many participants do we have, and from how many departments?
- how many participants are there, on average, from each department? what is the minimum and maximum?
- what is the average employment length for our participants?
- how many departments are 'virtual departments' vs office-based?  
- what is the overall average reported workplace-pride? 
- how much variation in workplace-pride is due to differences between departments?  


::: {.callout-tip collapse="true"}
#### Hints

The first lot of these questions can be answered using things like `count()`, `summary()`, `table()`, `mean()`, `min()` etc. See [1: Clustered Data #determining-sample-sizes](https://uoepsy.github.io/lmm/01_clustered.html#determining-sample-sizes){target="_blank"}

For the last one, we can use the ICC! See [1: Clustered Data #icc](https://uoepsy.github.io/lmm/01_clustered.html#icc---quantifying-clustering-in-an-outcome-variable){target="_blank"}

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

How many respondents do we have, and from how many departments?
```{r}
nrow(jsup)
length(table(jsup$dept))
```


How many respondents are there, on average, from each dept? What is the minimum and maximum number of people in any one department?
```{r}
jsup |>
  count(dept) |> 
  summarise(min=min(n),
            max=max(n),
            median=median(n)
  )
```

What is the average employment length of respondents?
```{r}
mean(jsup$employment_length)
```

How many departments are virtual vs office based?
This requires a bit more than just `table(jsup$virtual)`, because we are describing a variable at the _department_ level.  
```{r}
jsup |> 
  group_by(virtual) |>
  summarise(
    ndept = n_distinct(dept)
  )
```

What is the overall average 'workplace-pride'? What is the standard deviation?
```{r}
mean(jsup$wp)
sd(jsup$wp)
```

Finally, how much variation in workplace-pride is attributable to department-level differences?  
```{r}
ICC::ICCbare(x = dept, y = wp, data = jsup)
```


`r solend()`

`r qbegin(qcounter())`
What if we would like to know whether, when adjusting for differences due to employment length and department and roles, workplace-pride differs between people working in virtual-departments compared to office-based ones?  

Can you add this to the model? What happens?  
`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Let's add the `virtual` predictor to our model. Note that we don't actually get a coefficient here - it is giving us an `NA`!  
```{r}
mod4 <- lm(wp ~ employment_length + dept + role + virtual, data = jsup)

summary(mod4)
```

So what is happening? If we think about it, if we separate out "differences due to departments" then there is nothing left to compare between departments that are virtual vs office based.
Adding the between-department predictor of `virtual` doesn't explain anything more - the residual sums of squares doesn't decrease at all:  
```{r}
anova(
  lm(wp ~ employment_length + dept + role, data = jsup),
  lm(wp ~ employment_length + dept + role + virtual, data = jsup)
)
```

Another way of thinking about this: knowing the average workplace-pride for the department that someone is in tells me what to expect about that person's workplace pride. But once I know their department's average workplace-pride, knowing whether it is 'virtual' or 'office-based' doesn't tell me anything new, for the very fact that the virtual/office-based distinction comes from *comparing different departments*.  

But we're not really interested in these departments specifically! What would be nice would be if we can look at the relevant effects of interest (things like `role` and `virtual`), but then just think of the department differences as just some sort of random variation. So we want to think of departments in a similar way to how we think of our individual *employees* - they vary randomly around what we expect - only they're at a different _level_ of observation.  


`r solend()`



