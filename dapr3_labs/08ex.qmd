---
title: "W8 Exercises: EFA"
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
source('assets/setup.R')
library(xaringanExtra)
library(tidyverse)
library(patchwork)
xaringanExtra::use_panelset()
qcounter <- function(){
  if(!exists("qcounter_i")){
    qcounter_i <<- 1
  }else{
    qcounter_i <<- qcounter_i + 1
  }
  qcounter_i
}
```



# Conduct Problems  

:::frame
__Data: Conduct Problems__  

A researcher is developing a new brief measure of Conduct Problems. She has collected data from n=450 adolescents on 10 items, which cover the following behaviours:  

1. Breaking curfew
1. Vandalism
1. Skipping school
1. Bullying
1. Spreading malicious rumours
1. Fighting
1. Lying
1. Using a weapon 
1. Stealing
1. Threatening others


Our task is to use the dimension reduction techniques we learned about in the lecture to help inform how to organise the items she has developed into subscales.  

The data can be found at [https://uoepsy.github.io/data/conduct_probs_scale.csv](https://uoepsy.github.io/data/conduct_probs_scale.csv){target="_blank"} 

:::

`r qbegin(qcounter())`
Read in the dataset.  
Create a correlation matrix for *the items*, and inspect the items to check their suitability for exploratory factor analysis


::: {.callout-tip collapse="true"}
#### Hints

Take a look at [Reading 9# Initial Checks](r09_efa.html#initial-checks){target="_blank"}.

:::


`r qend()` 
`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`
```{r}
cpdata <- read.csv("https://uoepsy.github.io/data/conduct_probs_scale.csv")
# discard the first column
cpdata <- cpdata[,-1]
```

Here's a correlation matrix. There's no obvious blocks of items here, but we can see that there are some fairly high correlations, as well as some weaker ones. All are positive.  
```{r}
heatmap(cor(cpdata))
```

The Bartlett's test comes out with a p-value of 0 (which isn't possible, but it's been rounded for some reason). This suggests that we reject the null of this test (that our correlation matrix is proportional to the identity matrix). This is good. It basically means "we have some non-zero correlations"!  
```{r}
library(psych)
cortest.bartlett(cor(cpdata), n=450)
```

The overall sampling adequacy is 0.87, which is pretty good! (or rather, which is 'meritorious'!).  MSA for all items is >.8
```{r}
KMO(cpdata)  
```

Finally, all the relationships here look fairly linear:
```{r}
pairs.panels(cpdata)
```

`r solend()`


`r qbegin(qcounter())`
How many dimensions should be retained?  

This question can be answered in the same way as we did for PCA - use a scree plot, parallel analysis, and MAP test to guide you.   

`r qend()` 

`r solbegin(label="Scree", slabel=F, show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The scree plot shows a kink at 3, which suggests retaining 2 components. 
```{r}
scree(cpdata)
```

`r solend()`
`r solbegin(label="MAP", slabel=F, show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The MAP suggests retaining 2 factors. I'm just extracting the actual `map` values here to save having to show all the other output. We can see that the 2nd entry is the smallest: 
```{r}
VSS(cpdata, plot = FALSE, n = ncol(cpdata))$map
```


`r solend()`
`r solbegin(label="Parallel Analysis", slabel=F, show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Parallel analysis suggests 2 factors as well:
```{r}
fa.parallel(cpdata, fa = "both")
```

`r solend()`
`r solbegin(label="Making a decision", slabel=F, show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Again, a quite clear picture that 2 factors is preferred:  
```{r}
#| echo: false
tibble(
  guides = c("Scree","MAP","Parallel Analysis"),
  suggestion = rep(2,3)
) |> gt::gt()
```

`r solend()`

`r qbegin(qcounter())`
Use the function `fa()` from the **psych** package to conduct and EFA to extract 2 factors (this is what *we* suggest based on the various tests above, but *you* might feel differently - the ideal number of factors is subjective!). Use a suitable rotation (`rotate = ?`) and extraction method (`fm = ?`).  

::: {.callout-tip collapse="true"}
#### Hints

Would you expect factors to be correlated? If so, you'll want an oblique rotation.  
See [R9#doing-an-efa](r09_efa.html#doing-an-efa){target="_blank"}.  

:::



`r qend()` 
`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`
For example, you could choose an oblimin rotation to allow factors to correlate. Let's use MLE as the estimator.  
```{r}
conduct_efa <- fa(cpdata, nfactors=2, rotate='oblimin', fm="ml")
```
`r solend()`

`r qbegin(qcounter())`
Inspect your solution. Make sure to look at and think about the loadings, the variance accounted for, and the factor correlations (if estimated).  

::: {.callout-tip collapse="true"}
#### Hints

Just printing an `fa` object:
```{r}
#| eval: false
myfa <- fa(data, ..... )
myfa
```
Will give you lots and lots of information.  
You can extract individual parts using:  

- `myfa$loadings` for the loadings
- `myfa$Vaccounted` for the variance accounted for by each factor
- `myfa$Phi` for the factor correlation matrix

You can find a quick guide to reading the `fa` output here: [efa_output.pdf](https://uoepsy.github.io/dapr3/2425/misc/efa_output.pdf){target="_blank"}.  

:::

`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

Things look pretty good here. Each item has a clear primary loading on to one of the factors, and the complexity for all items is 1 (meaning they're clearly link to just one of the factors). The `h2` column is showing that the 2 factor solution is explaining 39%+ of the variance in each item.  Both factors are well determined, having a at least 3 salient loadings.  

The 2 factors together explain 57% of the variance in the data - both factors explain a similar amount (29% for factor 1, 28% for factor 2). 

We can also see that there is a moderate correlation between the two factors. Use of an oblique rotation was appropriate - if the correlation had been very weak, then it might not have differed much from if we used an orthogonal rotation.  

```{r}
#| eval: false
conduct_efa
```
```{r}
#| echo: false
.pp(conduct_efa, l=list(1:28))
```


`r solend()`



`r qbegin(qcounter())`
Look back to the description of the items, and suggest a name for your factors based on the patterns of loadings.  


::: {.callout-tip collapse="true"}
#### Hints

To sort the loadings, you can use
```{r}
#| eval: false
print(myfa$loadings, sort = TRUE)
```

:::

`r qend()` 
`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`
You can inspect the loadings using:
```{r}
print(conduct_efa$loadings, sort=TRUE)
```
We can see that, ordered like this, we have five items that have high loadings for one factor and another five items that have high loadings for the other.  
  
The five items for factor 2 all have in common that they are non-aggressive forms of conduct problems. The five items for factor 1 are all more aggressive behaviours. We could, therefore, label our factors: 'aggressive' and 'non-aggressive' conduct problems.
`r solend()`

`r qbegin(qcounter())`
Compare three different solutions: 

1) your current solution from the previous questions
2) one where you fit 1 more factor
3) one where you fit 1 fewer factors   

Which one looks best?  


::: {.callout-tip collapse="true"}
#### Hints

We're looking here to assess:  

- how much variance is accounted for by each solution
- do all factors load on 3+ items at a salient level?  
- do all items have at least one loading at a salient level?
- are there any "Heywood cases" (communalities or standardised loadings that are >1)?
- should we perhaps remove some of the more complex items?
- is the factor structure (items that load on to each factor) coherent, and does it make theoretical sense?

:::


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

The 1-factor model explains 37% of the variance (as opposed to the 57% explained by the 2 factor solution), and all items load fairly high on the factor. The downside here is that we're not discerning between different types of conduct problems that we did in the 2 factor solution.  
```{r}
conduct_1 <- fa(cpdata, nfactors=1, fm="ml")
conduct_1
```

The 3-factor model explains 60% of the variance (only 3% more than the 2-factor model). Notably, the third factor is not very clearly defined - it only has 1 salient loading (possibly 2 if we consider the 0.3 to be salient, but that item is primarily loaded on the 2nd factor). 
```{r}
conduct_3 <- fa(cpdata, nfactors=3, rotate='oblimin', fm="ml")
conduct_3
```



`r solend()`


`r qbegin(qcounter())`
Write a brief paragraph or two that summarises your method and the results from your chosen optimal factor structure for the 10 conduct problems.  


::: {.callout-tip collapse="true"}
#### Hints

Write about the process that led you to the number of factors. Discuss the patterns of loadings and provide definitions of the factors. 

:::




`r qend()`
`r solbegin(show=params$SHOW, toggle=params$TOGGLE)`
The main principles governing the reporting of statistical results are transparency and reproducibility (i.e., someone should be able to reproduce your analysis based on your description).

An example summary would be:

:::int 

First, the data were checked for their suitability for factor analysis. Normality was checked using visual inspection of histograms, linearity was checked through the inspection of the linear and lowess lines for the pairwise relations of the variables, and factorability was confirmed using a KMO test, which yielded an overall KMO of $.87$ with no variable KMOs $<.50$. 
An exploratory factor analysis was conducted to inform the structure of a new conduct problems test. Inspection of a scree plot alongside parallel analysis (using principal components analysis; PA-PCA) and the MAP test were used to guide the number of factors to retain. All three methods suggested retaining two factors; however, a one-factor and three-factor solution were inspected to confirm that the two-factor solution was optimal from a substantive and practical perspective, e.g., that it neither blurred important factor distinctions nor included a minor factor that would be better combined with the other in a one-factor solution. These factor analyses were conducted using maximum likelihood estimation and (for the two- and three-factor solutions) an oblimin rotation, because it was expected that the factors would correlate. Inspection of the factor loadings and correlations reinforced that the two-factor solution was optimal: both factors were well-determined, including 5 loadings $>|0.3|$ and the one-factor model blurred the distinction between different forms of conduct problems. 
The factor loadings are provided in @tbl-loadingtab^[You should provide the table of factor loadings. It is conventional to omit factor loadings $<|0.3|$; however, be sure to ensure that you mention this in a table note.]. Based on the pattern of factor loadings, the two factors were labelled 'aggressive conduct problems' and 'non-aggressive conduct problems'. These factors had a  correlation of $r=.43$. Overall, they accounted for 57% of the variance in the items, suggesting that a two-factor solution effectively summarised the variation in the items.


```{r}
#| label: tbl-loadingtab
#| echo: false
#| tbl-cap: "Factor Loadings"
loadings = unclass(conduct_efa$loadings)
loadings = round(loadings, 3)
loadings = loadings[order(loadings[,1],decreasing = T),]
loadings[abs(loadings) < 0.3] = NA
loadings[!is.na(loadings[,2]),] <- 
  loadings[!is.na(loadings[,2]),][
    order(loadings[!is.na(loadings[,2]),2],decreasing = T),
  ]
options(knitr.kable.NA = '')
knitr::kable(loadings, digits = 2)
```

:::

`r solend()`


# The Creativity Scale

:::frame
__Dataset: creativity.rdata__  

In the lecture in Week 7, we did a very quick crowd-sourcing of "what we think 'creativity' is", and then we turned these into questionnaire items, and we completed the questionnaire (well, some of us completed it!).  

The data can be loaded into your R environment using: 
```{r}
#| eval: false
load(url("https://uoepsy.github.io/dapr3/2526/misc/creativity.rdata"))
```

After you run this, you should find two things in your environment:  

- `crdat` = the raw data from Qualtrics
- `critems` = the wordings of each question. This can be useful because if you want to quickly remember what the 4th question was, you can just type `critems[4]` in your console and it will print it out.  

:::

`r qbegin(qcounter())`
Our task is going to be to conduct EFA to explore the underlying structure of our scale. As this is a completely new scale that we have just pulled out of our hats, we will feel free to consider removing "problematic" items (and then going back to the start of the EFA process but using the reduced set).  

This will involve making **lots** of decisions along the way.  

You can find an analysis here:  

- [Rmd](https://uoepsy.github.io/dapr3/2526/misc/creativity.Rmd){target="_blank"}
- [html](https://uoepsy.github.io/dapr3/2526/misc/creativity.html){target="_blank"}

Go through the code and jot down all the decision points, and ask yourself: 1. why did we make those decisions, and 2. what other decisions _could_ we have made?  


`r qend()`
`r solbegin(show=params$SHOW_SOLS, toggle=params$TOGGLE)`

- correlations were low, overall KMO was low, and we decided to just carry on regardless. We could have stopped here and gone back to the drawing board - rewritten our items (more carefully) and collected new data.
- our subjective reading of the scree plot led us to 1-3 factors. you could argue there's a kink at the 5th point, but then you could also argue that this is all below the eigenvalue of 1. Eigenvalues below 1 are dimensions that are essentially capturing less variability in the data than a single variable would.  
- there were decision points when we fit the 3 models to go with principal axis factoring and to use an oblique rotation. The skew wasn't *terrible*, so many researchers might not have worried too much and just gone with ML.
- we viewed the 2 factor model as looking more suitable. why? the 1 factor model had a lot of items with no salient loadings. the 3 factor model had a factor (PA3) which had only 2 items with it as the primary loading (creativity_2 was cross loading on PA2 and PA3). So the third factor looked like it was under-identified. We also said that we didn't mind about losing 7% of the variance.  
- A clear decision point is removal of creativity_8. There were various things that fed into this choice - it had low KMO, and low correlations, and in the factor models was not loading on to any factor apart from in the 3-factor solution, where it PA3 was defined mainly by creativity_8 and not much else. So it sort of feels like responses to this question are capturing something different from the others. In addition (and very importantly!), when we read the wording of the question, it didn't scream "creativity".  
- After removing creativity_8, most of the same decisions came all over again. We then got to the decision of creativity_5, which didn't have a salient loading in either the 1 or 2 factor solutions. The wording of this question is more closely related to the idea of "creativity", but perhaps it's not worded very well. The *ability* to connect ideas from different areas isn't quite the same thing as having the creative *idea* to do so. 
- finally, once we removed creativity_5, by keeping the 2-factor solution we made a decision that both of these factors represented useful aspects of "creativity". This is actually a decision that often gets overlooked in research --- we do all the stats to develop a scale and then forget to stop at the end and just think about whether what we've ended up with actually looks like what we wanted.^[missing this step leads to the jingle-jangle fallacies, because we would now go out and tell everyeon that this scale measures 'creativity'!]. You might look at the two sets of questions and see something else. We named our factors "novel" and "art", and I could see a very valid argument that neither of these really capture the vague idea of what I think of as "creativity". Artistic expression does not necessitate creativity? Nor novelty for the sake of novelty.  


`r solend()`


